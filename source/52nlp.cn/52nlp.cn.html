<!DOCTYPE html>
<!--[if IE 7]>
<html class="ie ie7" lang="zh-CN">
<![endif]-->
<!--[if IE 8]>
<html class="ie ie8" lang="zh-CN">
<![endif]-->
<!--[if !(IE 7) & !(IE 8)]><!-->
<html lang="zh-CN"><!--<![endif]--><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<title>我爱自然语言处理 | I Love Natural Language Processing</title>
<link rel="profile" href="http://gmpg.org/xfn/11">
<link rel="pingback" href="http://www.52nlp.cn/xmlrpc.php">
<!--[if lt IE 9]>
<script src="http://www.52nlp.cn/wp-content/themes/twentytwelve/js/html5.js" type="text/javascript"></script>
<![endif]-->
<link rel="dns-prefetch" href="http://s.w.org/">
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 » Feed" href="http://www.52nlp.cn/feed">
<link rel="alternate" type="application/rss+xml" title="我爱自然语言处理 » 评论Feed" href="http://www.52nlp.cn/comments/feed">
		<script type="text/javascript" async="" defer="defer" src="52nlp.cn_files/piwik.js"></script><script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.4\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.4\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/www.52nlp.cn\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.9.6"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55357,56692,8205,9792,65039],[55357,56692,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="52nlp.cn_files/wp-emoji-release.js" type="text/javascript" defer="defer"></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel="stylesheet" id="yarppWidgetCss-css" href="52nlp.cn_files/widget.css" type="text/css" media="all">
<link rel="stylesheet" id="wp-syntax-css-css" href="52nlp.cn_files/wp-syntax.css" type="text/css" media="all">
<link rel="stylesheet" id="twentytwelve-style-css" href="52nlp.cn_files/style.css" type="text/css" media="all">
<!--[if lt IE 9]>
<link rel='stylesheet' id='twentytwelve-ie-css'  href='http://www.52nlp.cn/wp-content/themes/twentytwelve/css/ie.css?ver=20121010' type='text/css' media='all' />
<![endif]-->
<script type="text/javascript" src="52nlp.cn_files/jquery.js"></script>
<script type="text/javascript" src="52nlp.cn_files/jquery-migrate.js"></script>
<link rel="https://api.w.org/" href="http://www.52nlp.cn/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://www.52nlp.cn/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://www.52nlp.cn/wp-includes/wlwmanifest.xml"> 
<meta name="generator" content="WordPress 4.9.6">
<!-- wp thread comment 1.4.9.4.002 -->
<style type="text/css" media="screen">
.editComment, .editableComment, .textComment{
	display: inline;
}
.comment-childs{
	border: 1px solid #999;
	margin: 5px 2px 2px 4px;
	padding: 4px 2px 2px 4px;
	background-color: white;
}
.comment-childs > p {
        word-break: break-word;
        word-wrap: break-word;
}
.chalt{
	background-color: #E2E2E2;
}
#newcomment{
	border:1px dashed #777;width:90%;
}
#newcommentsubmit{
	color:red;
}
.adminreplycomment{
	border:1px dashed #777;
	width:99%;
	margin:4px;
	padding:4px;
}
.mvccls{
	color: #999;
}
			
</style>
</head>

<body class="home blog">
<div id="page" class="hfeed site">
	<header id="masthead" class="site-header" role="banner">
		<hgroup>
			<h1 class="site-title"><a href="http://www.52nlp.cn/" title="我爱自然语言处理" rel="home">我爱自然语言处理</a></h1>
			<h2 class="site-description">I Love Natural Language Processing</h2>
		</hgroup>

		<nav id="site-navigation" class="main-navigation" role="navigation">
			<button class="menu-toggle">菜单</button>
			<a class="assistive-text" href="#content" title="跳至内容">跳至内容</a>
			<div class="nav-menu"><ul>
<li class="current_page_item"><a href="http://www.52nlp.cn/">首页</a></li><li class="page_item page-item-9840"><a href="http://www.52nlp.cn/%e8%af%be%e7%a8%8b">公开课</a></li>
<li class="page_item page-item-9088"><a href="http://www.52nlp.cn/%e4%b9%a6%e7%b1%8d">书籍</a></li>
<li class="page_item page-item-9435"><a href="http://coursegraph.com/" target="_blank">课程图谱</a></li>
<li class="page_item page-item-9432"><a href="http://www.nlpjob.com/" target="_blank">招聘求职</a></li>
<li class="page_item page-item-2557 page_item_has_children"><a href="http://www.52nlp.cn/resources">资源</a>
<ul class="children">
	<li class="page_item page-item-1271"><a href="http://www.52nlp.cn/resources/wpmatheditor">WpMathEditor</a></li>
</ul>
</li>
<li class="page_item page-item-2"><a href="http://www.52nlp.cn/about">关于</a></li>
<li class="page_item page-item-9013"><a href="http://weibo.com/52nlp" target="_blank">微博</a></li>
</ul></div>
		</nav><!-- #site-navigation -->

			</header><!-- #masthead -->

	<div id="main" class="wrapper">

	<div id="primary" class="site-content">
		<div id="content" role="main">
		
										
	<article id="post-10334" class="post-10334 post type-post status-publish format-standard hentry category-1153 tag-1080ti tag-1080ti- tag-cuda tag-cuda9 tag-cuda9-0 tag-cuda9-2 tag-cudnn tag-cudnn7 tag-cudnn7-0 tag-cudnn7-1 tag-cudnn7-x tag-ducda9-x tag-gtx-1080ti tag-keras tag-keras2-1 tag-tensorflow tag-tensorflow1-8 tag-ubuntu tag-ubuntu16-04 tag-1387 tag-1388 tag-1386 tag-1018 tag-1263 tag-diy tag-1382 tag-1261 tag-1384 tag-1383 tag-1264">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8-1080ti-ubuntu16-04-cuda9-2-cudnn7-1-tensorflow-keras" rel="bookmark">从零开始搭建深度学习服务器: 1080TI四卡并行（Ubuntu16.04+CUDA9.2+cuDNN7.1+TensorFlow+Keras）</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8-1080ti-ubuntu16-04-cuda9-2-cudnn7-1-tensorflow-keras#comments">7条回复</a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p>这个系列写了好几篇文章，这是相关文章的索引，仅供参考：</p>
<ul>
<li><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E6%94%92%E6%9C%BA%E5%B0%8F%E8%AE%B0" rel="noopener" target="_blank">深度学习主机攒机小记</a></li>
<li><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu-16-04-nvidia-gtx-1080-cuda-8" rel="noopener" target="_blank">深度学习主机环境配置: Ubuntu16.04+Nvidia GTX 1080+CUDA8.0</a></li>
<li><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu16-04-geforce-gtx1080-tensorflow" rel="noopener" target="_blank">深度学习主机环境配置: Ubuntu16.04+GeForce GTX 1080+TensorFlow</a></li>
<li><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-ubuntu17-04-nvidia-gtx-1080-cuda-9-0-cudnn-7-0-tensorflow-1-3" rel="noopener" target="_blank">深度学习服务器环境配置: Ubuntu17.04+Nvidia GTX 1080+CUDA 9.0+cuDNN 7.0+TensorFlow 1.3</a></li>
<li><a href="http://www.52nlp.cn/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8-%E7%A1%AC%E4%BB%B6%E9%80%89%E6%8B%A9" rel="noopener" target="_blank">从零开始搭建深度学习服务器：硬件选择</a></li>
<li><a href="http://www.52nlp.cn/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEubuntu-1080ti-cuda-cudnn" rel="noopener" target="_blank">从零开始搭建深度学习服务器: 基础环境配置（Ubuntu + GTX 1080 TI + CUDA + cuDNN）</a></li>
<li><a href="http://www.52nlp.cn/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8-tensorflow-pytorch-torch" rel="noopener" target="_blank">从零开始搭建深度学习服务器: 深度学习工具安装（TensorFlow + PyTorch + Torch）</a></li>
<li><a href="http://www.52nlp.cn/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85-theano-mxnet" target="_blank">从零开始搭建深度学习服务器: 深度学习工具安装（Theano + MXNet）</a></li>
<li><a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8-1080ti-ubuntu16-04-cuda9-2-cudnn7-1-tensorflow-keras" rel="noopener" target="_blank">从零开始搭建深度学习服务器: 1080TI四卡并行（Ubuntu16.04+CUDA9.2+cuDNN7.1+TensorFlow+Keras）</a></li>
</ul>
<p>最近公司又弄了一套4卡1080TI机器，配置基本上和之前是一致的，只是显卡换成了技嘉的伪公版1080TI：技嘉GIGABYTE GTX1080Ti 涡轮风扇108TTURBO-11GD</p>
<pre>部件	型号	价格	链接	备注
CPU	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZR1aFQIWBFYfXCUCEg5VHloUAxUPUisfSlpMWGVCHlBDGRlLQx5BXg1cAAQJS14MB1USWxADEwZSE1wKW1dbCCteaXYUYxB%2FXnZCSn1SHihxRVJdCXArGQ4iAFAcXx0FGgZlHl0XAyI3VRprVGwbBlASWiUHEg9VE1gVBhs3VR9aHAUXD1ETUh0LFTdSKw1FQVpGHUsEQzIiN2U%3D&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVsVCxICVBpaEgoVGAxeB0g%3D" rel="noopener" target="_blank">英特尔（Intel）酷睿六核i7-6850K 盒装CPU处理器</a> 	4599	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZR1aFQIWBFYfXCUCEg5VHloUAxUPUisfSlpMWGVCHlBDGRlLQx5BXg1cAAQJS14MB1USWxADEwZSE1wKW1dbCCteaXYUYxB%2FXnZCSn1SHihxRVJdCXArGQ4iAFAcXx0FGgZlHl0XAyI3VRprVGwbBlASWiUHEg9VE1gVBhs3VR9aHAUXD1ETUh0LFTdSKw1FQVpGHUsEQzIiN2U%3D&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVsVCxICVBpaEgoVGAxeB0g%3D" rel="noopener" target="_blank">http://item.jd.com/11814000696.html</a>	
散热器	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprHAcRBVISa1FdSlkKKwJQR1MMSwUDUFZOGA5OREdcThlcHlgXBRsYDF4HSDJxHTdMGXN3SGc9Rw0QehAZVGw5QgRyC1krXBAFFg9SE1olBxQFVCtrFQMiUTsbWhQDEwZUHFMTMhcHXRtTFgIWDmUbXxQLFQJQH14UBRoBZRxrQ1JRTxRTC0pUIjdlKw%3D%3D&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVIQARAAXAQCUF5P" rel="noopener" target="_blank">美商海盗船 H55 水冷</a>	449	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprHAcRBVISa1FdSlkKKwJQR1MMSwUDUFZOGA5OREdcThlcHlgXBRsYDF4HSDJxHTdMGXN3SGc9Rw0QehAZVGw5QgRyC1krXBAFFg9SE1olBxQFVCtrFQMiUTsbWhQDEwZUHFMTMhcHXRtTFgIWDmUbXxQLFQJQH14UBRoBZRxrQ1JRTxRTC0pUIjdlKw%3D%3D&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVIQARAAXAQCUF5P" rel="noopener" target="_blank">https://item.jd.com/10850633518.html</a>	
主板	华硕（ASUS）华硕 X99-E WS/USB 3.1工作站主板	4759	
内存	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprFQoaBlEdWCVGTV8LRGtMR1dGXgVFTUdGW0pADgpQTFtLG1MdAxYBVgQCUF5PNxEBLUdXUXgofjljWhVYIBIOQmNBewMXVyUFFwBRE1wdAyICUxlaJTISBmVNNRUDEwZUGloSChQ3UBtTFQoRB1ESaxUGEw5SHl4TCxAPUBJrEjJEVxZTGl1STVFlK2sl&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVsdChMDUxhETEdOWg%3D%3D" rel="noopener" target="_blank">美商海盗船(USCORSAIR) 复仇者LPX DDR4 3000 32GB(16Gx4条)</a>  	2799 * 2	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprFQoaBlEdWCVGTV8LRGtMR1dGXgVFTUdGW0pADgpQTFtLG1MdAxYBVgQCUF5PNxEBLUdXUXgofjljWhVYIBIOQmNBewMXVyUFFwBRE1wdAyICUxlaJTISBmVNNRUDEwZUGloSChQ3UBtTFQoRB1ESaxUGEw5SHl4TCxAPUBJrEjJEVxZTGl1STVFlK2sl&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVsdChMDUxhETEdOWg%3D%3D" rel="noopener" target="_blank">https://item.jd.com/1990572.html</a>	
SSD	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprFwQQD1QTXSVGTV8LRGtMR1dGXgVFTUdGW0pADgpQTFtLGV0XChMPUwQCUF5PN1ZoDwtBFFgjfgtBdkR%2BIWAFVWBLfQMXVyUFFwBRE1wdAyICUxlaJTISBmVNNRUDEwZUHV8dCxI3UBtTFQoRB1ESaxUGEw5SHlscBxMBVR9rEjJEVxZTGl1STVFlK2sl&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVkTABoGXR1ETEdOWg%3D%3D" rel="noopener" target="_blank">三星(SAMSUNG) 960 EVO 250G M.2 NVMe 固态硬盘</a>	599	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprFwQQD1QTXSVGTV8LRGtMR1dGXgVFTUdGW0pADgpQTFtLGV0XChMPUwQCUF5PN1ZoDwtBFFgjfgtBdkR%2BIWAFVWBLfQMXVyUFFwBRE1wdAyICUxlaJTISBmVNNRUDEwZUHV8dCxI3UBtTFQoRB1ESaxUGEw5SHlscBxMBVR9rEjJEVxZTGl1STVFlK2sl&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBVkTABoGXR1ETEdOWg%3D%3D" rel="noopener" target="_blank">https://item.jd.com/3739097.html</a>		
硬盘	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprEAERBlYfXSVGTV8LRGtMR1dGXgVFTUdGW0pADgpQTFtLHlgWAxEDUwQCUF5PNxMYJV0DR1ELeCB3RG1OA34hfFFsVwMXVyUFFwBRE1wdAyICUxlaJTISBmVNNRUDEwZUGloTAhM3UBtTFQoRB1ESaxUGEw5SHlMdBhcHVBprEjJEVxZTGl1STVFlK2sl&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBV4WARMEUR1ETEdOWg%3D%3D" rel="noopener" target="_blank">希捷(SEAGATE)酷鱼系列 4TB 5900转 台式机机械硬盘 * 2</a> 	629 * 2	<a href="http://union-click.jd.com/jdc?e=0&amp;p=AyIHZRprEAERBlYfXSVGTV8LRGtMR1dGXgVFTUdGW0pADgpQTFtLHlgWAxEDUwQCUF5PNxMYJV0DR1ELeCB3RG1OA34hfFFsVwMXVyUFFwBRE1wdAyICUxlaJTISBmVNNRUDEwZUGloTAhM3UBtTFQoRB1ESaxUGEw5SHlMdBhcHVBprEjJEVxZTGl1STVFlK2sl&amp;t=W1dCFBBFC1pXUwkEAEAdQFkJBV4WARMEUR1ETEdOWg%3D%3D" rel="noopener" target="_blank">https://item.jd.com/4220257.html</a>	
电源	<a href="http://union-click.jd.com/jdc?d=ARYd0B" rel="noopener" target="_blank">美商海盗船 AX1500i 全模组电源 80Plus金牌</a>	3699	<a href="http://union-click.jd.com/jdc?d=ARYd0B" rel="noopener" target="_blank">https://item.jd.com/10783917878.html</a>
机箱	<a href="http://union-click.jd.com/jdc?d=1MlxaL" rel="noopener" target="_blank">美商海盗船 AIR540 USB3.0 </a>	949	<a href="http://union-click.jd.com/jdc?d=1MlxaL" rel="noopener" target="_blank">http://item.jd.com/12173900062.html</a>
显卡	<a href="http://union-click.jd.com/jdc?d=kNxgkP" rel="noopener" target="_blank">技嘉（GIGABYTE） GTX1080Ti 11GB 非公版高端游戏显卡深度学习涡轮</a> * 4 7400 * 4   <a href="http://union-click.jd.com/jdc?d=kNxgkP"> https://item.jd.com/10583752777.html</a>
</pre>
<p>这台深度学习主机大概是这样的：</p>
<p><a href="http://www.52nlp.cn/?p=10334"><img src="52nlp.cn_files/1127162545-768x1024.jpg" alt="深度学习主机" class="aligncenter size-large wp-image-10352" srcset="http://www.52nlp.cn/wp-content/uploads/2018/06/1127162545-768x1024.jpg 768w, http://www.52nlp.cn/wp-content/uploads/2018/06/1127162545-225x300.jpg 225w, http://www.52nlp.cn/wp-content/uploads/2018/06/1127162545-624x832.jpg 624w, http://www.52nlp.cn/wp-content/uploads/2018/06/1127162545.jpg 1080w" sizes="(max-width: 625px) 100vw, 625px" width="625" height="833"></a></p>
<p>安装完Ubuntu16.04之后，我又开始了CUDA、cuDnn等深度学习环境和工具的安装之旅，时隔大半年，又有了很多变化，特别是CUDA9.x和cuDnn7.x已经成了标配，这里记录一下。</p>
<p><strong>安装CUDA9.x</strong></p>
<p><strong>注：如果还需要安装Tensorflow1.8，建议这里安装CUDA9.0，我在另一台机器上遇到了一点问题，怀疑和我这台机器先安装CUDA9.0，再安装CUDA9.2有关。</strong></p>
<p>依然从英伟达官方下载当前的<a href="https://developer.nvidia.com/cuda-downloads" rel="noopener" target="_blank">CUDA版本</a>，我选择了最新的CUDA9.2：</p>
<p><img src="52nlp.cn_files/-2018-06-16-5_002.png" alt="" class="aligncenter size-full wp-image-10358" srcset="http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.41.33.png 867w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.41.33-300x174.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.41.33-768x446.png 768w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.41.33-624x362.png 624w" sizes="(max-width: 867px) 100vw, 867px" width="867" height="503"></p>
<p>点选完对应Ubuntu16.04的CUDA9.2 deb版本之后，英伟达官方主页会给出安装提示：</p>
<blockquote><p>Installation Instructions:<br>
`sudo dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb`<br>
`sudo apt-key add /var/cuda-repo-<version>/7fa2af80.pub`<br>
`sudo apt-get update`<br>
`sudo apt-get install cuda`</version></p></blockquote>
<p>在下载完大概1.2G的cuda deb版本之后，实际安装命令是这样的：</p>
<pre>sudo dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb
sudo apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub
sudo apt-get update
sudo apt-get install cuda</pre>
<p>官方CUDA下载下载页面还附带了一个cuBLAS 9.2 Patch更新，官方强烈建议安装: </p>
<blockquote><p>This update includes fix to cublas GEMM APIs on V100 
Tensor Core GPUs when used with default algorithm 
CUBLAS_GEMM_DEFAULT_TENSOR_OP. We strongly recommend installing this 
update as part of CUDA Toolkit 9.2 installation.</p></blockquote>
<p>可以用如下方式安装这个Patch更新：</p>
<pre>sudo dpkg -i cuda-repo-ubuntu1604-9-2-local-cublas-update-1_1.0-1_amd64.deb 
sudo apt-get update  
sudo apt-get upgrade cuda</pre>
<p>CUDA9.2安装完毕之后，1080TI的显卡驱动也附带安装了，可以重启机器，然后用 nvidia-smi 命令查看一下：</p>
<p><a href="http://www.52nlp.cn/?p=10334"><img src="52nlp.cn_files/-2018-06-16-5_003.png" alt="" class="aligncenter size-full wp-image-10362" srcset="http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.42.37.png 648w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.42.37-300x156.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.42.37-624x324.png 624w" sizes="(max-width: 648px) 100vw, 648px" width="648" height="336"></a></p>
<p>最后在在 ~/.bashrc 中设置环境变量：</p>
<pre>export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
export CUDA_HOME=/usr/local/cuda
</pre>
<p>运行 source ~/.bashrc 使其生效。</p>
<p><strong>安装cuDNN7.x</strong></p>
<p>同样去英伟达官网的cuDNN下载页面：<a href="https://developer.nvidia.com/rdp/cudnn-download" rel="noopener" target="_blank">https://developer.nvidia.com/rdp/cudnn-download</a>，最新版本是cuDNN7.1.4，有三个版本可以选择，分别面向CUDA8.0, CUDA9.0, CUDA9.2:</p>
<p><a href="http://www.52nlp.cn/?p=10334"><img src="52nlp.cn_files/-2018-06-16-5.png" alt="cudnn7.1.4 cuda9.2 ubuntu16.04" class="aligncenter size-full wp-image-10367" srcset="http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.40.25.png 723w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.40.25-300x185.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/06/屏幕快照-2018-06-16-下午5.40.25-624x386.png 624w" sizes="(max-width: 723px) 100vw, 723px" width="723" height="447"></a></p>
<p>下载完cuDNN7.1的压缩包之后解压，然后将相关文件拷贝到cuda的系统路径下即可：</p>
<pre>tar -zxvf cudnn-9.2-linux-x64-v7.1.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda/include/
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -d 
sudo chmod a+r /usr/local/cuda/include/cudnn.h
sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</pre>
<p><strong>安装TensorFlow 1.8</strong></p>
<p>TensorFlow的安装变得越来越简单，现在TensorFlow的官网也有中文安装文档了：<a href="https://www.tensorflow.org/install/install_linux?hl=zh-cn" rel="noopener" target="_blank">https://www.tensorflow.org/install/install_linux?hl=zh-cn</a> , 我们Follow这个文档，用Virtualenv的安装方式进行TensorFlow的安装，不过首先要配置一下基础环境。</p>
<p>首先在Ubuntu16.04里安装 libcupti-dev 库：</p>
<blockquote><p>这是 NVIDIA CUDA 分析工具接口。此库提供高级分析支持。要安装此库，请针对 CUDA 工具包 8.0 或更高版本发出以下命令：</p>
<p>$ sudo apt-get install cuda-command-line-tools<br>
并将其路径添加到您的 LD_LIBRARY_PATH 环境变量中：</p>
<p>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64<br>
对于 CUDA 工具包 7.5 或更低版本，请发出以下命令：</p>
<p>$ sudo apt-get install libcupti-dev</p></blockquote>
<p>然而我运行“sudo apt-get install cuda-command-line-tools”命令后得到的却是：</p>
<blockquote><p>E: 无法定位软件包 cuda-command-line-tools</p></blockquote>
<p>Google后发现其实在安装CUDA9.2的时候，这个包已经安装了，在CUDA的路径下这个库已经有了：</p>
<pre>/usr/local/cuda/extras/CUPTI/lib64$ ls
libcupti.so  libcupti.so.9.2  libcupti.so.9.2.88
</pre>
<p>现在只需要将其加入到环境变量中，在~/.bashrc中添加如下声明并令source ~/.bashrc另其生效即可：</p>
<pre>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64
</pre>
<p>剩下的就更简单了：</p>
<pre>sudo apt-get install python-pip python-dev python-virtualenv 
virtualenv --system-site-packages tensorflow1.8
source tensorflow1.8/bin/activate
easy_install -U pip
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade tensorflow-gpu
</pre>
<p>强烈建议将<a href="https://mirror.tuna.tsinghua.edu.cn/help/pypi/" rel="noopener" target="_blank">清华的pip源</a>写到配置文件里，这样就更方便快捷了。</p>
<p>最后测试一下TensorFlow1.8:</p>

<div class="wp_syntax" style="position:relative;"><table><tbody><tr><td class="code"><pre class="python" style="font-family:monospace;">Python 2.7.12 <span style="color: black;">(</span>default<span style="color: #66cc66;">,</span> Dec  <span style="color: #ff4500;">4</span> <span style="color: #ff4500;">2017</span><span style="color: #66cc66;">,</span> <span style="color: #ff4500;">14</span>:<span style="color: #ff4500;">50</span>:<span style="color: #ff4500;">18</span><span style="color: black;">)</span> 
<span style="color: black;">[</span>GCC 5.4.0 <span style="color: #ff4500;">20160609</span><span style="color: black;">]</span> on linux2
Type <span style="color: #483d8b;">"help"</span><span style="color: #66cc66;">,</span> <span style="color: #483d8b;">"copyright"</span><span style="color: #66cc66;">,</span> <span style="color: #483d8b;">"credits"</span> <span style="color: #ff7700;font-weight:bold;">or</span> <span style="color: #483d8b;">"license"</span> <span style="color: #ff7700;font-weight:bold;">for</span> more information.
<span style="color: #66cc66;">&gt;&gt;&gt;</span> <span style="color: #ff7700;font-weight:bold;">import</span> tensorflow <span style="color: #ff7700;font-weight:bold;">as</span> tf
<span style="color: #66cc66;">&gt;&gt;&gt;</span> sess <span style="color: #66cc66;">=</span> tf.<span style="color: black;">Session</span><span style="color: black;">(</span>config<span style="color: #66cc66;">=</span>tf.<span style="color: black;">ConfigProto</span><span style="color: black;">(</span>log_device_placement<span style="color: #66cc66;">=</span><span style="color: #008000;">True</span><span style="color: black;">)</span><span style="color: black;">)</span>
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">34.158680</span>: I tensorflow/core/<span style="color: #dc143c;">platform</span>/cpu_feature_guard.<span style="color: black;">cc</span>:<span style="color: #ff4500;">140</span><span style="color: black;">]</span> Your CPU supports instructions that this TensorFlow binary was <span style="color: #ff7700;font-weight:bold;">not</span> compiled to use: AVX2 FMA
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">34.381812</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1356</span><span style="color: black;">]</span> Found device <span style="color: #ff4500;">0</span> <span style="color: #ff7700;font-weight:bold;">with</span> properties: 
name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti major: <span style="color: #ff4500;">6</span> minor: <span style="color: #ff4500;">1</span> memoryClockRate<span style="color: black;">(</span>GHz<span style="color: black;">)</span>: <span style="color: #ff4500;">1.582</span>
pciBusID: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">05</span>:<span style="color: #ff4500;">00.0</span>
totalMemory: 10.91GiB freeMemory: 5.53GiB
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">34.551451</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1356</span><span style="color: black;">]</span> Found device <span style="color: #ff4500;">1</span> <span style="color: #ff7700;font-weight:bold;">with</span> properties: 
name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti major: <span style="color: #ff4500;">6</span> minor: <span style="color: #ff4500;">1</span> memoryClockRate<span style="color: black;">(</span>GHz<span style="color: black;">)</span>: <span style="color: #ff4500;">1.582</span>
pciBusID: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">06</span>:<span style="color: #ff4500;">00.0</span>
totalMemory: 10.92GiB freeMemory: 5.80GiB
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">34.780350</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1356</span><span style="color: black;">]</span> Found device <span style="color: #ff4500;">2</span> <span style="color: #ff7700;font-weight:bold;">with</span> properties: 
name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti major: <span style="color: #ff4500;">6</span> minor: <span style="color: #ff4500;">1</span> memoryClockRate<span style="color: black;">(</span>GHz<span style="color: black;">)</span>: <span style="color: #ff4500;">1.582</span>
pciBusID: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">09</span>:<span style="color: #ff4500;">00.0</span>
totalMemory: 10.92GiB freeMemory: 5.80GiB
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">34.959199</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1356</span><span style="color: black;">]</span> Found device <span style="color: #ff4500;">3</span> <span style="color: #ff7700;font-weight:bold;">with</span> properties: 
name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti major: <span style="color: #ff4500;">6</span> minor: <span style="color: #ff4500;">1</span> memoryClockRate<span style="color: black;">(</span>GHz<span style="color: black;">)</span>: <span style="color: #ff4500;">1.582</span>
pciBusID: <span style="color: #ff4500;">0000</span>:0a:<span style="color: #ff4500;">00.0</span>
totalMemory: 10.92GiB freeMemory: 5.80GiB
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">34.966403</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1435</span><span style="color: black;">]</span> Adding visible gpu devices: <span style="color: #ff4500;">0</span><span style="color: #66cc66;">,</span> <span style="color: #ff4500;">1</span><span style="color: #66cc66;">,</span> <span style="color: #ff4500;">2</span><span style="color: #66cc66;">,</span> <span style="color: #ff4500;">3</span>
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.373745</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">923</span><span style="color: black;">]</span> Device interconnect StreamExecutor <span style="color: #ff7700;font-weight:bold;">with</span> strength <span style="color: #ff4500;">1</span> edge matrix:
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.373785</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">929</span><span style="color: black;">]</span>      <span style="color: #ff4500;">0</span> <span style="color: #ff4500;">1</span> <span style="color: #ff4500;">2</span> <span style="color: #ff4500;">3</span> 
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.373798</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">942</span><span style="color: black;">]</span> <span style="color: #ff4500;">0</span>:   N Y Y Y 
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.373804</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">942</span><span style="color: black;">]</span> <span style="color: #ff4500;">1</span>:   Y N Y Y 
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.373808</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">942</span><span style="color: black;">]</span> <span style="color: #ff4500;">2</span>:   Y Y N Y 
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.373814</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">942</span><span style="color: black;">]</span> <span style="color: #ff4500;">3</span>:   Y Y Y N 
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.374516</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1053</span><span style="color: black;">]</span> Created TensorFlow device <span style="color: black;">(</span>/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">0</span> <span style="color: #ff7700;font-weight:bold;">with</span> <span style="color: #ff4500;">5307</span> MB memory<span style="color: black;">)</span> -<span style="color: #66cc66;">&gt;</span> physical GPU <span style="color: black;">(</span>device: <span style="color: #ff4500;">0</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">05</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span><span style="color: black;">)</span>
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.444426</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1053</span><span style="color: black;">]</span> Created TensorFlow device <span style="color: black;">(</span>/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">1</span> <span style="color: #ff7700;font-weight:bold;">with</span> <span style="color: #ff4500;">5582</span> MB memory<span style="color: black;">)</span> -<span style="color: #66cc66;">&gt;</span> physical GPU <span style="color: black;">(</span>device: <span style="color: #ff4500;">1</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">06</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span><span style="color: black;">)</span>
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.506340</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1053</span><span style="color: black;">]</span> Created TensorFlow device <span style="color: black;">(</span>/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">2</span> <span style="color: #ff7700;font-weight:bold;">with</span> <span style="color: #ff4500;">5582</span> MB memory<span style="color: black;">)</span> -<span style="color: #66cc66;">&gt;</span> physical GPU <span style="color: black;">(</span>device: <span style="color: #ff4500;">2</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">09</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span><span style="color: black;">)</span>
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.614736</span>: I tensorflow/core/common_runtime/gpu/gpu_device.<span style="color: black;">cc</span>:<span style="color: #ff4500;">1053</span><span style="color: black;">]</span> Created TensorFlow device <span style="color: black;">(</span>/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">3</span> <span style="color: #ff7700;font-weight:bold;">with</span> <span style="color: #ff4500;">5582</span> MB memory<span style="color: black;">)</span> -<span style="color: #66cc66;">&gt;</span> physical GPU <span style="color: black;">(</span>device: <span style="color: #ff4500;">3</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:0a:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span><span style="color: black;">)</span>
Device mapping:
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">0</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">0</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">05</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">1</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">1</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">06</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">2</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">2</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">09</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">3</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">3</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:0a:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
<span style="color: #ff4500;">2018</span>-<span style="color: #ff4500;">06</span>-<span style="color: #ff4500;">17</span> <span style="color: #ff4500;">12</span>:<span style="color: #ff4500;">15</span>:<span style="color: #ff4500;">36.689345</span>: I tensorflow/core/common_runtime/direct_session.<span style="color: black;">cc</span>:<span style="color: #ff4500;">284</span><span style="color: black;">]</span> Device mapping:
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">0</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">0</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">05</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">1</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">1</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">06</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">2</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">2</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:<span style="color: #ff4500;">09</span>:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span>
/job:localhost/replica:<span style="color: #ff4500;">0</span>/task:<span style="color: #ff4500;">0</span>/device:GPU:<span style="color: #ff4500;">3</span> -<span style="color: #66cc66;">&gt;</span> device: <span style="color: #ff4500;">3</span><span style="color: #66cc66;">,</span> name: GeForce GTX <span style="color: #ff4500;">1080</span> Ti<span style="color: #66cc66;">,</span> pci bus <span style="color: #008000;">id</span>: <span style="color: #ff4500;">0000</span>:0a:<span style="color: #ff4500;">00.0</span><span style="color: #66cc66;">,</span> compute capability: <span style="color: #ff4500;">6.1</span></pre></td></tr></tbody></table><p class="theCode" style="display:none;">Python
 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; sess = 
tf.Session(config=tf.ConfigProto(log_device_placement=True))
2018-06-17 12:15:34.158680: I 
tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports 
instructions that this TensorFlow binary was not compiled to use: AVX2 
FMA
2018-06-17 12:15:34.381812: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 
with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:05:00.0
totalMemory: 10.91GiB freeMemory: 5.53GiB
2018-06-17 12:15:34.551451: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 
with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:06:00.0
totalMemory: 10.92GiB freeMemory: 5.80GiB
2018-06-17 12:15:34.780350: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 
with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:09:00.0
totalMemory: 10.92GiB freeMemory: 5.80GiB
2018-06-17 12:15:34.959199: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 3 
with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:0a:00.0
totalMemory: 10.92GiB freeMemory: 5.80GiB
2018-06-17 12:15:34.966403: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible 
gpu devices: 0, 1, 2, 3
2018-06-17 12:15:36.373745: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device 
interconnect StreamExecutor with strength 1 edge matrix:
2018-06-17 12:15:36.373785: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 
2018-06-17 12:15:36.373798: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y 
2018-06-17 12:15:36.373804: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y 
2018-06-17 12:15:36.373808: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y 
2018-06-17 12:15:36.373814: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N 
2018-06-17 12:15:36.374516: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created 
TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 
5307 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 
Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)
2018-06-17 12:15:36.444426: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created 
TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 
5582 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 
Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)
2018-06-17 12:15:36.506340: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created 
TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 
5582 MB memory) -&gt; physical GPU (device: 2, name: GeForce GTX 1080 
Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)
2018-06-17 12:15:36.614736: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created 
TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 
5582 MB memory) -&gt; physical GPU (device: 3, name: GeForce GTX 1080 
Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:1 -&gt; device: 1, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:2 -&gt; device: 2, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:3 -&gt; device: 3, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1
2018-06-17 12:15:36.689345: I 
tensorflow/core/common_runtime/direct_session.cc:284] Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:1 -&gt; device: 1, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:2 -&gt; device: 2, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:3 -&gt; device: 3, name: 
GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1</p></div>

<p><strong>安装Keras2.1.x</strong></p>
<p>Keras的后端支持TensorFlow, Theano, CNTK，在安装完TensorFlow 
GPU版本之后，继续安装Keras非常简单，在TensorFlow的虚拟环境中，直接"pip install 
keras"即可，安装的版本是Keras2.1.6:</p>
<blockquote><p>Installing collected packages: h5py, scipy, pyyaml, keras<br>
Successfully installed h5py-2.7.1 keras-2.1.6 pyyaml-3.12 scipy-1.1.0</p></blockquote>
<p>测试一下：</p>

<div class="wp_syntax" style="position:relative;"><table><tbody><tr><td class="code"><pre class="python" style="font-family:monospace;">Python 2.7.12 <span style="color: black;">(</span>default<span style="color: #66cc66;">,</span> Dec  <span style="color: #ff4500;">4</span> <span style="color: #ff4500;">2017</span><span style="color: #66cc66;">,</span> <span style="color: #ff4500;">14</span>:<span style="color: #ff4500;">50</span>:<span style="color: #ff4500;">18</span><span style="color: black;">)</span> 
<span style="color: black;">[</span>GCC 5.4.0 <span style="color: #ff4500;">20160609</span><span style="color: black;">]</span> on linux2
Type <span style="color: #483d8b;">"help"</span><span style="color: #66cc66;">,</span> <span style="color: #483d8b;">"copyright"</span><span style="color: #66cc66;">,</span> <span style="color: #483d8b;">"credits"</span> <span style="color: #ff7700;font-weight:bold;">or</span> <span style="color: #483d8b;">"license"</span> <span style="color: #ff7700;font-weight:bold;">for</span> more information.
<span style="color: #66cc66;">&gt;&gt;&gt;</span> <span style="color: #ff7700;font-weight:bold;">import</span> keras
Using TensorFlow backend.</pre></td></tr></tbody></table><p class="theCode" style="display:none;">Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import keras
Using TensorFlow backend.</p></div>

<p>注：原创文章，转载请注明出处及保留链接“<a href="http://www.52nlp.cn/">我爱自然语言处理</a>”：<a href="http://www.52nlp.cn/">http://www.52nlp.cn</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8-1080ti-ubuntu16-04-cuda9-2-cudnn7-1-tensorflow-keras">从零开始搭建深度学习服务器: 1080TI四卡并行（Ubuntu16.04+CUDA9.2+cuDNN7.1+TensorFlow+Keras）</a> <a href="http://www.52nlp.cn/?p=10334">http://www.52nlp.cn/?p=10334</a></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8-1080ti-ubuntu16-04-cuda9-2-cudnn7-1-tensorflow-keras" title="13:23" rel="bookmark"><time class="entry-date" datetime="2018-06-17T13:23:04+00:00">2018年06月17号</time></a>。属于<a href="http://www.52nlp.cn/category/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="category tag">深度学习</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/1080ti" rel="tag">1080TI</a>、<a href="http://www.52nlp.cn/tag/1080ti-%e6%98%be%e5%8d%a1" rel="tag">1080TI 显卡</a>、<a href="http://www.52nlp.cn/tag/cuda" rel="tag">CUDA</a>、<a href="http://www.52nlp.cn/tag/cuda9" rel="tag">CUDA9</a>、<a href="http://www.52nlp.cn/tag/cuda9-0" rel="tag">CUDA9.0</a>、<a href="http://www.52nlp.cn/tag/cuda9-0%e4%b8%8b%e8%bd%bd" rel="tag">CUDA9.0下载</a>、<a href="http://www.52nlp.cn/tag/cuda9-0%e5%ae%89%e8%a3%85" rel="tag">CUDA9.0安装</a>、<a href="http://www.52nlp.cn/tag/cuda9-2" rel="tag">cuda9.2</a>、<a href="http://www.52nlp.cn/tag/cuda9%e4%b8%8b%e8%bd%bd" rel="tag">CUDA9下载</a>、<a href="http://www.52nlp.cn/tag/cuda9%e5%ae%89%e8%a3%85" rel="tag">CUDA9安装</a>、<a href="http://www.52nlp.cn/tag/cuda%e4%b8%8b%e8%bd%bd" rel="tag">CUDA下载</a>、<a href="http://www.52nlp.cn/tag/cuda%e5%ae%89%e8%a3%85" rel="tag">CUDA安装</a>、<a href="http://www.52nlp.cn/tag/cudnn" rel="tag">cuDNN</a>、<a href="http://www.52nlp.cn/tag/cudnn7" rel="tag">cuDNN7</a>、<a href="http://www.52nlp.cn/tag/cudnn7-0" rel="tag">cuDNN7.0</a>、<a href="http://www.52nlp.cn/tag/cudnn7-0%e4%b8%8b%e8%bd%bd" rel="tag">cuDNN7.0下载</a>、<a href="http://www.52nlp.cn/tag/cudnn7-0%e5%ae%89%e8%a3%85" rel="tag">cuDNN7.0安装</a>、<a href="http://www.52nlp.cn/tag/cudnn7-1" rel="tag">cuDNN7.1</a>、<a href="http://www.52nlp.cn/tag/cudnn7-x" rel="tag">cuDNN7.x</a>、<a href="http://www.52nlp.cn/tag/cudnn7%e5%ae%89%e8%a3%85" rel="tag">cuDNN7安装</a>、<a href="http://www.52nlp.cn/tag/cudnn%e4%b8%8b%e8%bd%bd" rel="tag">cuDNN下载</a>、<a href="http://www.52nlp.cn/tag/cudnn%e5%ae%89%e8%a3%85" rel="tag">cuDNN安装</a>、<a href="http://www.52nlp.cn/tag/ducda9-x" rel="tag">ducda9.x</a>、<a href="http://www.52nlp.cn/tag/gtx-1080ti" rel="tag">GTX 1080TI</a>、<a href="http://www.52nlp.cn/tag/keras" rel="tag">Keras</a>、<a href="http://www.52nlp.cn/tag/keras2-1" rel="tag">Keras2.1</a>、<a href="http://www.52nlp.cn/tag/tensorflow" rel="tag">TensorFlow</a>、<a href="http://www.52nlp.cn/tag/tensorflow1-8" rel="tag">TensorFlow1.8</a>、<a href="http://www.52nlp.cn/tag/ubuntu" rel="tag">ubuntu</a>、<a href="http://www.52nlp.cn/tag/ubuntu16-04" rel="tag">Ubuntu16.04</a>、<a href="http://www.52nlp.cn/tag/%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e6%90%ad%e5%bb%ba%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba" rel="tag">从零开始搭建深度学习主机</a>、<a href="http://www.52nlp.cn/tag/%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e6%90%ad%e5%bb%ba%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%b7%a5%e4%bd%9c%e7%ab%99" rel="tag">从零开始搭建深度学习工作站</a>、<a href="http://www.52nlp.cn/tag/%e4%bb%8e%e9%9b%b6%e5%bc%80%e5%a7%8b%e6%90%ad%e5%bb%ba%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8" rel="tag">从零开始搭建深度学习服务器</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">深度学习</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba" rel="tag">深度学习主机</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%badiy" rel="tag">深度学习主机DIY</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%98%be%e5%8d%a1" rel="tag">深度学习显卡</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8" rel="tag">深度学习服务器</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8%e6%90%ad%e5%bb%ba" rel="tag">深度学习服务器搭建</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%bb%84%e5%bb%ba" rel="tag">深度学习服务器组建</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%ba%e5%99%a8" rel="tag">深度学习机器</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/admin" title="查看所有由52nlp发布的文章" rel="author">52nlp</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10218" class="post-10218 post type-post status-publish format-standard hentry category-machine-translation category-nlp tag-ai tag-ai-challenger tag-ai-challenger-2017 tag-aipatent tag-attention-is-all-you-need tag-bleu tag-bpe tag-gnmt tag-google tag-jieba tag-lstm tag-lua tag-nmt tag-opennmt tag-opennmt-py tag-punctuator tag-punctuator2 tag-pytorch tag-rnn tag-subword tag-tensor2tensor tag-tensorflow tag-tensorflow-nmt tag-tensorflow-tensor2tensor tag-torch tag-transformer tag-1558 tag-1556 tag-1557 tag-1555 tag-82 tag-1553 tag-1552 tag-1530 tag-343 tag-1529 tag-1528 tag-1523 tag-1554 tag-6 tag-1524 tag-1525 tag-1018 tag-513 tag-1533 tag-1522 tag-31 tag-1549 tag-1526 tag-1527 tag-1532 tag-1531">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/ai-challenger-2017-%e5%a5%87%e9%81%87%e8%ae%b0" rel="bookmark">AI Challenger 2017 奇遇记</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/ai-challenger-2017-%e5%a5%87%e9%81%87%e8%ae%b0#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p><strong>本文记录一下去年下半年参加的AI Challenger比赛的过程，有那么一点意思，之所以说是奇遇，看完文章就明白了。</strong></p>
<p>去年8月，由创新工场、搜狗、今日头条联合举办的“<a href="https://challenger.ai/" rel="noopener" target="_blank">AI challenger全球AI挑战赛</a>”首届比赛正式开赛。比赛共设6个赛道，包括英中机器同声传译、英中机器文本翻译、场景分类、图像中文描述、人体骨骼关键点预测以及虚拟股票趋势预测，一时汇集了众多关注的目光：</p>
<blockquote><p>“AI Challenger 
全球AI挑战赛”是面向全球人工智能（AI）人才的开放数据集和编程竞赛平台，致力于打造大型、全面的科研数据集与世界级竞赛平台，从科研角度出发，满足
学术界对高质量数据集的需求，推进人工智能在科研与商业领域的结合，促进世界范围内人工智能研发人员共同探索前沿领域的技术突破及应用创新。在2017年
的首届大赛中，AI 
Challenger发布了千万量级的机器翻译数据集、百万量级的计算机视觉数据集，一系列兼具学术前沿性和产业应用价值的竞赛以及超过200万人民币的
奖金，吸引了来自全球65个国家的8892支团队参赛，成为目前国内规模最大的科研数据集平台、以及最大的非商业化竞赛平台。 AI 
Challenger以服务、培养AI高端人才为使命，打造良性可持续的AI科研新生态。</p></blockquote>
<p>不过AI Challenger 最吸引我的不是每项比赛数十万元的奖金（这个掂量一下也拿不到），而是英中<a href="http://www.52nlp.cn/category/machine-translation" rel="noopener" target="_blank">机器翻译</a>提供的高达1千万的中英双语句对语料，这个量级，在开放的中英语料里仅次于联合国平行语料库，相当的有诱惑力：</p>
<blockquote><p>简介<br>
英中机器文本翻译作为此次比赛的任务之一，目标是评测各个团队机器翻译的能力。本次机器翻译语言方向为英文到中文。测试文本为口语领域数据。参赛队伍需要
根据评测方提供的数据训练机器翻译系统，可以自由的选择机器翻译技术。例如，基于规则的翻译技术、统计机器翻译及神经网络机器翻译等。参赛队伍可以使用系
统融合技术，但是系统融合系统不参与排名。需要指出，神经网络机器翻译常见的Ensemble方法，本次评测不认定为系统融合技术。</p>
<p>数据说明<br>
我们将所有数据分割成为训练集、验证集和测试集合。我们提供了超过1000万的英中对照的句子对作为数据集合。其中，训练集合占据绝大部分，验证集合
8000对，测试集A 8000条，测试集B 
8000条。训练数据主要来源于英语学习网站和电影字幕，领域为口语领域。所有双语句对经过人工检查，数据集从规模、相关度、质量上都有保障。一个英中对
照的句子对，包含一句英文和一句中文文本，中文句子由英文句子人工翻译而成。中英文句子分别保存到两个文件中，两个文件中的中英文句子以行号形成一一对应
的关系。验证集和测试集最终是以标准的XML格式发布给参赛方。</p>
<p>训练条件<br>
本次评测只允许参赛方使用使用评测方指定的数据训练机器翻译系统，并对其排名。参赛方需遵守以下关于训练方式的说明。参赛方可以使用基本的自然语言处理工具，例如中文分词和命名实体识别。</p></blockquote>
<p>大概十年前我读研期间做得是<a href="http://www.52nlp.cn/tag/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91" rel="noopener" target="_blank">统计机器翻译</a>，那个时候能接触到的中英句对最多到过2、3百万，用得最多的工具是知名的开源统计机器翻译工具<a href="http://www.52nlp.cn/tag/moses" rel="noopener" target="_blank">Moses</a>，也在这里写了不少相关的文章。后来工作先后从事过机器翻译、广告文本挖掘相关的工作，与<a href="http://www.52nlp.cn/category/machine-translation" rel="noopener" target="_blank">机器翻译</a>渐行渐远。这一两年，我花了很多时间在专利数据挖掘上，深知<a href="http://fanyi.aipatent.com/" rel="noopener" target="_blank">专利数据翻译</a>的重要性，也了解到机器翻译对于<a href="http://fanyi.aipatent.com/" rel="noopener" target="_blank">专利翻译</a>有天然的吸引力。加之这几年来<a href="http://www.52nlp.cn/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" rel="noopener" target="_blank">深度学习</a>如火如荼，神经网络机器翻译横空出世，Google, 微软，Facebook等公司关于机器翻译的PR一浪高过一浪，大有“取代”人翻译的感觉，这些都都给了我很大的触动，但是一直没有机会走进<a href="http://www.52nlp.cn/tag/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">神经网络机器翻译</a>。刚好这个时候自己又在家里重新组了一台1080TI<a href="http://www.52nlp.cn/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%8D%E5%8A%A1%E5%99%A8-%E7%A1%AC%E4%BB%B6%E9%80%89%E6%8B%A9" rel="noopener" target="_blank">深度学习主机</a>，加上AI Challenger提供的机器翻译数据机会，我把这次参赛的目标定为：</p>
<ul>
<li>了解目前神经网络机器翻译NMT的发展趋势</li>
<li>学习并调研相关的NMT开源工具</li>
<li>将NMT应用在中英日三语之间的专利翻译产品上</li>
</ul>
<p>相对于统计机器翻译，神经网络机器翻译的开源工具更加丰富，这也和最近几年深度学习开源平台遍地开花有关，每个深度学习平台基本上都附有一两个典型
的神经网络机器翻译工具和例子。不过需要说明的是，以下这些关于NMT工具的记录大多数是去年9月到12月期间的调研，很多神经网络机器翻译工具还在不断
的迭代和演讲中，下面的一些描述可能都有了变化。</p>
<p>虽然之前也或多或少的碰到过一些NMT工具，但是这一次我的神经网络机器翻译开源工具之旅是从<a href="http://opennmt.net/" rel="noopener" target="_blank">OpenNMT</a>开启的，这个开源NMT工具由哈佛NLP组推出，诞生于2016年年末，不过主版本基于Torch, 默认语言是Lua，对于喜爱Python的我来说还不算太方便。所以首先尝试了OpenNMT的Pytorch版本: <a href="https://github.com/OpenNMT/OpenNMT-py" rel="noopener" target="_blank">OpenNMT-py</a>，用AI Challenger官方平台提供中英翻译句对中的500万句对迅速跑了一个OpenNMT-py的默认模型：</p>
<blockquote><p>Step 2: Train the model<br>
python train.py -data data/demo -save_model demo-model<br>
The main train command is quite simple. Minimally it takes a data file 
and a save file. This will run the default model, which consists of a 
2-layer LSTM with 500 hidden units on both the encoder/decoder. </p></blockquote>
<p>然后走了一遍AI Challenger的比赛流程，第一次提交记录如下：</p>
<p><strong>2017.09.26 第一次提交：训练数据500万， opennmt-py， default，线下验证集结果：0.2325，线上提交测试集结果：0.22670 </strong></p>
<p>走完了比赛流程，接下来我要认真的审视这次英中机器翻译比赛了，在第二轮训练模型开始前，我首先对数据做了标准化的预处理：</p>
<ol>
<li>数据shuf之后选择了8000句对作为开发集，8000句对作为测试集，剩下的980多万句对作为训练集； </li>
<li>英文数据按照统计机器翻译工具<a href="http://www.statmt.org/moses/?n=Moses.Baseline" rel="noopener" target="_blank">Moses </a>的预处理流程进行了tokenize和truecase；中文数据直接用<a href="https://github.com/fxsjy/jieba" rel="noopener" target="_blank">Jieba</a>中文分词工具进行分词；</li>
</ol>
<p>这一次我将目光瞄准了Google的NMT系统：GNMT， Google的Research Blog是一个好地方: <a href="https://ai.googleblog.com/2017/07/building-your-own-neural-machine.html" rel="noopener" target="_blank">Building Your Own Neural Machine Translation System in TensorFlow</a>，我从这篇文章入手，然后学习使用Tensorflow的NMT开源工具: <a href="https://github.com/tensorflow/nmt" rel="noopener" target="_blank">Tensorflow-NMT</a>，第一次使用<a href="https://github.com/rsennrich/subword-nmt" rel="noopener" target="_blank">subword bpe</a>处理数据，训练了一个4层的gnmt英中模型，记录如下：</p>
<p><strong>2017.10.05 第二次提交：训练集988万句对, tf-nmt, gnmt-4-layer，bpe16000, 线下验证集结果0.2739，线上提交测试集结果：0.26830</strong></p>
<p>这次的结果不错，BLEU值较第一次提交有4个点的提升，我继续尝试使用bpe处理，一周后，做了第三次提交：</p>
<p><strong>2017.10.12 第三次提交：训练集988万句对，tf-nmt, gnmt-4-layer，bpe32000, 线下验证集结果0.2759，线上提交测试集结果：0.27180</strong></p>
<p>依然有一些提高，不过幅度不大。这一次，为了调研各种NMT开源工具，我又把目光锁定到<a href="http://opennmt.net/" rel="noopener" target="_blank">OpenNMT</a>，事实上，到目前为止，接触到的几个神经网络机器翻译开源工具中，和统计机器翻译开源工具Moses最像的就是OpenNMT，有自己独立的官网，文档相当详细，论坛活跃度很高，并且有不同的分支版本，包括主版本 <a href="https://github.com/OpenNMT/OpenNMT" rel="noopener" target="_blank">OpenNMT-lua</a>, Pytorch版本 <a href="https://github.com/OpenNMT/OpenNMT-py" rel="noopener" target="_blank">OpenNMT-py</a>, TensorFlow版本 <a href="https://github.com/OpenNMT/OpenNMT-tf" rel="noopener" target="_blank">OpenNMT-tf</a> 。所以为了这次实验我在深度学习主机中安装了Torch和OpenNMT-lua版本，接下来半个月做了两次OpenNMT训练英中神经网络翻译模型的尝试，不过在验证集的结果和上面的差不多或者略低，没有实质性提高，所以我放弃了这两次提交。</p>
<p>也在这个阶段，从不同途径了解到Google新推的Transformer模型很牛，依然从Google Research Blog入手：<a href="http://Transformer: A Novel Neural Network Architecture for Language Understanding" rel="noopener" target="_blank">Transformer: A Novel Neural Network Architecture for Language Understanding</a> ，学习这篇神文：《<a href="https://arxiv.org/abs/1706.03762" rel="noopener" target="_blank">Attention Is All You Need</a>》 和尝试相关的Transformer开源工具 <a href="https://github.com/tensorflow/tensor2tensor/" rel="noopener" target="_blank">TensorFlow-Tensor2Tensor</a>。一图胜千言，谷歌AI博客上给得这个图片让人无比期待，不过实际操作中还是踩了很多坑：</p>
<p><img src="52nlp.cn_files/transformer.png" alt="" class="aligncenter size-full wp-image-10265" srcset="http://www.52nlp.cn/wp-content/uploads/2018/06/transformer.png 640w, http://www.52nlp.cn/wp-content/uploads/2018/06/transformer-300x186.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/06/transformer-624x386.png 624w" sizes="(max-width: 640px) 100vw, 640px" width="640" height="396"></p>
<p>还是和之前学习使用开源工具的方法类似，我第一次的目标主要是走通tensor2tensor，所以跑了一个 wmt32k base_single 的英中transformer模型，不过结果一般，记录如下：</p>
<p><strong>2017.11.03 第六次实验：t2t transformer wmt32k base_single, 线下验证集BLEU: 0.2605，未提交</strong></p>
<p>之后我又换为wmt32k big_single的设置，再次训练英中transformer模型，这一次，终于在线下验证集的BLEU值上，达到了之前GNMT最好的结果，所以我做了第四次线上提交，不过测试集A的结果还略低一些，记录如下：</p>
<p><strong>2017.11.06 第七次实验：t2t transformer wmt32k big_single，线下验证集结果 0.2759， 线上测试集得分：0.26950</strong></p>
<p>不过这些结果和博客以及论文里宣称的结果相差很大，我开始去检查差异点，包括tensor2tensor的issue以及论文，其实论文里关于实验的部分交代的很清楚：</p>
<blockquote><p>On the WMT 2014 English-to-German translation task, the 
big transformer model (Transformer (big) in Table 2) outperforms the 
best previously reported models (including ensembles) by more than 2.0 
BLEU, establishing a new state-of-the-art BLEU score of 28.4. The 
configuration of this model is listed in the bottom line of Table 3. 
Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all
 previously published models and ensembles, at a fraction of the 
training cost of any of the competitive models.</p>
<p>On the WMT 2014 English-to-French translation task, our big model 
achieves a BLEU score of 41.0, outperforming all of the previously 
published single models, at less than 1/4 the training cost of the 
previous state-of-the-art model. The Transformer (big) model trained for
 English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.</p>
<p>For the base models, we used a single model obtained by averaging the
 last 5 checkpoints, which were written at 10-minute intervals. For the 
big models, we averaged the last 20 checkpoints. We used beam search 
with a beam size of 4 and length penalty α = 0.6 . These hyperparameters
 were chosen after experimentation on the development set. We set the 
maximum output length during inference to input length + 50, but 
terminate early when possible.</p></blockquote>
<p>总结起来有2个地方可以改进：第一，是对checkpoints进行average, 这个效果立竿见影：</p>
<p><strong>2017.11.07 第八次实验：t2t transformer wmt32k big_single average model, 线下验证集得分 0.2810 , 提交测试集得分：0.27330</strong></p>
<p>第二，要有高性能的<a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E6%94%92%E6%9C%BA%E5%B0%8F%E8%AE%B0" rel="noopener" target="_blank">深度学习服务器</a>。谷歌实验中最好的结果是在8块 P100 GPU的机器上训练了3.5天，对我的单机1080TI<a href="http://www.52nlp.cn/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%9C%BA%E6%94%92%E6%9C%BA%E5%B0%8F%E8%AE%B0" rel="noopener" target="_blank">深度学习主机</a>来说，一方面训练时对参数做了取舍，另一方面用时间换空间，尝试增加训练步数，直接将训练步数增加到100万次，结果还是不错的：</p>
<p><strong>2017.11.15 第九次实验：t2t transformer wmt32k big_single 1000k 10beam，线下验证集得分0.2911，线上提交测试集得分0.28560</strong></p>
<p>然后继续average checkpoints:<br>
<strong>2017.11.16 第十次提交： t2t transformer wmt32k big_single 1000k average 10beam, 线下验证集得分0.2930，线上提交测试集得分0.28780</strong></p>
<p>这两个方法确实能有效提高BLEU值，所以我继续沿用这个策略，按着训练时间推算了一下，估计这台机器在12月初比赛正式结束前大概可以训练一个
250万次的模型，当然，这个给自己预留了最后提交比赛结果的时间。不过在11月27日，我在英中机器翻译比赛测试集A结束提交前提交了一个训练了140
万次，并做了模型average的提交，算是这个赛道Test A关闭前的最后一次提交：</p>
<p><strong>2017.11.27 第十一次提交 t2t transformer wmt32k big_single 1400k.beam10.a0.9.average， 验证集 0.2938 测试集 0.28950</strong></p>
<p>12月1日凌晨测试集B正式放出，这个是最终排名的重要依据，只有2次提交机会，并且结果不会实时更新，只有等到12月3号之后才会放出最终排名。我的英中2500k Transformer模型大概在12月2号训练完毕，我做了Test B的第一次提交：</p>
<p><strong>2017.12.2 average b10 a0.9: 0.2972(验证集） </strong></p>
<p>之后，我逐一检查了保留的20个checkpoint在验证集上的得分，最终选择了高于平均值的11个checkpoint的average又做了第二次提交，虽然验证集只高了0.0001, 但是在这样的比赛中，“蚊子肉也是肉啊”：</p>
<p><strong>2017.12.3 average select 11 b10 a0.9: 0.2973(验证集)</strong></p>
<p>这就是我在英中机器文本翻译比赛中的整个历程，在Test 
A的最终排名大概在二十几名，但是最后一次模型的结果应该还能提高，所以预期是前20，剩下的就是等待TEST 
B的最终排名结果了。做到这个份上，其实我还挺满意的，不过故事如果真的到此就结束了，那算不上奇遇，有意思的事情才刚开始。</p>
<p>AI Challenger 
2017有两个赛道和机器翻译有关，一个是英中机器文本翻译比赛（最高奖金30万），另外一个是英中机器同声传译比赛（最高奖金40万），一开始报名的时
候，直观上觉得后者比较复杂，一方面奖金部分说明了问题，另外赛题描述部分也让人觉得涉及到语音处理，比较复杂：</p>
<blockquote><p>简介<br>
随着最近深度学习在语音、自然语言处理里面的应用，语音识别的错误率在不断降低，机器翻译的效果也在不断提高。语音处理和机器翻译的进步也推动机器同声传
译的进步。如果竞赛任务同时考虑语音识别、机器翻译和语音合成这些项目，参赛队伍遇到的难度会很大。所以本次评测重点也在语音识别后的文本处理和机器翻译
任务。翻译语言方向为英文到中文。</p>
<p>语音识别后处理模块：语音识别后的文本与书面语有很多不同。识别后文本具有（1）包含有识别错误；（2）识别结果没有标点符号；（3）源端为比较长
的句子，例如对40~50s的语音标注后的文本，没有断句；（4）口语化文本，夹杂语气词等特点。由于本次比赛没有提供错误和正确对照的文本用于训练纠错
模块。本次比赛提供的测试集合的源端文本是人工对语音标注后的文本，不包含识别错误。针对其它的特点，参赛队伍可以这几个方面考虑优化，但不限于以下几个
方面：</p>
<p>1. 针对无标点的情况，参赛方可以利用提供的英文单语数据训练自动标点模块。用自动标点模块对测试集合文本进行添加标点。自动标点也属于序列标注任务，选手可以使用统计模型或是神经网络的模型进行建模。</p>
<p>2. 针对断句：源端文本都是比较长的文本，不利于机器翻译，参赛者可以设定断句策略。例如，参赛者可以依据标点来进行断句，将每个小的分句送入机器翻译系统。</p>
<p>3. 针对口语化：参赛队伍可以制定一些去除口语词的规则来处理测试集合。</p>
<p>机器翻译模块：将识别后处理的文本翻译成目标语言。参赛队伍需要根据评测方提供的数据训练机器翻译系统，可以自由的选择机器翻译技术。例如，基于规
则的翻译技术、基于实例的翻译技术、统计机器翻译及神经网络机器翻译等。参赛队伍可以使用系统融合技术，但是系统融合系统不参与排名。</p>
<p>数据说明<br>
机器翻译训练集。我们提供了1000万左右英中对照的句子对作为训练集合。训练数据领域为口语领域。所有双语句对经过人工检查，数据集从规模、相关度、质量上都有保障。一个英中对照的句子对，包含一句英文和一句中文文本，中文句子由英文句子人工翻译而成。</p>
<p>自动标点训练数据。选手可以利用提供的1000万文本训练自动标点系统。</p>
<p>验证集和测试集。我们会分别选取多个英语演讲的题材的音频，总时长在3~6小时之间，然后按照内容切分成30s~50s不等长度的音频数据，人工标
注出音频对应的英文文本。人工标注的文本不翻译识别错误、无标点、含有语气词等。人工标注的好的英文文本会由专业译员翻译成中文文本，就形成了英中对照的
句子对。抽取的英中对照的句子对会被分割为验证集和测试集。验证集和测试集最终是以标准的XML格式提供给选手。</p></blockquote>
<p>我在一开始的时候考虑到这个比赛同样提供上千万句对的语料，所以当时顺手报名了这个同声传译比赛，但是直到最后一刻，我还没有仔细看过或者准备过这
个任务。不过12月2号当我第一次完成英中机器翻译比赛的测试集B提交后，以完成作业的心态了解了一下这个英中机器同传比赛的题意以及数据集，发现这里提
供的训练集和英中机器翻译比赛的数据是一致的，也就是说机器翻译模块可以复用之前训练的英中Transformer模型，而真正需要解决的，是标点符号自
动标注模块以及断句模块。</p>
<p>感谢Google、Github和开源世界，在测试了几个自动标点标注模块后，我把目光锁定在 <a href="https://github.com/ottokart/punctuator2" rel="noopener" target="_blank">punctuator2</a>（A
 bidirectional recurrent neural network model with attention mechanism 
for restoring missing punctuation in unsegmented text）, 
一个带attention机制的双向RNN无标点文本标点符号还原工具，通过它很快的构建了英文文本自动标点标注模块，并且用在了英中机器同声传译比赛的
验证集和测试集上，验证集结果不算太差，所以对应英中机器翻译的模型，我也做了两次测试集B的提交，但是至于结果如何，我根本无法判断，因为在测试集A
上，我没有提交过一次，所以无法判断测试集和验证集的正相关性。但是完成了 AI Challenger 
的相关“作业“，我基本上心满意足了，至于结果如何，Who Care?</p>
<p>大约一个周之后测试集B上的结果揭晓，我在英中机器翻译文本比赛上进了前20，英中同声传译比赛上进了前10，不过前者的参数队伍有150多支，后
者不足30支，特别是测试集B的提交队伍不到15支，有点诡异。原本以为这就结束了，不过到了12月中旬的某个周末，我微信突然收到了AI 
Challenger小助手的催收信息，大意是需要提交什么代码验证，问我为什么一直没有提交？我一脸错愕，她让我赶紧查看邮件，原来早在一个周之前的
12月9号，AI Challenger发了一封邮件，主题是这样的：“AI Challenger 2017 TOP10 选手通知”</p>
<blockquote><p>亲爱的AI Challenger,</p>
<p>恭喜你，过五关斩六将进入了TOP10，进入前十的机率是0.56%，每一位都是千里挑一的人才。非常不容易也非常优秀！  </p>
<p>为了保证竞赛公平公正性，您还需要在12月10日中午12点前按如下格式提交您的代码至大赛核验邮箱aichallenger@chuangxin.com</p>
<p>邮件格式：<br>
主题：AI ChallengerTOP10代码提交-队伍名称-赛道<br>
正文：<br>
队伍名称<br>
全体队员信息：姓名-AI Challenger昵称-电话-邮箱-所在机构-专业&amp;年级</p>
<p> 附件：（文件名称）<br>
1- 代码</p>
<p>非常感谢您的合作。</p></blockquote>
<p>原来测试集B上的前10名同学需要提交代码复核，我原来以为只有前5名需要去北京现场答辩的同学要做这个，没想到前10名都需要做，赶紧和AI 
Challenger小助手沟通了一下，因为自己几乎都是通过开源工具完成的比赛，就简单的提交了一份说明文档过去了。正是在参加AI 
Challenger比赛的同一时期，我们的<a href="http://fanyi.aipatent.com/" rel="noopener" target="_blank">专利机器翻译产品</a>也马不停蹄的开展了，出于对两个赛道前几名队伍BLEU值的仰望，我准备去北京旁听一下现场答辩，所以当天还和AI Challenger小助手沟通了一下现场观摩的问题，小助手说，前十名可以直接来，所以我觉得进入前十名还是不错的。</p>
<p>没想到第二天一早又收到Challenger小助手的微信留言，大意是：你不用自己买票来观摩比赛了，因为前面有几支队伍因种种原因放弃现场答辩，
你自动递补为第5名，需要来北京参加12月21日的现场决赛答辩和颁奖礼，我们给你买机票和定酒店。吃不吃惊？意不意外？我当时的第一反应这真是2017
年本人遇到最奇特的一件事情。。。然后很快收到了一封决赛邀请函：</p>
<blockquote><p>亲爱的AI Challenger,</p>
<p>恭喜你，过五关斩六将走到了决赛，进入决赛的机率是0.28%，每一位都是千里挑一的人才。非常不容易也非常优秀！  </p>
<p>“AI Challenger 
全球AI挑战赛”面向人工智能领域科研人才，致力于打造大型、全面的科研数据集与世界级竞赛平台。由创新工场、搜狗、今日头条联合创建，旨在从科研角度出
发，满足学术界对高质量数据集的需求，推进人工智能在科研与商业领域的结合，促进世界范围内人工智能研发人员共同探索前沿领域的技术突破及应用创新。</p>
<p>2017年是AI Challenger的诞生年，我们公布了百万量级的计算机视觉数据集、千万量级的机器翻译数据集，并主办多条细分赛道的AI竞赛。本次英中机器同传竞赛主要任务为集中优化语音识别后处理和机器翻译模块，旨在解决机器同声传译中的技术问题。</p>
<p>......</p>
<p>恭喜所有的入围选手！所有的入围者将在12月21日到中国北京进行现场答辩，本次大赛将以最终榜单排名结合答辩表现，加权计算总成绩，决出最终的大奖。</p>
<p>在答辩之前，我们需要Top5团队于12月18日下午17点前提交包括：<br>
1-答辩PPT、<br>
2-队员情况（个人姓名、个人高清半身照片、个人学校-年级-专业/公司-部门-职务、是否有指导老师-如有，请附上老师150字内简介）<br>
3-团队出席名单（涉及报销事宜）<br>
4-代码（供审查，如有作弊情况将按大赛规则处理）<br>
5-150字内个人简介-选手手册素材（建议为三段话，第一段话是背景介绍，包括你的学校、实验室、师从老师等信息；第二段话可以介绍你的技术优势，包括Paper、竞赛履历、实习履历、项目经历；第三段话支持自由发挥，个人主页、你的爱好，让我们发现一个独一无二的你）<br>
......</p></blockquote>
<p>虽然去北京参加现场决赛也只是陪太子读书，不过最终还是决定去参加现场答辩，当然这里还有一关需要验证，前10名只需要提交代码或者代码描述即可，
前5名参加决赛的同学还要复现整个流程，我很快被小助手拉入一个小群，里面有来自搜狗的工程师同学，他们给我提供了一台深度学习机器，让我复现整个过程以
及最终核验比赛结果。当然，留给我的时间比较紧张，12月21号要去北京参加现场答辩，当时已经是12月18号了，所以Challenger小助手特地给
我将时间留到了最后一刻。准备PPT和复现整个流程同时进行（复现并不是等于重新训练一遍，譬如机器翻译模型可以直接上传之前训练好的），终于赶在最后时
刻完工。不过我自己答辩现场的感觉匆匆忙忙，效果也一般，但是学习了一圈其他获奖队伍的思路，很有收获：Transformer是主流获奖模型，但是很多
功夫在细节，包括数据预处理阶段的筛选，数据 &amp; 模型后处理的比拼，当然，牛逼的深度学习机器也是不可或缺的。</p>
<p>附上当时现场答辩PPT上写得几点思考，抛砖引玉，欢迎大家一起探讨机器翻译特别是神经网络机器翻译的现状和未来：</p>
<ul>
<li>NMT开源工具的生态问题，这个过程中我们尝试了OpenNMT, OpenNMT-py, OpenNMT-tf, Tensorflow-nmt, Tensor2Tensor等工具, 总体感觉OpenNMT的生态最完备，很像SMT时代的Moses</li>
<li>NMT的工程化和产品化问题，从学术产品到工程产品，还有很多细节要打磨</li>
<li>面向垂直领域的机器翻译：专利机器翻译是一个多领域的机器翻译问题</li>
<li>由衷感谢这些从idea到开源工具都无私奉献的研究者和从业者们，我们只是站在了你们的肩膀上</li>
</ul>
<p>当然，参加完AI Challenger比赛之后我们并没有停止对于神经网络机器翻译应用的探索，也有了一些新的体会。这半年来我们一直在打磨<a href="http://fanyi.aipatent.com/" rel="noopener" target="_blank">AIpatent机器翻译引擎</a>，目标是面向中英专利翻译、中日专利翻译、日英专利翻译提供专业的专利翻译引擎，欢迎有这方面需求的同学试用我们的引擎，目前还在不断迭代中。</p>
<p>注：原创文章，转载请注明出处及保留链接“<a href="http://www.52nlp.cn/">我爱自然语言处理</a>”：<a href="http://www.52nlp.cn/">http://www.52nlp.cn</a></p>
<p>本文链接地址：<a href="http://www.52nlp.cn/ai-challenger-2017-%E5%A5%87%E9%81%87%E8%AE%B0">AI Challenger 2017 奇遇记</a> <a href="http://www.52nlp.cn/?p=10218">http://www.52nlp.cn/?p=10218</a></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/ai-challenger-2017-%e5%a5%87%e9%81%87%e8%ae%b0" title="23:11" rel="bookmark"><time class="entry-date" datetime="2018-06-04T23:11:19+00:00">2018年06月4号</time></a>。属于<a href="http://www.52nlp.cn/category/machine-translation" rel="category tag">机器翻译</a>、<a href="http://www.52nlp.cn/category/nlp" rel="category tag">自然语言处理</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/ai" rel="tag">AI</a>、<a href="http://www.52nlp.cn/tag/ai-challenger" rel="tag">AI Challenger</a>、<a href="http://www.52nlp.cn/tag/ai-challenger-2017" rel="tag">AI Challenger 2017</a>、<a href="http://www.52nlp.cn/tag/aipatent" rel="tag">AIPatent</a>、<a href="http://www.52nlp.cn/tag/aipatent%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91%e5%bc%95%e6%93%8e" rel="tag">AIpatent机器翻译引擎</a>、<a href="http://www.52nlp.cn/tag/attention-is-all-you-need" rel="tag">Attention Is All You Need</a>、<a href="http://www.52nlp.cn/tag/bleu" rel="tag">bleu</a>、<a href="http://www.52nlp.cn/tag/bpe" rel="tag">bpe</a>、<a href="http://www.52nlp.cn/tag/gnmt" rel="tag">GNMT</a>、<a href="http://www.52nlp.cn/tag/google" rel="tag">Google</a>、<a href="http://www.52nlp.cn/tag/jieba" rel="tag">Jieba</a>、<a href="http://www.52nlp.cn/tag/jieba%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d" rel="tag">Jieba中文分词</a>、<a href="http://www.52nlp.cn/tag/lstm" rel="tag">LSTM</a>、<a href="http://www.52nlp.cn/tag/lua" rel="tag">Lua</a>、<a href="http://www.52nlp.cn/tag/nmt" rel="tag">NMT</a>、<a href="http://www.52nlp.cn/tag/opennmt" rel="tag">OpenNMT</a>、<a href="http://www.52nlp.cn/tag/opennmt-py" rel="tag">OpenNMT-py</a>、<a href="http://www.52nlp.cn/tag/punctuator" rel="tag">punctuator</a>、<a href="http://www.52nlp.cn/tag/punctuator2" rel="tag">punctuator2</a>、<a href="http://www.52nlp.cn/tag/pytorch" rel="tag">PyTorch</a>、<a href="http://www.52nlp.cn/tag/rnn" rel="tag">RNN</a>、<a href="http://www.52nlp.cn/tag/subword" rel="tag">subword</a>、<a href="http://www.52nlp.cn/tag/tensor2tensor" rel="tag">Tensor2Tensor</a>、<a href="http://www.52nlp.cn/tag/tensorflow" rel="tag">TensorFlow</a>、<a href="http://www.52nlp.cn/tag/tensorflow-nmt" rel="tag">Tensorflow-nmt</a>、<a href="http://www.52nlp.cn/tag/tensorflow-tensor2tensor" rel="tag">TensorFlow-Tensor2Tensor</a>、<a href="http://www.52nlp.cn/tag/torch" rel="tag">Torch</a>、<a href="http://www.52nlp.cn/tag/transformer" rel="tag">transformer</a>、<a href="http://www.52nlp.cn/tag/transformer%e6%a8%a1%e5%9e%8b" rel="tag">transformer模型</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%93%e5%88%a9%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91" rel="tag">专利机器翻译</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%93%e5%88%a9%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91%e5%bc%95%e6%93%8e" rel="tag">专利机器翻译引擎</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%93%e5%88%a9%e7%bf%bb%e8%af%91" rel="tag">专利翻译</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%93%e5%88%a9%e7%bf%bb%e8%af%91%e5%bc%95%e6%93%8e" rel="tag">专利翻译引擎</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d" rel="tag">中文分词</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%ad%e6%97%a5%e4%b8%93%e5%88%a9%e7%bf%bb%e8%af%91" rel="tag">中日专利翻译</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%ad%e8%8b%b1%e4%b8%93%e5%88%a9%e7%bf%bb%e8%af%91" rel="tag">中英专利翻译</a>、<a href="http://www.52nlp.cn/tag/%e4%ba%ba%e4%bd%93%e9%aa%a8%e9%aa%bc%e5%85%b3%e9%94%ae%e7%82%b9%e9%a2%84%e6%b5%8b" rel="tag">人体骨骼关键点预测</a>、<a href="http://www.52nlp.cn/tag/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd" rel="tag">人工智能</a>、<a href="http://www.52nlp.cn/tag/%e5%9b%be%e5%83%8f%e4%b8%ad%e6%96%87%e6%8f%8f%e8%bf%b0" rel="tag">图像中文描述</a>、<a href="http://www.52nlp.cn/tag/%e5%9c%ba%e6%99%af%e5%88%86%e7%b1%bb" rel="tag">场景分类</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e6%8d%ae%e7%ab%9e%e8%b5%9b" rel="tag">数据竞赛</a>、<a href="http://www.52nlp.cn/tag/%e6%97%a5%e8%8b%b1%e4%b8%93%e5%88%a9%e7%bf%bb%e8%af%91" rel="tag">日英专利翻译</a>、<a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91" rel="tag">机器翻译</a>、<a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91%e6%95%b0%e6%8d%ae%e9%9b%86" rel="tag">机器翻译数据集</a>、<a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91%e8%af%ad%e6%96%99" rel="tag">机器翻译语料</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">深度学习</a>、<a href="http://www.52nlp.cn/tag/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" rel="tag">神经网络</a>、<a href="http://www.52nlp.cn/tag/%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91" rel="tag">神经网络机器翻译</a>、<a href="http://www.52nlp.cn/tag/%e7%ab%9e%e8%b5%9b" rel="tag">竞赛</a>、<a href="http://www.52nlp.cn/tag/%e7%bb%9f%e8%ae%a1%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91" rel="tag">统计机器翻译</a>、<a href="http://www.52nlp.cn/tag/%e8%87%aa%e5%8a%a8%e6%a0%87%e7%82%b9%e6%a0%87%e6%b3%a8%e6%a8%a1%e5%9d%97" rel="tag">自动标点标注模块</a>、<a href="http://www.52nlp.cn/tag/%e8%8b%b1%e4%b8%ad%e6%9c%ba%e5%99%a8%e5%90%8c%e5%a3%b0%e4%bc%a0%e8%af%91" rel="tag">英中机器同声传译</a>、<a href="http://www.52nlp.cn/tag/%e8%8b%b1%e4%b8%ad%e6%9c%ba%e5%99%a8%e6%96%87%e6%9c%ac%e7%bf%bb%e8%af%91" rel="tag">英中机器文本翻译</a>、<a href="http://www.52nlp.cn/tag/%e8%8b%b1%e4%b8%ad%e6%9c%ba%e5%99%a8%e7%bf%bb%e8%af%91" rel="tag">英中机器翻译</a>、<a href="http://www.52nlp.cn/tag/%e8%99%9a%e6%8b%9f%e8%82%a1%e7%a5%a8%e8%b6%8b%e5%8a%bf%e9%a2%84%e6%b5%8b" rel="tag">虚拟股票趋势预测</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/admin" title="查看所有由52nlp发布的文章" rel="author">52nlp</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10198" class="post-10198 post type-post status-publish format-standard hentry category-1490 tag-coursera tag-pca tag-1087 tag-1512 tag-1513 tag-1514 tag-1519 tag-291 tag-1515 tag-1498 tag-1516 tag-1447 tag-1239 tag-1517 tag-1157 tag-1518">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e5%ad%a6%e8%af%be%e7%a8%8b-%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90" rel="bookmark">Coursera上数学类相关课程（公开课）汇总推荐</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e5%ad%a6%e8%af%be%e7%a8%8b-%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p>数学课程是基础，Coursera上有很多数学公开课，这里做个汇总，注意由于Coursera上有一批很有特色的统计学相关的数学课程，我们将在下一期里单独汇总。</p>
<p>1 斯坦福大学 <a href="http://coursegraph.com/coursera-mathematical-thinking" rel="noopener" target="_blank">Introduction to Mathematical Thinking（数学思维导论）</a></p>
<p>http://coursegraph.com/coursera-mathematical-thinking</p>
<p>引用<a href="http://coursegraph.com/coursera_maththink" rel="noopener" target="_blank">老版课程</a>一个同学的评价，供参考：</p>
<blockquote><p>这门课是高中数学到大学数学的一个过度。高中数学一般重计算不太注重证明，这门课讲了基本的逻辑，数学语言（两个 
quantifier，there exists， for 
all）和证明的几个基本方法，比如证明充要条件要从两个方向证、证伪只需要举个反例，原命题不好证的时候可以证等价的逆否命题以及很常用的数学归纳法。
课程讲了数论里一些基本定理，然后通过让你证一些看起来显然而不需要证明的证明题来训练你证明的技能和逻辑思考的能力，看起来显然的命题也是要证明才能说
服人的，课程最后简略的讲了下数学分析里面实数的引入，但这部分讲的不完整。Keith Devlin 是个 old school 
的讲师，上课只用纸和笔，也是属于比较热情的讲师，他每周都会录几个答疑的视频。这门比较适合大一的新生上，开得也比较频繁。</p></blockquote>
<p>课程简介：</p>
<blockquote><p>Learn how to think the way mathematicians do – a powerful
 cognitive process developed over thousands of years. Mathematical 
thinking is not the same as doing mathematics – at least not as 
mathematics is typically presented in our school system. School math 
typically focuses on learning procedures to solve highly stereotyped 
problems. Professional mathematicians think a certain way to solve real 
problems, problems that can arise from the everyday world, or from 
science, or from within mathematics itself. The key to success in school
 math is to learn to think inside-the-box. In contrast, a key feature of
 mathematical thinking is thinking outside-the-box – a valuable ability 
in today’s world. This course helps to develop that crucial way of 
thinking.</p></blockquote>
<p>2 加州大学尔湾分校 初级微积分系列课程</p>
<p>1）<a href="http://coursegraph.com/coursera-pre-calculus" rel="noopener" target="_blank">Pre-Calculus: Functions（初级微积分：函数）</a></p>
<p>http://coursegraph.com/coursera-pre-calculus</p>
<blockquote><p>This course covers mathematical topics in college 
algebra, with an emphasis on functions. The course is designed to help 
prepare students to enroll for a first semester course in single 
variable calculus. Upon completing this course, you will be able to: 1. 
Solve linear and quadratic equations 2. Solve some classes of rational 
and radical equations 3. Graph polynomial, rational, piece-wise, 
exponential and logarithmic functions 4. Find integer roots of 
polynomial equations 5. Solve exponential and logarithm equations 6. 
Understand the inverse relations between exponential and logarithm 
equations 7. Compute values of exponential and logarithm expressions 
using basic properties</p></blockquote>
<p>2）<a href="http://coursegraph.com/coursera-trigonometry" rel="noopener" target="_blank">Pre-Calculus: Trigonometry(初级微积分：三角）</a></p>
<p>http://coursegraph.com/coursera-trigonometry</p>
<blockquote><p>This course covers mathematical topics in trigonometry. 
Trigonometry is the study of triangle angles and lengths, but 
trigonometric functions have far reaching applications beyond simple 
studies of triangles. This course is designed to help prepare students 
to enroll for a first semester course in single variable calculus. Upon 
completing this course, you will be able to: 1. Evaluate trigonometric 
functions using the unit circle and right triangle approaches 2. Solve 
trigonometric equations 3. Verify trigonometric identities 4. Prove and 
use basic trigonometric identities. 5. Manipulate trigonometric 
expressions using standard identities 6. Solve right triangles 7. Apply 
the Law of Sines and the Law of Cosines</p></blockquote>
<p>3 宾夕法尼亚大学的 单变量微积分系列课程</p>
<p>1）<a href="http://coursegraph.com/coursera-single-variable-calculus" rel="noopener" target="_blank">Calculus: Single Variable Part 1 - Functions（单变量微积分1：函数）</a><br>
http://coursegraph.com/coursera-single-variable-calculus</p>
<blockquote><p>Calculus is one of the grandest achievements of human 
thought, explaining everything from planetary orbits to the optimal size
 of a city to the periodicity of a heartbeat. This brisk course covers 
the core ideas of single-variable Calculus with emphases on conceptual 
understanding and applications. The course is ideal for students 
beginning in the engineering, physical, and social sciences. 
Distinguishing features of the course include: 1) the introduction and 
use of Taylor series and approximations from the beginning; 2) a novel 
synthesis of discrete and continuous forms of Calculus; 3) an emphasis 
on the conceptual over the computational; and 4) a clear, dynamic, 
unified approach. In this first part--part one of five--you will extend 
your understanding of Taylor series, review limits, learn the *why* 
behind l'Hopital's rule, and, most importantly, learn a new language for
 describing growth and decay of functions: the BIG O.</p></blockquote>
<p>2）<a href="http://coursegraph.com/coursera-differentiation-calculus" rel="noopener" target="_blank">Calculus: Single Variable Part 2 - Differentiation（单变量微积分2：微分）</a></p>
<p>http://coursegraph.com/coursera-differentiation-calculus</p>
<blockquote><p>Calculus is one of the grandest achievements of human 
thought, explaining everything from planetary orbits to the optimal size
 of a city to the periodicity of a heartbeat. This brisk course covers 
the core ideas of single-variable Calculus with emphases on conceptual 
understanding and applications. The course is ideal for students 
beginning in the engineering, physical, and social sciences. 
Distinguishing features of the course include: 1) the introduction and 
use of Taylor series and approximations from the beginning; 2) a novel 
synthesis of discrete and continuous forms of Calculus; 3) an emphasis 
on the conceptual over the computational; and 4) a clear, dynamic, 
unified approach. In this second part--part two of five--we cover 
derivatives, differentiation rules, linearization, higher derivatives, 
optimization, differentials, and differentiation operators.</p></blockquote>
<p>3）<a href="http://coursegraph.com/coursera-integration-calculus" rel="noopener" target="_blank">Calculus: Single Variable Part 3 - Integration（单变量微积分3：积分）</a></p>
<p>http://coursegraph.com/coursera-integration-calculus</p>
<blockquote><p>Calculus is one of the grandest achievements of human 
thought, explaining everything from planetary orbits to the optimal size
 of a city to the periodicity of a heartbeat. This brisk course covers 
the core ideas of single-variable Calculus with emphases on conceptual 
understanding and applications. The course is ideal for students 
beginning in the engineering, physical, and social sciences. 
Distinguishing features of the course include: 1) the introduction and 
use of Taylor series and approximations from the beginning; 2) a novel 
synthesis of discrete and continuous forms of Calculus; 3) an emphasis 
on the conceptual over the computational; and 4) a clear, dynamic, 
unified approach. In this third part--part three of five--we cover 
integrating differential equations, techniques of integration, the 
fundamental theorem of integral calculus, and difficult integrals.</p></blockquote>
<p>4) <a href="http://coursegraph.com/coursera-applications-calculus" rel="noopener" target="_blank">Calculus: Single Variable Part 4 - Applications（单变量微积分4：应用）</a></p>
<p>http://coursegraph.com/coursera-applications-calculus</p>
<blockquote><p>Calculus is one of the grandest achievements of human 
thought, explaining everything from planetary orbits to the optimal size
 of a city to the periodicity of a heartbeat. This brisk course covers 
the core ideas of single-variable Calculus with emphases on conceptual 
understanding and applications. The course is ideal for students 
beginning in the engineering, physical, and social sciences. 
Distinguishing features of the course include: 1) the introduction and 
use of Taylor series and approximations from the beginning; 2) a novel 
synthesis of discrete and continuous forms of Calculus; 3) an emphasis 
on the conceptual over the computational; and 4) a clear, dynamic, 
unified approach. In this fourth part--part four of five--we cover 
computing areas and volumes, other geometric applications, physical 
applications, and averages and mass. We also introduce probability.</p></blockquote>
<p>4 杜克大学 <a href="http://coursegraph.com/coursera-datasciencemathskills" rel="noopener" target="_blank">Data Science Math Skills（数据科学中的数学技巧）</a></p>
<p>http://coursegraph.com/coursera-datasciencemathskills</p>
<p>这门课程主要介绍数据科学中涉及的相关数学概念，让学生了解基本的数学概念，掌握基本的数学语言，内容涵盖集合论、求和的Sigma符号、数学上的笛卡尔（x，y）平面、指数、对数和自然对数函数，概率论以及叶斯定理等：</p>
<blockquote><p>Data science courses contain math—no avoiding that! This 
course is designed to teach learners the basic math you will need in 
order to be successful in almost any data science math course and was 
created for learners who have basic math skills but may not have taken 
algebra or pre-calculus. Data Science Math Skills introduces the core 
math that data science is built upon, with no extra complexity, 
introducing unfamiliar ideas and math symbols one-at-a-time. Learners 
who complete this course will master the vocabulary, notation, concepts,
 and algebra rules that all data scientists must know before moving on 
to more advanced material. </p></blockquote>
<p>5 加州大学圣迭戈分校 <a href="http://coursegraph.com/coursera-specializations-discrete-mathematics" rel="noopener" target="_blank">Introduction to Discrete Mathematics for Computer Science Specialization（面向计算机科学的离散数学专项课程)</a></p>
<p>http://coursegraph.com/coursera-specializations-discrete-mathematics</p>
<p>面向计算机科学的离散数学专项课程（Introduction to Discrete Mathematics for Computer 
Science 
Specialization），这个系列包含5门子课程，涵盖证明、组合数学与概率、图论，数论和密码学，配送问题项目等，感兴趣的同学可以关注: 
Build a Foundation for Your Career in IT-Master the math powering our 
lives and prepare for your software engineer or security analyst career</p>
<blockquote><p>Discrete Math is needed to see mathematical structures in
 the object you work with, and understand their properties. This ability
 is important for software engineers, data scientists, security and 
financial analysts (it is not a coincidence that math puzzles are often 
used for interviews). We cover the basic notions and results 
(combinatorics, graphs, probability, number theory) that are universally
 needed. To deliver techniques and ideas in discrete mathematics to the 
learner we extensively use interactive puzzles specially created for 
this specialization. To bring the learners experience closer to 
IT-applications we incorporate programming examples, problems and 
projects in our courses.</p></blockquote>
<p>1) <a href="http://coursegraph.com/coursera-what-is-a-proof" rel="noopener" target="_blank">What is a Proof（什么是证明）</a></p>
<p>http://coursegraph.com/coursera-what-is-a-proof</p>
<blockquote><p>There is a perceived barrier to mathematics: proofs. In 
this course we will try to convince you that this barrier is more 
frightening than prohibitive: most proofs are easy to understand if 
explained correctly, and often they are even fun. We provide an 
accompanied excursion in the “proof zoo” showing you examples of 
techniques of different kind applied to different topics. We use some 
puzzles as examples, not because they are “practical”, but because 
discussing them we learn important reasoning and problem solving 
techniques that are useful. We hope you enjoy playing with the puzzles 
and inventing/understandings the proofs. As prerequisites we assume only
 basic math (e.g., we expect you to know what is a square or how to add 
fractions), basic programming in python (functions, loops, recursion), 
common sense and curiosity. Our intended audience are all people that 
work or plan to work in IT, starting from motivated high school 
students.</p></blockquote>
<p>2）<a href="http://coursegraph.com/coursera-combinatorics" rel="noopener" target="_blank">Combinatorics and Probability（组合和概率）</a></p>
<blockquote><p>Counting is one of the basic mathematically related tasks
 we encounter on a day to day basis. The main question here is the 
following. If we need to count something, can we do anything better than
 just counting all objects one by one? Do we need to create a list of 
all phone numbers to ensure that there are enough phone numbers for 
everyone? Is there a way to tell that our algorithm will run in a 
reasonable time before implementing and actually running it? All these 
questions are addressed by a mathematical field called Combinatorics. In
 this course we discuss most standard combinatorial settings that can 
help to answer questions of this type. We will especially concentrate on
 developing the ability to distinguish these settings in real life and 
algorithmic problems. This will help the learner to actually implement 
new knowledge. Apart from that we will discuss recursive technique for 
counting that is important for algorithmic implementations. One of the 
main `consumers’ of Combinatorics is Probability Theory. This area is 
connected with numerous sides of life, on one hand being an important 
concept in everyday life and on the other hand being an indispensable 
tool in such modern and important fields as Statistics and Machine 
Learning. In this course we will concentrate on providing the working 
knowledge of basics of probability and a good intuition in this area. 
The practice shows that such an intuition is not easy to develop. In the
 end of the course we will create a program that successfully plays a 
tricky and very counterintuitive dice game. As prerequisites we assume 
only basic math (e.g., we expect you to know what is a square or how to 
add fractions), basic programming in python (functions, loops, 
recursion), common sense and curiosity. Our intended audience are all 
people that work or plan to work in IT, starting from motivated high 
school students.</p></blockquote>
<p>3）<a href="http://coursegraph.com/coursera-graphs" rel="noopener" target="_blank">Introduction to Graph Theory（图论导论）</a></p>
<blockquote><p>We invite you to a fascinating journey into Graph Theory —
 an area which connects the elegance of painting and the rigor of 
mathematics; is simple, but not unsophisticated. Graph Theory gives us, 
both an easy way to pictorially represent many major mathematical 
results, and insights into the deep theories behind them. In this 
course, among other intriguing applications, we will see how GPS systems
 find shortest routes, how engineers design integrated circuits, how 
biologists assemble genomes, why a political map can always be colored 
using a few colors. We will study Ramsey Theory which proves that in a 
large system, complete disorder is impossible! By the end of the course,
 we will implement an algorithm which finds an optimal assignment of 
students to schools. This algorithm, developed by David Gale and Lloyd 
S. Shapley, was later recognized by the conferral of Nobel Prize in 
Economics. As prerequisites we assume only basic math (e.g., we expect 
you to know what is a square or how to add fractions), basic programming
 in python (functions, loops, recursion), common sense and curiosity. 
Our intended audience are all people that work or plan to work in IT, 
starting from motivated high school students.</p></blockquote>
<p>4) <a href="http://coursegraph.com/coursera-number-theory-cryptography" rel="noopener" target="_blank">Number Theory and Cryptography（数论和密码学）</a></p>
<blockquote><p>We all learn numbers from the childhood. Some of us like 
to count, others hate it, but any person uses numbers everyday to buy 
things, pay for services, estimated time and necessary resources. People
 have been wondering about numbers’ properties for thousands of years. 
And for thousands of years it was more or less just a game that was only
 interesting for pure mathematicians. Famous 20th century mathematician 
G.H. Hardy once said “The Theory of Numbers has always been regarded as 
one of the most obviously useless branches of Pure Mathematics”. Just 30
 years after his death, an algorithm for encryption of secret messages 
was developed using achievements of number theory. It was called RSA 
after the names of its authors, and its implementation is probably the 
most frequently used computer program in the word nowadays. Without it, 
nobody would be able to make secure payments over the internet, or even 
log in securely to e-mail and other personal services. In this short 
course, we will make the whole journey from the foundation to RSA in 4 
weeks. By the end, you will be able to apply the basics of the number 
theory to encrypt and decrypt messages, and to break the code if one 
applies RSA carelessly. You will even pass a cryptographic quest! As 
prerequisites we assume only basic math (e.g., we expect you to know 
what is a square or how to add fractions), basic programming in python 
(functions, loops, recursion), common sense and curiosity. Our intended 
audience are all people that work or plan to work in IT, starting from 
motivated high school students.</p></blockquote>
<p>5）<a href="http://coursegraph.com/coursera-delivery-problem" rel="noopener" target="_blank">Solving Delivery Problem（解决旅行商问题）</a></p>
<p>http://coursegraph.com/coursera-delivery-problem</p>
<blockquote><p>We’ll implement together an efficient program for a 
problem needed by delivery companies all over the world millions times 
per day — the travelling salesman problem. The goal in this problem is 
to visit all the given places as quickly as possible. How to find an 
optimal solution to this problem quickly? We still don’t have provably 
efficient algorithms for this difficult computational problem and this 
is the essence of the P versus NP problem, the most important open 
question in Computer Science. Still, we’ll implement several efficient 
solutions for real world instances of the travelling salesman problem. 
While designing these solutions, we will rely heavily on the material 
learned in the courses of the specialization: proof techniques, 
combinatorics, probability, graph theory. We’ll see several examples of 
using discrete mathematics ideas to get more and more efficient 
solutions.</p></blockquote>
<p>6 伦敦帝国理工学院 <a href="http://coursegraph.com/coursera-specializations-mathematics-machine-learning" rel="noopener" target="_blank">Mathematics for Machine Learning Specialization（面向机器学习的数学专项课程系列）</a></p>
<p>http://coursegraph.com/coursera-specializations-mathematics-machine-learning</p>
<p>伦敦帝国理工学院的面向机器学习的数学专项课程系列（Mathematics for Machine Learning 
Specialization），该系列包含3门子课程，涵盖线性代数，多变量微积分，以及主成分分析（PCA），这个专项系列课程的目标是弥补数学与机
器学习以及数据科学鸿沟，感兴趣的同学可以关注：Mathematics for Machine Learning。Learn about the 
prerequisite mathematics for applications in data science and machine 
learning</p>
<blockquote><p>For a lot of higher level courses in Machine Learning and
 Data Science, you find you need to freshen up on the basics in maths - 
stuff you may have studied before in school or university, but which was
 taught in another context, or not very intuitively, such that you 
struggle to relate it to how it’s used in Computer Science. This 
specialisation aims to bridge that gap, getting you up to speed in the 
underlying maths, building an intuitive understanding, and relating it 
to Machine Learning and Data Science. In the first course on Linear 
Algebra we look at what linear algebra is and how it relates to data. 
Then we look through what vectors and matrices are and how to work with 
them. The second course, Multivariate Calculus, builds on this to look 
at how to optimise fitting functions to get good fits to data. It starts
 from introductory calculus and then uses the matrices and vectors from 
the first course to look at data fitting. The third course, 
Dimensionality Reduction with Principal Components Analysis, uses the 
maths from the first two courses to do simple optimisation for the 
situation where you don’t have an understanding of how the data 
variables relate to each other. At the end of this specialisation you 
will have gained the prerequisite mathematical knowledge to continue 
your journey and take more advanced courses in machine learning.</p></blockquote>
<p>1） <a href="http://coursegraph.com/coursera-linear-algebra-machine-learning" rel="noopener" target="_blank">Mathematics for Machine Learning: Linear Algebra（面向机器学习的数学：线性代数）</a></p>
<p>http://coursegraph.com/coursera-linear-algebra-machine-learning</p>
<blockquote><p>In this course on Linear Algebra we look at what linear 
algebra is and how it relates to vectors and matrices. Then we look 
through what vectors and matrices are and how to work with them, 
including the knotty problem of eigenvalues and eigenvectors, and how to
 use these to solve problems. Finally we look at how to use these to do 
fun things with datasets - like how to rotate images of faces and how to
 extract eigenvectors to look at how the Pagerank algorithm works. Since
 we're aiming at data-driven applications, we'll be implementing some of
 these ideas in code, not just on pencil and paper. Towards the end of 
the course, you'll write code blocks and encounter Jupyter notebooks in 
Python, but don't worry, these will be quite short, focussed on the 
concepts, and will guide you through if you’ve not coded before. At the 
end of this course you will have an intuitive understanding of vectors 
and matrices that will help you bridge the gap into linear algebra 
problems, and how to apply these concepts to machine learning.</p></blockquote>
<p>2）<a href="http://coursegraph.com/coursera-multivariate-calculus-machine-learning" rel="noopener" target="_blank">Mathematics for Machine Learning: Multivariate Calculus（面向机器学习的数学：多变量微积分）</a></p>
<p>http://coursegraph.com/coursera-multivariate-calculus-machine-learning</p>
<blockquote><p>This course offers a brief introduction to the 
multivariate calculus required to build many common machine learning 
techniques. We start at the very beginning with a refresher on the “rise
 over run” formulation of a slope, before converting this to the formal 
definition of the gradient of a function. We then start to build up a 
set of tools for making calculus easier and faster. Next, we learn how 
to calculate vectors that point up hill on multidimensional surfaces and
 even put this into action using an interactive game. We take a look at 
how we can use calculus to build approximations to functions, as well as
 helping us to quantify how accurate we should expect those 
approximations to be. We also spend some time talking about where 
calculus comes up in the training of neural networks, before finally 
showing you how it is applied in linear regression models. This course 
is intended to offer an intuitive understanding of calculus, as well as 
the language necessary to look concepts up yourselves when you get 
stuck. Hopefully, without going into too much detail, you’ll still come 
away with the confidence to dive into some more focused machine learning
 courses in future.</p></blockquote>
<p>3）<a href="http://coursegraph.com/coursera-pca-machine-learning" rel="noopener" target="_blank">Mathematics for Machine Learning: PCA（面向机器学习的数学：主成分分析）</a></p>
<p>http://coursegraph.com/coursera-pca-machine-learning</p>
<blockquote><p>This course introduces the mathematical foundations to 
derive Principal Component Analysis (PCA), a fundamental dimensionality 
reduction technique. We'll cover some basic statistics of data sets, 
such as mean values and variances, we'll compute distances and angles 
between vectors using inner products and derive orthogonal projections 
of data onto lower-dimensional subspaces. Using all these tools, we'll 
then derive PCA as a method that minimizes the average squared 
reconstruction error between data points and their reconstruction. At 
the end of this course, you'll be familiar with important mathematical 
concepts and you can implement PCA all by yourself. If you’re 
struggling, you’ll find a set of jupyter notebooks that will allow you 
to explore properties of the techniques and walk you through what you 
need to do to get on track. If you are already an expert, this course 
may refresh some of your knowledge.</p></blockquote>
<p>注：本文首发“<a href="http://blog.coursegraph.com/">课程图谱博客</a>”：<a href="http://blog.coursegraph.com/">http://blog.coursegraph.com</a> </p>
<p>同步发布到这里,  本文链接地址：<a href="http://blog.coursegraph.com/coursera%E4%B8%8A%E6%95%B0%E5%AD%A6%E7%B1%BB%E7%9B%B8%E5%85%B3%E8%AF%BE%E7%A8%8B%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB%E6%8E%A8%E8%8D%90">http://blog.coursegraph.com/coursera上数学类相关课程数学公开课汇总推荐</a>  <a href="http://blog.coursegraph.com/?p=804">http://blog.coursegraph.com/?p=804</a></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e5%ad%a6%e8%af%be%e7%a8%8b-%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90" title="22:38" rel="bookmark"><time class="entry-date" datetime="2018-05-06T22:38:43+00:00">2018年05月6号</time></a>。属于<a href="http://www.52nlp.cn/category/%e5%85%ac%e5%bc%80%e8%af%be" rel="category tag">公开课</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/coursera" rel="tag">Coursera</a>、<a href="http://www.52nlp.cn/tag/coursera%e6%95%b0%e5%ad%a6" rel="tag">Coursera数学</a>、<a href="http://www.52nlp.cn/tag/pca" rel="tag">PCA</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90" rel="tag">主成分分析</a>、<a href="http://www.52nlp.cn/tag/%e5%8d%95%e5%8f%98%e9%87%8f%e5%be%ae%e7%a7%af%e5%88%86" rel="tag">单变量微积分</a>、<a href="http://www.52nlp.cn/tag/%e5%a4%9a%e5%8f%98%e9%87%8f%e5%be%ae%e7%a7%af%e5%88%86" rel="tag">多变量微积分</a>、<a href="http://www.52nlp.cn/tag/%e5%be%ae%e7%a7%af%e5%88%86" rel="tag">微积分</a>、<a href="http://www.52nlp.cn/tag/%e5%be%ae%e7%a7%af%e5%88%86%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">微积分公开课</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6" rel="tag">数学</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">数学公开课</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6%e6%80%9d%e7%bb%b4" rel="tag">数学思维</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6%e8%af%be%e7%a8%8b" rel="tag">数学课程</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e6%8d%ae%e7%a7%91%e5%ad%a6" rel="tag">数据科学</a>、<a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">机器学习</a>、<a href="http://www.52nlp.cn/tag/%e7%a6%bb%e6%95%a3%e6%95%b0%e5%ad%a6" rel="tag">离散数学</a>、<a href="http://www.52nlp.cn/tag/%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0" rel="tag">线性代数</a>、<a href="http://www.52nlp.cn/tag/%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">线性代数公开课</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/52opencourse" title="查看所有由课程图谱发布的文章" rel="author">课程图谱</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10165" class="post-10165 post type-post status-publish format-standard hentry category-informal-essay tag-dns tag-http tag-1507 tag-1508">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/%e7%94%b5%e4%bf%a1%e8%bf%90%e8%90%a5%e5%95%86%e5%8a%ab%e6%8c%81%e4%bd%95%e6%97%b6%e4%bc%91" rel="bookmark">电信运营商劫持何时休</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/%e7%94%b5%e4%bf%a1%e8%bf%90%e8%90%a5%e5%95%86%e5%8a%ab%e6%8c%81%e4%bd%95%e6%97%b6%e4%bc%91#comments">4条回复</a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p><strong>这篇文章本来不想发在这里，但是觉得这个事情在微博上一句两句说不清楚，所以就在这里记录一下。</strong></p>
<p>我安装了3条江苏电信宽带，两个是比较普通的宽带：1个在家里，1个在公司，另外一个是企业用的专线，含静态IP，也在公司。最近突然发现用普通宽
带访问购物网站的时候，会做一次跳转，先到一个第三方网站，然后再到购物网站，虽然这个过程可能是一两秒的事情，但是作为技术人员，对这个还是比较敏感
的：</p>
<p>这是输入"www.jd.com" 的时候，会先跳转到p.yiqifa.com这个垃圾网站，再访问京东：</p>
<p><img src="52nlp.cn_files/-2018-04-27-10_002.png" alt="" class="aligncenter size-full wp-image-10172" srcset="http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.16.51.png 648w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.16.51-300x19.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.16.51-624x40.png 624w" sizes="(max-width: 648px) 100vw, 648px" width="648" height="42"></p>
<p>这是输入"you.163.com"的时候，会先跳转到p.gouwubang.com这个垃圾网站，再访问网易严选：</p>
<p><img src="52nlp.cn_files/-2018-04-27-10.png" alt="" class="aligncenter size-full wp-image-10173" srcset="http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.18.03.png 675w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.18.03-300x17.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.18.03-624x36.png 624w" sizes="(max-width: 675px) 100vw, 675px" width="675" height="39"></p>
<p>因为我主要用的是Mac 
Pro，几乎不存在中毒的问题，但是还是要检查一下是不是浏览器被做了手脚，上面的访问是在chrome下出现的，所以我换了firefox，也是同样的
问题，所以，浏览器中招的问题可以被排除。其次，我让公司其他同事试了一下，也是同样的问题，所以这个不是个人电脑的现象。最后，我又同时做了两个测试，
一个是切换到公司专线的路由器wifi上，这个现象立马消失，另外一个是回家测试，继续发现这个现象，基本上可以断定是更上一级的电信网络有鬼了。</p>
<p>同时google了一下 "p.yiqifa.com", "p.gouwubang.com"，发现相似的事情被吐槽和投诉不是一两天：</p>
<p><a href="http://blog.sina.com.cn/s/blog_725bb05f0102v3v0.html">http://blog.sina.com.cn/s/blog_725bb05f0102v3v0.html</a><br>
<a href="https://www.v2ex.com/t/220616">https://www.v2ex.com/t/220616</a><br>
<a href="https://www.zhihu.com/question/20014335" rel="noopener" target="_blank">https://www.zhihu.com/question/20014335</a><br>
<a href="https://www.amobbs.com/thread-5638527-1-1.html" rel="noopener" target="_blank">https://www.amobbs.com/thread-5638527-1-1.html</a><br>
<a href="http://tieba.baidu.com/p/2794866155" rel="noopener" target="_blank">http://tieba.baidu.com/p/2794866155</a></p>
<p>再输入"www.yiqifa.com" , "www.gouwubang.com"，会发现这两个网站都直指“北京亿玛在线科技股份有限公司”：</p>
<p><img src="52nlp.cn_files/-2018-04-27-10_003.png" alt="" class="aligncenter size-full wp-image-10182" srcset="http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.51.54.png 513w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.51.54-300x32.png 300w" sizes="(max-width: 513px) 100vw, 513px" width="513" height="54"><img src="52nlp.cn_files/-2018-04-27-10_004.png" alt="" class="aligncenter size-full wp-image-10183" srcset="http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.51.28.png 682w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.51.28-300x33.png 300w, http://www.52nlp.cn/wp-content/uploads/2018/04/屏幕快照-2018-04-27-上午10.51.28-624x68.png 624w" sizes="(max-width: 682px) 100vw, 682px" width="682" height="74"></p>
<p>最后google一下这个亿玛在线，发现这家公司是做流量生意的，而且“贵”为新三板头部公司，缘来如此。</p>
<p>现在基本上可以断定是电信运营商劫持了，或者说电信有内鬼和这家公司“合作”。这类事情，之前遇到过一次，就是每天用网站的时候时不时的突然给你一
个中间网站，带广告，或者右下角弹窗，当时打电话投诉，后来电信一个做技术的和我联系，我给他邮件截图，后来这两条电信宽带真的清净了，据说电信内部有
“白名单”，比较挑剔的用户会放到白名单里不会被骚扰，大多数不明情况的群众依然被“劫持”。</p>
<p>于是我再一次拨打10000号投诉，然后第二天以前给安装宽带的师傅联系我，我说这不是他的事情，要让内部技术人员解决，第二天他打电话让我修改
DNS，说有可能“DNS污染”，建议用114那个，但是我搜了一下，114DNS也不靠谱，所以试着换了一下腾讯DNSPod的DNS，依然如故，所以
这个解决方案无效，基本可以断定是“http劫持”。继续投诉，10000号让我拨打02512300，说是他们的上级主管单位的投诉电话，但是这个电
话，打了好久，一直让等待。</p>
<p>之前google的时候发现一些小伙伴提议的工信部电信投诉是终极大招：<a href="http://www.chinatcc.gov.cn:8080/cms/shensus/" rel="noopener" target="_blank">http://www.chinatcc.gov.cn:8080/cms/shensus/</a>
 
，于是尝试了一下，完全符合投诉告知：已经和电信沟通过，但是他们没有解决问题。反馈速度很快，几天之后电信又联系我，说已经让内部人员处理了，让我再试
试，依然如故，之后他们派出了师傅上门，截图录像，解决问题的态度还不错，但是第二天10000号电话告诉我他们技术人员认为这是第三方网站搞得鬼，他们
没有办法。。。Excuse Me 。。。态度虽然很好，但是却没有解决问题，所以目前我正在等待工信部回访电话，依然准备继续投诉中。</p>
<p>目前用了一个网友提供的临时方法，在hosts加入：<br>
127.0.0.1 p.gouwubang.com<br>
127.0.0.1 p.yiqifa.com<br>
127.0.0.1 c.duomai.com<br>
127.0.0.1 www.duomai.com<br>
127.0.0.1 sh.baibucy.com<br>
127.0.0.1 top.qichexin.com<br>
127.0.0.1 hao.baibucy.com<br>
127.0.0.1 p.egou.com<br>
127.0.0.1 tj.xiaoyucn.com<br>
127.0.0.1 click.linktech.cn<br>
127.0.0.1 www.pinzhitmall.com<br>
127.0.0.1 www.shihuo.cn<br>
127.0.0.1 www.yitaopt.com</p>
<p>这个方法，只能让跳转到这些网站的时候无效；另外就是在访问大型网站的时候最好加上https, 类似https://www.jd.com, 
https://you.163.com，https目前还无法被分析劫持。这类http劫持最让人不爽的是每次网站浏览行为都被劫持分析，然后如果是有
利可图的网站，他们就强制跳转。网上吐槽的也只不过是冰上一角，很多小白用户根本就不知道自己被运营商当猴耍。也许电信会表示自己很无辜，也许真的是他们
的内鬼和外面这些无良公司勾结，但是这个问题何时能得到彻底解决？</p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/%e7%94%b5%e4%bf%a1%e8%bf%90%e8%90%a5%e5%95%86%e5%8a%ab%e6%8c%81%e4%bd%95%e6%97%b6%e4%bc%91" title="11:37" rel="bookmark"><time class="entry-date" datetime="2018-04-27T11:37:19+00:00">2018年04月27号</time></a>。属于<a href="http://www.52nlp.cn/category/informal-essay" rel="category tag">随笔</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/dns%e6%b1%a1%e6%9f%93" rel="tag">dns污染</a>、<a href="http://www.52nlp.cn/tag/http%e5%8a%ab%e6%8c%81" rel="tag">http劫持</a>、<a href="http://www.52nlp.cn/tag/%e7%94%b5%e4%bf%a1%e5%8a%ab%e6%8c%81" rel="tag">电信劫持</a>、<a href="http://www.52nlp.cn/tag/%e8%bf%90%e8%90%a5%e5%95%86%e5%8a%ab%e6%8c%81" rel="tag">运营商劫持</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/admin" title="查看所有由52nlp发布的文章" rel="author">52nlp</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10160" class="post-10160 post type-post status-publish format-standard hentry category-1490 tag-coursera tag-1500 tag-480 tag-1495 tag-1501 tag-1496 tag-1502 tag-1503 tag-1504 tag-291 tag-1498 tag-1497 tag-526 tag-416 tag-1499 tag-1505 tag-1506 tag-1351">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e5%8d%9a%e5%bc%88%e8%ae%ba%e8%af%be%e7%a8%8b-%e5%8d%9a%e5%bc%88%e8%ae%ba%e5%85%ac%e5%bc%80%e8%af%be-game-theory-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90" rel="bookmark">Coursera上博弈论相关课程（公开课）汇总推荐</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e5%8d%9a%e5%bc%88%e8%ae%ba%e8%af%be%e7%a8%8b-%e5%8d%9a%e5%bc%88%e8%ae%ba%e5%85%ac%e5%bc%80%e8%af%be-game-theory-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p><a href="http://blog.coursegraph.com/tag/%E5%8D%9A%E5%BC%88%E8%AE%BA" rel="noopener" target="_blank">博弈论（Game Theory）</a>很有意思，大家可能首先想到的就是赌博，据说博弈论最早源于赌博策略和数学，下面是来自<a href="https://zh.wikipedia.org/wiki/%E5%8D%9A%E5%BC%88%E8%AE%BA" rel="noopener" target="_blank">维基百科</a>的解释：</p>
<blockquote><p>博弈论（英语：game 
theory），又译为对策论，或者赛局理论，应用数学的一个分支，1944年冯·诺伊曼与奥斯卡·摩根斯特恩合著《博弈论与经济行为》，标志着现代系统
博弈理论的的初步形成，因此他被称为“博弈论之父”。博弈论被认为是20世纪经济学最伟大的成果之一。目前在生物学、经济学、国际关系、计算机科学、政治
学、军事战略和其他很多学科都有广泛的应用。主要研究公式化了的激励结构（游戏或者博弈）间的相互作用。是研究具有斗争或竞争性质现象的数学理论和方法。
也是运筹学的一个重要学科。</p></blockquote>
<p>作为互联网广告研发人员，应该或多或少了解一点计算广告学，其中支撑Google, 
百度等互联网巨头广告业务的竞价排名机制的核心之一就是博弈论。另外经济学中有很多博弈论的影子，电影“美丽心灵”中的主角数学家约翰纳什，由于他与另外
两位数学家在非合作博弈的均衡分析理论方面做出了开创性的贡献，对博弈论和经济学产生了重大影响，而获得1994年诺贝尔经济学奖，纳什均衡则是博弈论课
程中不可或缺的一节课。<a href="http://blog.coursegraph.com/coursera.php" rel="noopener" target="_blank">Coursera</a>上有好几门博弈论(<a href="http://coursegraph.com/search_results/game%20theory" rel="noopener" target="_blank">Game Theory</a>)相关的课程，这里做个汇总整理。</p>
<p>1. 斯坦福大学的 <a href="http://coursegraph.com/coursera-game-theory-1" rel="noopener" target="_blank">博弈论（Game Theory）</a></p>
<p>这门课程早在Coursera诞生之初就有了，后经多次优化，现在有上和下两个部分，这门课程属于博弈论上，重在博弈论基础，需要学习者有一定的数学思维和数学基础，例如基础的概率理论和一些微积分基础知识：</p>
<blockquote><p>This course is aimed at students, researchers, and 
practitioners who wish to understand more about strategic interactions. 
You must be comfortable with mathematical thinking and rigorous 
arguments. Relatively little specific math is required; but you should 
be familiar with basic probability theory (for example, you should know 
what a conditional probability is), and some very light calculus would 
be helpful.</p></blockquote>
<p>2. 斯坦福大学的 <a href="http://coursegraph.com/coursera-game-theory-2" rel="noopener" target="_blank">博弈论二: 高级应用（Game Theory II: Advanced Applications)</a></p>
<p>上门博弈论课程的续集，关注博弈论的应用，包括机制设计，拍卖机制等：</p>
<blockquote><p>Popularized by movies such as "A Beautiful Mind", game 
theory is the mathematical modeling of strategic interaction among 
rational (and irrational) agents. Over four weeks of lectures, this 
advanced course considers how to design interactions between agents in 
order to achieve good social outcomes. Three main topics are covered: 
social choice theory (i.e., collective decision making and voting 
systems), mechanism design, and auctions. In the first week we consider 
the problem of aggregating different agents' preferences, discussing 
voting rules and the challenges faced in collective decision making. We 
present some of the most important theoretical results in the area: 
notably, Arrow's Theorem, which proves that there is no "perfect" voting
 system, and also the Gibbard-Satterthwaite and Muller-Satterthwaite 
Theorems. We move on to consider the problem of making collective 
decisions when agents are self interested and can strategically 
misreport their preferences. We explain "mechanism design" -- a broad 
framework for designing interactions between self-interested agents -- 
and give some key theoretical results. Our third week focuses on the 
problem of designing mechanisms to maximize aggregate happiness across 
agents, and presents the powerful family of Vickrey-Clarke-Groves 
mechanisms. The course wraps up with a fourth week that considers the 
problem of allocating scarce resources among self-interested agents, and
 that provides an introduction to auction theory.</p></blockquote>
<p>3. 东京大学的 <a href="http://coursegraph.com/coursera-game-theory-introduction" rel="noopener" target="_blank">博弈论入门课程（Welcome to Game Theory）</a></p>
<p>入门级博弈论课程，由东京大学推出，英文授课：</p>
<blockquote><p>This course provides a brief introduction to game theory.
 Our main goal is to understand the basic ideas behind the key concepts 
in game theory, such as equilibrium, rationality, and cooperation. The 
course uses very little mathematics, and it is ideal for those who are 
looking for a conceptual introduction to game theory. Business 
competition, political campaigns, the struggle for existence by animals 
and plants, and so on, can all be regarded as a kind of “game,” in which
 individuals try to do their best against others. Game theory provides a
 general framework to describe and analyze how individuals behave in 
such “strategic” situations. This course focuses on the key concepts in 
game theory, and attempts to outline the informal basic ideas that are 
often hidden behind mathematical definitions. Game theory has been 
applied to a number of disciplines, including economics, political 
science, psychology, sociology, biology, and computer science. 
Therefore, a warm welcome is extended to audiences from all fields who 
are interested in what game theory is all about.</p></blockquote>
<p>4. 佐治亚理工学院的 <a href="http://coursegraph.com/coursera-combinatorial-game-theory" rel="noopener" target="_blank">组合博弈论（Games without Chance: Combinatorial Game Theory)</a></p>
<p>这门课程主要关注组合博弈论，覆盖不靠运气游戏背后的数学理论和分析：This course will cover the 
mathematical theory and analysis of simple games without chance moves.</p>
<blockquote><p>本课程将讲解如何运用数学理论，分析不含运气步骤（随机步骤）的简单游戏。本课程将探索不含运气步骤（随机步骤）的两个玩
家游戏中的数学理论。我们将讨论如何简化游戏，什么情况下游戏等同于数字运算，以及怎样的游戏才算公正。许多例子都是有关一此简单的游戏，有的你可能还没
有听说过：Hackenbush（“无向图删边”游戏）、Nim（“拈”游戏）、Push（推箱子游戏）、Toads and 
Frogs（“蟾蜍和青蛙”游戏），等。虽然完成这门课程并不能让你成为国际象棋或围棋高手，但是会让你更深入了解游戏的结构。</p></blockquote>
<p>5. 国立台湾大学的 <a href="http://coursegraph.com/coursera-shiyan-jingji-xue" rel="noopener" target="_blank">实验经济学: 行为博弈论 (Experimental Economics I: Behavioral Game Theory)</a></p>
<p>台湾大学王道一副教授 (Associate Professor)的实验经济学课程-行为博弈论：</p>
<blockquote><p>人是否会如同理论经济学的预测进行决策？这门课将透过每周的课程视频以及课后作业带你了解实验经济学的基本概念。每周将会
有习题练习以及指定阅读的期刊论文。你将会参与一些在线的实验、报告论文并且互评其他同学的报告。❖课程介绍（About the 
course）这是一门进阶的经济学课程，课程目标为介绍实验经济学的基本概念，并且让学生们能开始在这个领域从事自己的相关研究。详细课程目标如
下：1.实验经济学的介绍：在上完这堂课之后，学生应能列举经济学各个领域的数个知名实验，并且解释实验结果如何验证或否证经济理论及其他实地数据。2.
评论近期相关领域研究：上完这堂课之后，学生应能阅读并评论实验经济学相关的期刊论文。在课堂中，学生将会阅读指定的期刊论文，并且（在视频中）亲自上台
报告一篇论文。❖授课形式（Course 
format）1.本堂课将以视频的形式为主，搭配课后作业的形式来进行。每个同学将阅读一篇实验经济学论文，并录像成两段各10分钟的介绍视频并后上传
至Coursera（或上传到Youku，再复制连接到作业上传区）。第一段期中报告视频请同学介绍该论文所描述的实验设计，第二段，也就是期末报告视频
则介绍实验结果。此外每位同学至少需观看其他两位同学的呈现内容，并给予评论。2.这堂课将简单地运用以下赛局（博弈）概念：奈许均衡/纳什均衡
（Nash Equilibrium）混合策略均衡（Mixed Strategy 
Equilibrium）子赛局完美均衡/子博弈精练纳什均衡（SPNE）共识/共同知识（Common Knowledge）信念（Belief）</p></blockquote>
<p>注：本文首发“<a href="http://blog.coursegraph.com/">课程图谱博客</a>”：<a href="http://blog.coursegraph.com/">http://blog.coursegraph.com</a><br>
同步发布到这里,  本文链接地址：<a href="http://blog.coursegraph.com/coursera%E4%B8%8A%E5%8D%9A%E5%BC%88%E8%AE%BA%E8%AF%BE%E7%A8%8B%E5%8D%9A%E5%BC%88%E8%AE%BA%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB%E6%8E%A8%E8%8D%90">http://blog.coursegraph.com/coursera上博弈论课程博弈论公开课汇总推荐</a>  <a href="http://blog.coursegraph.com/?p=782">http://blog.coursegraph.com/?p=782</a></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e5%8d%9a%e5%bc%88%e8%ae%ba%e8%af%be%e7%a8%8b-%e5%8d%9a%e5%bc%88%e8%ae%ba%e5%85%ac%e5%bc%80%e8%af%be-game-theory-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90" title="12:43" rel="bookmark"><time class="entry-date" datetime="2018-04-06T12:43:02+00:00">2018年04月6号</time></a>。属于<a href="http://www.52nlp.cn/category/%e5%85%ac%e5%bc%80%e8%af%be" rel="category tag">公开课</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/coursera" rel="tag">Coursera</a>、<a href="http://www.52nlp.cn/tag/coursera%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">Coursera公开课</a>、<a href="http://www.52nlp.cn/tag/coursera%e8%af%be%e7%a8%8b" rel="tag">Coursera课程</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%9c%e4%ba%ac%e5%a4%a7%e5%ad%a6" rel="tag">东京大学</a>、<a href="http://www.52nlp.cn/tag/%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">公开课</a>、<a href="http://www.52nlp.cn/tag/%e5%8d%9a%e5%bc%88%e8%ae%ba" rel="tag">博弈论</a>、<a href="http://www.52nlp.cn/tag/%e5%8d%9a%e5%bc%88%e8%ae%ba%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">博弈论公开课</a>、<a href="http://www.52nlp.cn/tag/%e5%8d%9a%e5%bc%88%e8%ae%ba%e5%ba%94%e7%94%a8" rel="tag">博弈论应用</a>、<a href="http://www.52nlp.cn/tag/%e5%8d%9a%e5%bc%88%e8%ae%ba%e8%af%be%e7%a8%8b" rel="tag">博弈论课程</a>、<a href="http://www.52nlp.cn/tag/%e5%8f%b0%e6%b9%be%e5%a4%a7%e5%ad%a6" rel="tag">台湾大学</a>、<a href="http://www.52nlp.cn/tag/%e5%ae%9e%e9%aa%8c%e7%bb%8f%e6%b5%8e%e5%ad%a6" rel="tag">实验经济学</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6" rel="tag">数学</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6%e6%80%9d%e7%bb%b4" rel="tag">数学思维</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6%e6%a8%a1%e5%9e%8b" rel="tag">数学模型</a>、<a href="http://www.52nlp.cn/tag/%e6%96%af%e5%9d%a6%e7%a6%8f" rel="tag">斯坦福</a>、<a href="http://www.52nlp.cn/tag/%e6%96%af%e5%9d%a6%e7%a6%8f%e5%a4%a7%e5%ad%a6" rel="tag">斯坦福大学</a>、<a href="http://www.52nlp.cn/tag/%e7%bb%84%e5%90%88%e5%8d%9a%e5%bc%88%e8%ae%ba" rel="tag">组合博弈论</a>、<a href="http://www.52nlp.cn/tag/%e7%bb%8f%e6%b5%8e%e5%ad%a6" rel="tag">经济学</a>、<a href="http://www.52nlp.cn/tag/%e8%a1%8c%e4%b8%ba%e5%8d%9a%e5%bc%88%e8%ae%ba" rel="tag">行为博弈论</a>、<a href="http://www.52nlp.cn/tag/%e8%af%be%e7%a8%8b" rel="tag">课程</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/52opencourse" title="查看所有由课程图谱发布的文章" rel="author">课程图谱</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10148" class="post-10148 post type-post status-publish format-standard hentry category-chinese-information-processing category-word-segmentation category-nlp tag-ictclas tag-nlpir tag-82 tag-1488 tag-386 tag-1484 tag-895 tag-1489">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/%e6%8e%a8%e8%8d%90nlpir%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0" rel="bookmark">推荐NLPIR大数据语义智能分析平台</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/%e6%8e%a8%e8%8d%90nlpir%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p><strong>NLPIR大数据语义智能分析平台由北京理工大学大数据搜索与挖掘实验室（ Big Data Search and 
Mining 
Lab.BDSM@BIT）张华平博士主导，最近上线了新版，这里做个推荐。张华平博士最广为人知的产品是ICTCLAS中文分词平台，相信这更有助于大
家了解NLPIR大数据语义智能分析平台，以下摘自“<a href="https://mp.weixin.qq.com/s/vKQi1MAydrpMz5ChhsVYpg" rel="noopener" target="_blank">NLPIR大数据语义智能分析平台新版上线</a>”。</strong></p>
<p>NLPIR大数据语义智能分析平台针对大数据内容采编挖搜的综合需求，融合了网络精准采集、自然语言理解、文本挖掘和语义搜索的最新研究成果，先后历时十八年，服务了全球四十万家机构用户，是大数据时代语义智能分析的一大利器。</p>
<p><img src="52nlp.cn_files/-2018-03-20-3.png" alt="" class="aligncenter size-full wp-image-10149" srcset="http://www.52nlp.cn/wp-content/uploads/2018/03/屏幕快照-2018-03-20-下午3.19.05.png 548w, http://www.52nlp.cn/wp-content/uploads/2018/03/屏幕快照-2018-03-20-下午3.19.05-300x169.png 300w" sizes="(max-width: 548px) 100vw, 548px" width="548" height="309"></p>
<p>NLPIR大数据语义智能挖掘平台，针对大数据内容处理的需要，融合了网络精准采集、自然语言理解、文本挖掘和网络搜索的技术，提供了客户端工具、
云服务、二次开发接口。开发平台由多个中间件组成，各个中间件API可以无缝地融合到客户的各类复杂应用系统之中，可兼容
Windows，Linux，Android，Maemo5, FreeBSD等不同操作系统平台，可以供Java，C，C#等各类开发语言使用。</p>
<p>NLPIR大数据语义智能分析平台十三大功能：</p>
<p>1、精准采集：对境内外互联网海量信息实时精准采集，有主题采集（按照信息需求的主题采集）与站点采集两种模式（给定网址列表的站内定点采集功能）。</p>
<p>2、文档抽取：对doc、excel、pdf与ppt等多种主流文档格式，进行文本信息抽取，信息抽取准确，效率达到大数据处理的要求。</p>
<p>3、新词发现：从文本中挖掘出新词、新概念，用户可以用于专业词典的编撰，还可以进一步编辑标注，导入分词词典中，提高分词系统的准确度，并适应新的语言变化。</p>
<p>4、批量分词：对原始语料进行分词，自动识别人名地名机构名等未登录词，新词标注以及词性标注。并可在分析过程中，导入用户定义的词典。</p>
<p>5、语言统计：针对切分标注结果，系统可以自动地进行一元词频统计、二元词语转移概率统计。针对常用的术语，会自动给出相应的英文解释。</p>
<p>6、文本聚类：能够从大规模数据中自动分析出热点事件，并提供事件话题的关键特征描述。同时适用于长文本和短信、微博等短文本的热点分析。</p>
<p>7、文本分类：根据规则或训练的方法对大量文本进行分类，可用于新闻分类、简历分类、邮件分类、办公文档分类、区域分类等诸多方面。</p>
<p>8、摘要实体：对单篇或多篇文章，自动提炼出内容摘要，抽取人名、地名、机构名、时间及主题关键词；方便用户快速浏览文本内容。</p>
<p>9、智能过滤：对文本内容的语义智能过滤审查，内置国内最全词库，智能识别多种变种：形变、音变、繁简等多种变形，语义精准排歧。</p>
<p>10、情感分析：针对事先指定的分析对象，系统自动分析海量文档的情感倾向：情感极性及情感值测量，并在原文中给出正负面的得分和句子样例。</p>
<p>11、文档去重：快速准确地判断文件集合或数据库中是否存在相同或相似内容的记录，同时找出所有的重复记录。</p>
<p>12、全文检索：支持文本、数字、日期、字符串等各种数据类型，多字段的高效搜索，支持AND/OR/NOT以及NEAR邻近等查询语法，支持维语、藏语、蒙语、阿拉伯、韩语等多种少数民族语言的检索。</p>
<p>13、编码转换：自动识别内容的编码，并把编码统一转换为其他编码。</p>
<p>欢迎大家下载使用。</p>
<p>NLPIR大数据语义智能分析平台白皮书：</p>
<p><a href="http://www.nlpir.org/NLPIR-Parser-WhitePaper.pdf" rel="noopener" target="_blank">http://www.nlpir.org/NLPIR-Parser-WhitePaper.pdf</a> (约3MB)</p>
<p>NLPIR大数据语义智能分析平台：</p>
<p><a href="http://www.nlpir.org/NLPIR-Parser.zip" rel="noopener" target="_blank">http://www.nlpir.org/NLPIR-Parser.zip</a> (约160MB)</p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/%e6%8e%a8%e8%8d%90nlpir%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0" title="15:28" rel="bookmark"><time class="entry-date" datetime="2018-03-20T15:28:14+00:00">2018年03月20号</time></a>。属于<a href="http://www.52nlp.cn/category/chinese-information-processing" rel="category tag">中文信息处理</a>、<a href="http://www.52nlp.cn/category/word-segmentation" rel="category tag">中文分词</a>、<a href="http://www.52nlp.cn/category/nlp" rel="category tag">自然语言处理</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/ictclas" rel="tag">ICTCLAS</a>、<a href="http://www.52nlp.cn/tag/ictclas%e5%bc%a0%e5%8d%8e%e5%b9%b3%e5%8d%9a%e5%a3%ab" rel="tag">ICTCLAS张华平博士</a>、<a href="http://www.52nlp.cn/tag/nlpir" rel="tag">NLPIR</a>、<a href="http://www.52nlp.cn/tag/nlpir%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0" rel="tag">NLPIR大数据语义智能分析平台</a>、<a href="http://www.52nlp.cn/tag/%e4%b8%ad%e6%96%87%e5%88%86%e8%af%8d" rel="tag">中文分词</a>、<a href="http://www.52nlp.cn/tag/%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0" rel="tag">大数据语义智能分析平台</a>、<a href="http://www.52nlp.cn/tag/%e5%bc%a0%e5%8d%8e%e5%b9%b3" rel="tag">张华平</a>、<a href="http://www.52nlp.cn/tag/%e5%bc%a0%e5%8d%8e%e5%b9%b3%e5%8d%9a%e5%a3%ab" rel="tag">张华平博士</a>、<a href="http://www.52nlp.cn/tag/%e6%96%87%e6%9c%ac%e6%8c%96%e6%8e%98" rel="tag">文本挖掘</a>、<a href="http://www.52nlp.cn/tag/%e6%96%87%e6%9c%ac%e8%af%ad%e4%b9%89%e6%8c%96%e6%8e%98" rel="tag">文本语义挖掘</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/admin" title="查看所有由52nlp发布的文章" rel="author">52nlp</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10142" class="post-10142 post type-post status-publish format-standard hentry category-560 category-354 category-1153 category-nlp tag-1245 tag-1240 tag-1018 tag-1454">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/juzhenfenjiedatagrand" rel="bookmark">推荐系统中的矩阵分解技术（达观数据  周颢钰）</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/juzhenfenjiedatagrand#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<section style="font-size: 16px">
<section style="padding-right: 10px;padding-left: 10px">
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><img class="aligncenter" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894-15.jpeg" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" width="929" height="516"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-family: 微软雅黑;font-size: 14px">网络中的信息量呈现指数式增长，随之带来了信息过载问题。推荐系统是大数据时代下应运而生的产物，目前已广泛应用于电商、社交、短视频等领域。本文将针对推荐系统中基于隐语义模型的矩阵分解技术来进行讨论。</span></p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">1</span></section>
</section>
</section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">评分矩阵、奇异值分解与Funk-SVD</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-family: 微软雅黑;font-size: 14px">对于一个推荐系统，其用户数据可以整理成一个user-item矩阵。矩阵中每一行代表一个用户，而每一列则代表一个物品。若用户对物品有过评分，则矩阵中处在用户对应的行与物品对应的列交叉的位置表示用户对物品的评分值。这个user-item矩阵被称为评分矩阵。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="color: #333333;font-family: 微软雅黑;font-size: 14px"><img class="aligncenter" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_005.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" width="661" height="286"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-family: 微软雅黑;font-size: 14px">上图即为评分矩阵的一个例子。其中的？表示用户还没有对物品做出评价，而推荐系统最终的目标就是对于任意一个用户，预测出所有未评分物品的分值，并按分值从高到低的顺序将对应的物品推荐给用户。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">说到矩阵分解技术，首先想到的往往是<span style="color: #0070c0"><strong>特征值分解（eigendecomposition）</strong></span>与<strong><span style="color: #0070c0">奇异值分解（Singular value decomposition，SVD）</span></strong>。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">对于<strong>特征值分解</strong>，由于其只能作用于方阵，因此并不适合分解评分矩阵这个场景。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">而对于<strong>奇异值分解</strong>，其具体描述为：假设矩阵M是一个m*n的矩阵，则一定存在一个分解<img style="width: 113px;height: 25px" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_024.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" width="113" height="25">，
其中U是m*m的正交矩阵，V是n*n的正交矩阵，Σ是m*n的对角阵，可以说是完美契合分解评分矩阵这个需求。其中，对角阵Σ还有一个特殊的性质，它的
所有元素都非负，且依次减小。这个减小也特别快，在很多情况下，前10%的和就占了全部元素之和的99%以上，这就是说我们可以使用最大的k个值和对应大
小的U、V矩阵来近似描述原始的评分矩阵。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="background-color: #0070c0;color: #ffffff"><strong><span style="font-size: 14px">于是我们马上能得到一个解决方案：</span></strong></span><span style="font-size: 14px">对
原始评分矩阵M做奇异值分解，得到U、V及Σ，取Σ中较大的k类作为隐含特征，则此时M(m*n)被分解成U(m*k) 
Σ(k*k)V(k*n)，接下来就可以直接使用矩阵乘法来完成对原始评分矩阵的填充。但是实际上，这种方法存在一个致命的缺陷——奇异值分解要求矩阵是
稠密的。也就是说SVD不允许待分解矩阵中存在空白的部分，这一开始就与我们的问题所冲突了。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">当
然，也可以想办法对缺失值先进行简单的填充，例如使用全局平均值。然而，即使有了补全策略，在实际应用场景下，user和item的数目往往是成千上万
的，面对这样的规模传统SVD算法O(n^3)的时间复杂度显然是吃不消的。因此，直接使用传统SVD算法并不是一个好的选择。（达观数据周颢钰）</span></p>
<section style="border: 0px none initial">
<section style="background-color: #fefefe;padding-right: 15px;padding-left: 15px">
<section style="background-color: #0f0f19;width: 3.5em;height: 3.5em;padding: 8px 5px"><img style="width: 2.5em" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_014.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></section>
</section>
<section style="padding: 50px 20px 20px;line-height: 1.4;margin-top: -2em;border: 1px solid #c6c6c6">
<p style="text-align: center;margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">既然传统SVD在实际应用场景中面临着稀疏性问题和效率问题，<span style="color: #0070c0"><strong>那么有没有办法避开稀疏问题，同时提高运算效率呢？</strong></span></span></p>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">实际上早在06年，Simon Funk就提出了Funk-SVD算法，其主要思路是将原始评分矩阵M（m*n）分解成两个矩阵P（m*k）和Q（k*n），同时仅考察原始评分矩阵中有评分的项分解结果是否准确，而判别标准则是均方差。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">即对于矩阵M(m*n)，我们想办法将其分解为P(m*k)、Q(k*n)，此时对于原始矩阵中有评分的位置MUI来说，其在分解后矩阵中对应的值就是</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_008.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">那么对于整个评分矩阵而言，总的损失就是</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_010.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">只要我们能想办法最小化上面的损失SSE，就能以最小的扰动完成对原始评分矩阵的分解，在这之后只需要用计算M’ 的方式来完成对原始评分矩阵的填充即可。（达观数据 周颢钰）</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">这种方法被称之为<span style="color: #0070c0"><strong>隐语义模型（Latent factor model，LFM）</strong></span>，其算法意义层面的解释为通过<span style="color: #0070c0"><strong>隐含特征（latent factor）</strong></span>将user兴趣与item特征联系起来。</span></p>
<p style="margin-left: 10px;margin-right: 10px"><img class="aligncenter" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_006.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" width="768" height="185"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">对
于原始评分矩阵R，我们假定一共有三类隐含特征，于是将矩阵R（3*4）分解成用户特征矩阵P（3*3）与物品特征矩阵Q（3*4）。考察user1对
item1的评分，可以认为user1对三类隐含特征class1、class2、class3的感兴趣程度分别为P11、P12、P13，而这三类隐含
特征与item1相关程度则分别为Q11、Q21、Q31。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">回到上面的式子</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_013.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">可以发现用户U对物品I最终的评分就是由各个隐含特征维度下U对I感兴趣程度的和，这里U对I的感兴趣程度则是由U对当前隐含特征的感兴趣程度乘上I与当前隐含特征相关程度来表示的。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">于是，<span style="color: #ffffff;background-color: #0070c0">现在的问题就变成了如何求出使得SSE最小的矩阵P和Q</span>。</span></p>
<p>&nbsp;</p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">2</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">随机梯度下降法</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">在求解上文中提到的这类无约束最优化问题时，<span style="color: #0070c0"><strong>梯度下降法（Gradient Descent）</strong></span>是最常采用的方法之一，其核心思想非常简单，沿梯度下降的方向逐步迭代。梯度是一个向量，表示的是一个函数在该点处沿梯度的方向变化最快，变化率最大，而梯度下降的方向就是指的负梯度方向。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">根据梯度下降法的定义，其迭代最终必然会终止于一阶导数（对于多元函数来说则是一阶偏导数）为零的点，即驻点。对于可导函数来说，其极值点一定是驻点，而驻点并不一定是极值点，还可能是鞍点。另一方面，极值点也不一定是最值点。下面举几个简单的例子。</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_021.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">上图为函数<img style="width: 54px;height: 33px" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_011.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" width="54" height="33">。从图中可以看出，函数唯一的驻点 （0，0）为其最小值点。</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_027.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">上图为函数<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_025.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">。其一阶导数为<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_007.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">，从而可知其同样有唯一驻点（0，0）。从图中可以看出，函数并没有极值点。</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_023.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">上图为函数<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_028.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">。从图像中可以看出，函数一共有三个驻点，包括两个极小值点和一个极大值点，其中位于最左边的极小值点是函数的最小值点。</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894-152.jpg" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">上图为函数<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_020.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">。其中点 （0，0，0）为其若干个鞍点中的一个。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">从
上面几幅函数图像中可以看出梯度下降法在求解最小值时具有一定的局限性，用一句话概括就是，目标函数必须是凸函数。关于凸函数的判定，对于一元函数来说，
一般是求二阶导数，若其二阶导数非负，就称之为凸函数。对于多元函数来说判定方法类似，只是从判断一元函数的单个二阶导数是否非负，变成了判断所有变量的
二阶偏导数构成的<span style="color: #0070c0"><strong>黑塞矩阵（Hessian Matrix）</strong></span>是否为半正定矩阵。判断一个矩阵是否半正定可以判断所有特征值是否非负，或者判断所有主子式是否非负。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">回
到上面funk-svd的最优化问题上来。经过一番紧张刺激的计算之后，可以很遗憾地发现，我们最终的目标函数是非凸的。这就意味着单纯使用梯度下降法可
能会找到极大值、极小值或者鞍点。这三类点的稳定性按从小到大排列依次是极大值、鞍点、极小值，考虑实际运算中，浮点数运算都会有一定的误差，因此最终结
果很大几率会落入极小值点，同时也有落入鞍点的概率。而对于极大值点，除非初始值就是极大值，否在几乎不可能到达极大值点。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">为了从鞍点和极小值点中脱出，在梯度下降法的基础上衍生出了各式各样的改进算法，例如动态调整步长（即学习率），利用上一次结果的动量法，以及</span><span style="color: #0070c0"><span style="font-size: 14px"><strong>随机梯度下降法</strong></span><strong style="font-size: 14px">（Stochastic Gradient Descent， SGD）</strong></span><span style="font-size: 14px">等等。实际上，这些优化算法在当前最火热的深度学习中也占据着一席之地，例如adagrad、RMSprop，Adam等等。而本文则将主要介绍一下随机梯度下降法。（达观数据 周颢钰）</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><strong><span style="font-size: 14px">随机梯度下降法主要是用来解决求和形式的优化问题</span></strong><span style="font-size: 14px">，与上面需要优化的目标函数一致。其思想也很简单，既然对于求和式中每一项求梯度很麻烦，那么干脆就随机选其中一项计算梯度当作总的梯度来使用好了。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">具体应用到上文中的目标函数</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_022.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">SSE是关于P和Q的多元函数，当随机选定U和I之后，需要枚举所有的k，并且对<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_015.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">，以及<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_034.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">求偏导数。整个式子中仅有<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_033.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">这一项与之相关，通过链式法则可知</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_019.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">在实际的运算中，为了P和Q中所有的值都能得到更新，一般是按照在线学习的方式选择评分矩阵中有分数的点对应的U、I来进行迭代。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">值得一提的是，上面所说的各种优化都无法保证一定能找到最优解。有论文指出，单纯判断驻点是否是局部最优解就是一个NPC问题，但是也有论文指出SGD的解能大概率接近局部最优甚至全局最优。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><strong><span style="font-size: 14px">另外，相比于利用了黑塞矩阵的牛顿迭代法，梯度下降法在方向上的选择也不是最优的。</span></strong><span style="font-size: 14px">牛顿法相当于考虑了梯度的梯度，所以相对更快。而由于其线性逼近的特性，梯度下降法在极值点附近可能出现震荡，相比之下牛顿法就没有这个问题。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="color: #0070c0"><strong><span style="font-size: 14px">但是在实际应用中，计算黑塞矩阵的代价是非常大的，在这里梯度下降法的优势就凸显出来了。</span></strong></span><span style="font-size: 14px">因此，牛顿法往往应用于一些较为简单的模型，如逻辑回归。而对于稍微复杂一些的模型，梯度下降法及其各种进化版本则更受青睐。（达观数据 周颢钰）</span></p>
<p>&nbsp;</p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">3</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">基于Funk-SVD的改进算法</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">到这一步为止，我们已经能通过SGD找到一组分解方案了，然而对于填充矩阵的FunkSVD算法本身而言，目前这个形式是否过于简单了一些呢？</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">实际上，在Funk-SVD被提出之后，出现了一大批改进算法。本文将介绍其中某些经典的改进思路。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px;color: #0070c0"><strong>1</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><strong><span style="color: #262626;font-size: 15px">正则化</span></strong></p>
</section>
</section>
<p style="margin-left: 15px;margin-right: 15px;line-height: 1.75em"><span style="font-size: 14px">对于所有机器学习算法而言，<span style="color: #0070c0"><strong>过拟合</strong></span>一直是需要重视的一个问题，而加入正则化项则是防止过拟合的经典处理方法。对于上面的Funk-SVD算法而言，具体做法就是在损失函数后面加入一个L2正则项，即</span></p>
<p style="margin-left: 15px;margin-right: 15px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_030.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 15px;margin-right: 15px;line-height: 1.75em"><span style="font-size: 14px">其中，λ为正则化系数，而整个求解过程依然可以使用随机梯度下降来完成。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px;color: #0070c0"><strong>2</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><span style="font-size: 15px"><strong><span style="color: #262626">偏置</span></strong></span></p>
</section>
</section>
<p style="margin-left: 15px;margin-right: 15px"><span style="font-size: 14px">考察式子</span></p>
<p style="margin-left: 15px;margin-right: 15px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_009.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">可以发现这个式子表明用户U对物品 I 的评分全部是由U和I之间的联系带来的。</span><span style="font-size: 14px;text-decoration: underline;color: #0070c0"><strong>然而实际上，有很多性质是用户或者物品所独有的。比如某个用户非常严苛，不论对什么物品给出的分数都很低，这仅仅与用户自身有关。</strong></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">又
比如某个物品非常精美，所有用户都会给出较高的分数，这也仅仅与物品自身有关。因此，只通过用户与物品之间的联系来预测评分是不合理的，同时也需要考虑到
用户和物品自身的属性。于是，评分预测的公式也需要进行修正。不妨设整个评分矩阵的平均分为σ，用户U和物品I的偏置分别为<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_029.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">和<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_017.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">，那么此时的评分计算方法就变成了</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_002.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">同时，误差E除了由于M‘计算方式带来的变化之外，也同样需要加入U和I偏置的正则项，因此最终的误差函数变成了</span></p>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_031.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px;color: #0070c0"><strong>3</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><strong><span style="color: #262626;font-size: 15px">隐式反馈</span></strong></p>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">对于实际的应用场景中，经常有这样一种情况：</span><span style="font-size: 14px">用户点击查看了某一个物品，但是最终没有给出评分。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px"><span style="color: #0070c0"><strong>实际上，对于用户点击查看物品这个行为，排除误操作的情况，在其余的情况下可以认为用户被物品的描述</strong></span>，例如贴图或者文字描述等所吸引。这些信息我们称之为隐式反馈。事实上，一个推荐系统中有明确评分的数据是很少的，这类隐式数据才占了大头。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">可以发现，在我们上面的算法当中，并没有运用到这部分数据。于是对于评分的方法，我们可以在显式兴趣+偏置的基础上再添加隐式兴趣，即</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_012.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></p>
<p><span style="font-size: 14px">其中N(U)表示为用户U提供了隐式反馈的物品的集合。这就是svd++算法。</span></p>
<p><span style="font-size: 14px">此时的损失函数也同样需要加上隐式兴趣的正则项，即</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894-152.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong><span style="color: #0070c0">4</span></strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><strong><span style="color: #262626;font-size: 15px">对偶算法</span></strong></p>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">在上面的svd++中，我们是基于用户角度来考虑问题的，很明显我们同样可以基于物品的角度来考虑问题。具体来说就是</span></p>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_018.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">其中 N(I)表示为物品I提供了隐式反馈的用户的集合。类似地，在损失函数中也需要加上隐式兴趣的正则项。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">在实际运用中，可以将原始的svd++得到的结果与对偶算法得到的结果进行融合，使得预测更加准确。然而相比起物品的数目，用户的数目往往是要高出几个量级的，因此对偶算法在储存空间和运算时间的开销上都将远高于原始的svd++，</span><span style="font-size: 14px;text-decoration: underline;color: #0070c0"><strong>如何在效率和准确度之间找到平衡也是一个需要思考的问题。</strong></span><span style="font-size: 14px">（达观数据 周颢钰）</span></p>
<p>&nbsp;</p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">4</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">请因子分解机</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">矩阵分解的思想除了直接应用在分解评分矩阵上之外，其思想也能用在其他地方，接下来介绍的<span style="color: #0070c0"><strong>因子分解机（Factorization Machine，FM）</strong></span>就是一个例子。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">对于经典的逻辑回归算法，其sigmoid函数中的项实际上是一个线性回归</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_004.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">在这里我们认为各个特征之间是相互独立的，而事实上往往有些特征之间是相互关联、相互影响的。<span style="color: #0070c0"><strong>因此，就有必要想办法捕捉这些特征之间的相互影响。</strong></span>简单起见，先只捕捉二阶的关系，即特征之间两两之间的相互影响。具体反映到回归公式上，即为</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: center;line-height: 1.75em"><span style="font-size: 14px"><img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_016.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">具体来说就是使用 <img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_032.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">来描述<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_026.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">，对于w而言，其中可学习的项就对应了评分矩阵中有分值的项，而其他由于数据稀疏导致难以学习的项就相当于评分矩阵中的未评分项。这样一来，不仅解决了数据稀疏性带来的二阶权重学习问题，同时对于参数规模，也从<img title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_003.png" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术">级别降到了O(kn)级别。</span></p>
<p>&nbsp;</p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">5</span></section>
</section>
<section style="margin-left: 0.5em;text-align: left;color: inherit;border-left: 1px solid #0070c0">
<section style="color: inherit;text-align: inherit;padding-left: 6px"></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">与DNN的结合</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">深度学习无疑是近几年来最热门的机器学习技术。注意到隐语义模型中，隐含特征与深度学习中的embedding实际上是一回事，那么是否有可能借助DNN来帮助我们完成矩阵分解的工作呢？</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">实际上，在YouTube的文章《Deep neural networks for YouTube recommendations》中，就已经有了相关技术的应用。</span></p>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px"><img class="aligncenter" title="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2894_002.jpg" alt="技术干货丨想写出人见人爱的推荐系统，先了解经典矩阵分解技术" width="971" height="819"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">上图是YouTube初排模型的图示。具体的流程为：</span><span style="font-size: 14px">首
先通过nlp技术，如word2vec，预训练出所有物品的向量I表示；然后对于每一条用户对物品的点击，将用户的历史点击、历史搜索、地理位置信息等信
息经过各自的embedding操作，拼接起来作为输入，经过MLP训练后得到用户的向量表示U；而最终则是通过 softmax 
函数来校验U*I的结果是否准确。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">相比于传统的矩阵分解算法，使用DNN能为模型带来非线性的部分，提高拟合能力。另一方面，还可以很方便地加入各式各样的特征，提高模型的准确度。（达观数据 周颢钰）</span></p>
<p>&nbsp;</p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">6</span></section>
</section>
</section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">矩阵分解的优缺点</span></section>
</section>
</section>
<p style="line-height: 1.75em"><strong><span style="font-size: 14px;background-color: #0070c0;color: #ffffff">矩阵分解有如下优点：</span></strong></p>
<ol>
<li>
<p style="line-height: 1.75em"><span style="font-size: 14px">能将高维的矩阵映射成两个低维矩阵的乘积，很好地解决了数据稀疏的问题；</span></p>
</li>
<li>
<p style="line-height: 1.75em"><span style="font-size: 14px">具体实现和求解都很简洁，预测的精度也比较好；</span></p>
</li>
<li>
<p style="line-height: 1.75em"><span style="font-size: 14px">模型的可扩展性也非常优秀，其基本思想也能广泛运用于各种场景中。</span></p>
</li>
</ol>
<p style="line-height: 1.75em"><strong><span style="font-size: 14px;background-color: #0070c0;color: #ffffff">相对的，矩阵分解的缺点则有：</span></strong></p>
<ol>
<li>
<p style="line-height: 1.75em"><span style="font-size: 14px">可解释性很差，其隐空间中的维度无法与现实中的概念对应起来；</span></p>
</li>
<li>
<p style="line-height: 1.75em"><span style="font-size: 14px">训练速度慢，不过可以通过离线训练来弥补这个缺点；</span></p>
</li>
<li>
<p style="line-height: 1.75em"><span style="font-size: 14px">实际推荐场景中往往只关心topn结果的准确性，此时考察全局的均方差显然是不准确的。</span></p>
</li>
</ol>
<p style="line-height: 1.75em">
</p><section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">7</span></section>
</section>
<section style="margin-left: 0.5em;text-align: left;color: inherit;border-left: 1px solid #0070c0">
<section style="color: inherit;text-align: inherit;padding-left: 6px"></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">总结</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">矩阵分解作为推荐系统中的经典模型，已经经过了十几年的发展，时至今日依然被广泛应用于推荐系统当中，其基本思想更是在各式各样的模型中发挥出重要作用。<strong>但是对于推荐系统来说，仅仅有一个好的模型是远远不够的。影响推荐系统效果的因素非常之多。</strong>想要打造一个一流的推荐系统，除了一个强大的算法模型之外，更需要想方设法结合起具体业务，不断进行各种尝试、升级，方能取得最终的胜利。</span></p>
<p>&nbsp;</p>
<h2>参考文献</h2>
<p><span style="font-size: 14px;text-align: center">【1】Simon Funk, http://sifter.org/~simon/journal/20061211.html</span></p>
<p style="line-height: 1.75em"><span style="font-size: 14px">【2】Koren, Yehuda, Robert Bell, and Chris Volinsky. "Matrix factorization techniques for recommender systems." <em>Computer</em>42.8 (2009).</span></p>
<p style="line-height: 1.75em"><span style="font-size: 14px">【3】Jahrer, Michael, and Andreas Töscher. "Collaborative filtering ensemble." <em>Proceedings of the 2011 International Conference on KDD Cup 2011-Volume 18</em>. JMLR. org, 2011.</span></p>
<p style="line-height: 1.75em"><span style="font-size: 14px">【4】Rendle, Steffen. "Factorization machines." <em>Data Mining (ICDM), 2010 IEEE 10th International Conference on</em>. IEEE, 2010.</span></p>
<p style="line-height: 1.75em"><span style="font-size: 14px">【5】Covington, Paul, Jay Adams, and Emre Sargin. "Deep neural networks for youtube recommendations." <em>Proceedings of the 10th ACM Conference on Recommender Systems</em>. ACM, 2016.</span></p>
</section>
</section>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/juzhenfenjiedatagrand" title="16:59" rel="bookmark"><time class="entry-date" datetime="2018-03-15T16:59:42+00:00">2018年03月15号</time></a>。属于<a href="http://www.52nlp.cn/category/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f" rel="category tag">推荐系统</a>、<a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" rel="category tag">数据挖掘</a>、<a href="http://www.52nlp.cn/category/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="category tag">深度学习</a>、<a href="http://www.52nlp.cn/category/nlp" rel="category tag">自然语言处理</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f" rel="tag">推荐系统</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" rel="tag">数据挖掘</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">深度学习</a>、<a href="http://www.52nlp.cn/tag/%e7%ae%97%e6%b3%95" rel="tag">算法</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/chenyunwen" title="查看所有由recommender发布的文章" rel="author">recommender</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10137" class="post-10137 post type-post status-publish format-standard hentry category-1490 tag-coursera tag-480 tag-1478 tag-1479 tag-1480 tag-1482 tag-526 tag-1481 tag-1454 tag-1483 tag-1351">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84-%e7%ae%97%e6%b3%95%e8%af%be%e7%a8%8b-%e7%ae%97%e6%b3%95%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90" rel="bookmark">Coursera上数据结构 &amp; 算法课程（公开课）汇总推荐</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84-%e7%ae%97%e6%b3%95%e8%af%be%e7%a8%8b-%e7%ae%97%e6%b3%95%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p>数据结构和算法是基本功，<a href="http://blog.coursegraph.com/coursera.php" rel="noopener" target="_blank">Coursera</a>上有很多数据结构和算法方面的经典课程，这里做个总结。</p>
<p>1. 普林斯顿大学 Sedgewick 教授的 <a href="http://coursegraph.com/coursera-algorithms-part1" rel="noopener" target="_blank">算法1: Algorithms, Part I</a></p>
<p>这门算法课程已经开过很多轮，好评如潮 ，应该算得上是 Coursera 上的明星算法课程了，感兴趣的同学可以参考课程图谱上的旧版 <a href="http://coursegraph.com/coursera_algs4partI" rel="noopener" target="_blank">课程评论</a>，强烈推荐：</p>
<blockquote><p>This course covers the essential information that every 
serious programmer needs to know about algorithms and data structures, 
with emphasis on applications and scientific performance analysis of 
Java implementations. Part I covers elementary data structures, sorting,
 and searching algorithms. Part II focuses on graph- and 
string-processing algorithms.</p></blockquote>
<p>2. 普林斯顿大学 Sedgewick 教授的 <a href="http://coursegraph.com/coursera-algorithms-part2" rel="noopener" target="_blank">算法2: Algorithms, Part II</a></p>
<p>系列课程，依然强烈推荐，感兴趣的同学可以参考早期课程的评价：<a href="http://coursegraph.com/coursera_algs4partII" rel="noopener" target="_blank">http://coursegraph.com/coursera_algs4partII</a></p>
<blockquote><p>
“Part II较Part I在部分Programming 
Assignments上增加了timing和memory的难度，API100%不再意味着全部100%，这正是这门课程的精华之处：不是灌输算法知
识，而是通过实际操作的过程让学员深入理解数据结构和算法调优在经济上的意义。个人很喜欢论坛上大家在Performance 
Thread里贴出自己的report然后交流优化心得的过程，很有圆桌会议的架势。这门课的教授Robert 
Sedgewick师出名门，是Knuth在斯坦福的博士。老爷子年岁已近70，一直活跃在论坛上解答和讨论问题，敬业程度让人赞叹。”</p></blockquote>
<blockquote><p>This course covers the essential information that every 
serious programmer needs to know about algorithms and data structures, 
with emphasis on applications and scientific performance analysis of 
Java implementations. Part I covers elementary data structures, sorting,
 and searching algorithms. Part II focuses on graph- and 
string-processing algorithms.</p></blockquote>
<p>3. 斯坦福大学的 <a href="http://coursegraph.com/coursera-specializations-algorithms" rel="noopener" target="_blank">算法专项课程（Algorithms Specialization）</a></p>
<p>斯坦福大学的算法专项课程系列（Algorithms 
Specialization），这个系列包含4门子课程，涵盖基础的算法主题和高级算法主题，此前评价非常高，五颗星推荐，感兴趣的同学可以关注: 
Learn To Think Like A Computer Scientist-Master the fundamentals of the 
design and analysis of algorithms.</p>
<blockquote><p>Algorithms are the heart of computer science, and the 
subject has countless practical applications as well as intellectual 
depth. This specialization is an introduction to algorithms for learners
 with at least a little programming experience. The specialization is 
rigorous but emphasizes the big picture and conceptual understanding 
over low-level implementation and mathematical details. After completing
 this specialization, you will be well-positioned to ace your technical 
interviews and speak fluently about algorithms with other programmers 
and computer scientists. About the instructor: Tim Roughgarden has been a
 professor in the Computer Science Department at Stanford University 
since 2004. He has taught and published extensively on the subject of 
algorithms and their applications.</p></blockquote>
<p>可参考老版课程评论：<a href="http://coursegraph.com/coursera_algo" rel="noopener" target="_blank">Algorithms: Design and Analysis, Part 1</a> 和 <a href="http://coursegraph.com/coursera_algo2" rel="noopener" target="_blank">Algorithms: Design and Analysis, Part 2</a> </p>
<p>3.1 <a href="http://coursegraph.com/coursera-algorithms-divide-conquer" rel="noopener" target="_blank">Divide and Conquer, Sorting and Searching, and Randomized Algorithms</a></p>
<blockquote><p>The primary topics in this part of the specialization 
are: asymptotic ("Big-oh") notation, sorting and searching, divide and 
conquer (master method, integer and matrix multiplication, closest 
pair), and randomized algorithms (QuickSort, contraction algorithm for 
min cuts).</p></blockquote>
<p>3.2 <a href="http://coursegraph.com/coursera-algorithms-graphs-data-structures" rel="noopener" target="_blank">Graph Search, Shortest Paths, and Data Structures</a></p>
<blockquote><p>The primary topics in this part of the specialization 
are: data structures (heaps, balanced search trees, hash tables, bloom 
filters), graph primitives (applications of breadth-first and 
depth-first search, connectivity, shortest paths), and their 
applications (ranging from deduplication to social network analysis).</p></blockquote>
<p>3.3 <a href="http://coursegraph.com/coursera-algorithms-greedy" rel="noopener" target="_blank">Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming</a></p>
<blockquote><p>
The primary topics in this part of the specialization are: greedy 
algorithms (scheduling, minimum spanning trees, clustering, Huffman 
codes) and dynamic programming (knapsack, sequence alignment, optimal 
search trees).</p></blockquote>
<p>3.4 <a href="http://coursegraph.com/coursera-algorithms-npcomplete" rel="noopener" target="_blank">Shortest Paths Revisited, NP-Complete Problems and What To Do About Them</a></p>
<p>The primary topics in this part of the specialization are: shortest 
paths (Bellman-Ford, Floyd-Warshall, Johnson), NP-completeness and what 
it means for the algorithm designer, and strategies for coping with 
computationally intractable problems (analysis of heuristics, local 
search).</p>
<p>4. 北京大学的 <a href="http://coursegraph.com/coursera-specializations-biancheng-suanfa" rel="noopener" target="_blank">程序设计与算法专项课程系列</a></p>
<p>据说是国内学生选择最多的中文程序设计课程，这个系列包含7门子课程，分别是计算导论与C语言基础， C程序设计进阶 ，C++程序设计， 
算法基础， 数据结构基础， 高级数据结构与算法， 
程序开发项目实践，最后一个项目实践课程联合腾讯公司设计一个实际的应用问题：搜索引擎设计。感兴趣的同学可以关注：</p>
<blockquote><p>本专项课程旨在系统培养你的程序设计与编写能力。系列课程从计算机的基础知识讲起，无论你来自任何学科和行业背景，都能快
速理解；同时我们又系统性地介绍了C程序设计，C++程序设计，算法基础，数据结构与算法相关的内容，各门课之间联系紧密，循序渐进，能够帮你奠定坚实的
程序开发基础；课程全部配套在线编程测试，将有效地训练和提升你编写程序的实际动手能力。并通过结业实践项目为你提供应用程序设计解决复杂现实问题的锻
炼，从而积累实际开发的经验。因此，我们希望本专项课程能够帮助你完成从仅了解基本的计算机知识到能够利用高质量的程序解决实际问题的转变。 </p></blockquote>
<p>5. 加州大学圣地亚哥分校的 <a href="http://coursegraph.com/coursera-specializations-data-structures-algorithms" rel="noopener" target="_blank">数据结构与算法专项课程系列（Data Structures and Algorithms Specialization） </a></p>
<p>这个系列包含5门子课程和1门毕业项目课程，包括算法工具箱，数据结构 ，图算法，字符串算法 ，高级算法与算法复杂度，算法毕业项目 
等，感兴趣的同学可以关注: Master Algorithmic Programming Techniques-Learn algorithms
 through programming and advance your software engineering or data 
science career</p>
<blockquote><p>This specialization is a mix of theory and practice: you 
will learn algorithmic techniques for solving various computational 
problems and will implement about 100 algorithmic coding problems in a 
programming language of your choice. No other online course in 
Algorithms even comes close to offering you a wealth of programming 
challenges that you may face at your next job interview. To prepare you,
 we invested over 3000 hours into designing our challenges as an 
alternative to multiple choice questions that you usually find in MOOCs.
 Sorry, we do not believe in multiple choice questions when it comes to 
learning algorithms...or anything else in computer science! For each 
algorithm you develop and implement, we designed multiple tests to check
 its correctness and running time — you will have to debug your programs
 without even knowing what these tests are! It may sound difficult, but 
we believe it is the only way to truly understand how the algorithms 
work and to master the art of programming. The specialization contains 
two real-world projects: Big Networks and Genome Assembly. You will 
analyze both road networks and social networks and will learn how to 
compute the shortest route between New York and San Francisco (1000 
times faster than the standard shortest path algorithms!) Afterwards, 
you will learn how to assemble genomes from millions of short fragments 
of DNA and how assembly algorithms fuel recent developments in 
personalized medicine.</p></blockquote>
<p>注：本文首发“<a href="http://blog.coursegraph.com/">课程图谱博客</a>”：<a href="http://blog.coursegraph.com/">http://blog.coursegraph.com</a> ，同步发布到这里, 本文链接地址：<a href="http://blog.coursegraph.com/coursera%E4%B8%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95%E8%AF%BE%E7%A8%8B-%E7%AE%97%E6%B3%95%E5%85%AC%E5%BC%80%E8%AF%BE-%E6%B1%87%E6%80%BB%E6%8E%A8%E8%8D%90">http://blog.coursegraph.com/coursera上数据结构-算法课程-算法公开课-汇总推荐</a>  <a href="http://blog.coursegraph.com/?p=736">http://blog.coursegraph.com/?p=736</a></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84-%e7%ae%97%e6%b3%95%e8%af%be%e7%a8%8b-%e7%ae%97%e6%b3%95%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90" title="11:00" rel="bookmark"><time class="entry-date" datetime="2018-03-07T11:00:16+00:00">2018年03月7号</time></a>。属于<a href="http://www.52nlp.cn/category/%e5%85%ac%e5%bc%80%e8%af%be" rel="category tag">公开课</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/coursera" rel="tag">Coursera</a>、<a href="http://www.52nlp.cn/tag/coursera%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">Coursera公开课</a>、<a href="http://www.52nlp.cn/tag/coursera%e8%af%be%e7%a8%8b" rel="tag">Coursera课程</a>、<a href="http://www.52nlp.cn/tag/%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">公开课</a>、<a href="http://www.52nlp.cn/tag/%e5%8c%97%e4%ba%ac%e5%a4%a7%e5%ad%a6" rel="tag">北京大学</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84" rel="tag">数据结构</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84%e4%b8%8e%e7%ae%97%e6%b3%95" rel="tag">数据结构与算法</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">数据结构公开课</a>、<a href="http://www.52nlp.cn/tag/%e6%96%af%e5%9d%a6%e7%a6%8f" rel="tag">斯坦福</a>、<a href="http://www.52nlp.cn/tag/%e6%99%ae%e6%9e%97%e6%96%af%e9%a1%bf" rel="tag">普林斯顿</a>、<a href="http://www.52nlp.cn/tag/%e7%ae%97%e6%b3%95" rel="tag">算法</a>、<a href="http://www.52nlp.cn/tag/%e7%ae%97%e6%b3%95%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">算法公开课</a>、<a href="http://www.52nlp.cn/tag/%e8%af%be%e7%a8%8b" rel="tag">课程</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/52opencourse" title="查看所有由课程图谱发布的文章" rel="author">课程图谱</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10132" class="post-10132 post type-post status-publish format-standard hentry category-354 category-538 tag-a-b tag-1477 tag-1244 tag-970">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/abceshi" rel="bookmark">A/B测试的数学原理与深入理解（达观数据 陈运文）</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/abceshi#comments">1条回复</a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<section style="font-size: 16px;text-align: left">
<p style="margin-left: 8px;margin-right: 8px"><img class="aligncenter wp-image-10133" src="52nlp.cn_files/AB-1024x568.jpg" alt="" srcset="http://www.52nlp.cn/wp-content/uploads/2018/03/AB测试题图-1024x568.jpg 1024w, http://www.52nlp.cn/wp-content/uploads/2018/03/AB测试题图-300x167.jpg 300w, http://www.52nlp.cn/wp-content/uploads/2018/03/AB测试题图-768x426.jpg 768w, http://www.52nlp.cn/wp-content/uploads/2018/03/AB测试题图-624x346.jpg 624w" sizes="(max-width: 593px) 100vw, 593px" width="593" height="328"></p>
<p style="text-align: justify;line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px;text-align: justify"><span style="font-size: 14px;text-align: justify">当面对众多选择时，如何选才能最大化收益（或者说最小化我们的开销）？比如，怎么选择最优的上班的路线才能使途中花费的时间最少？假设每天上下班路线是确定的，我们便可以在账本中记下往返路线的长度。</span></span></p>
<p style="text-align: justify;line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">A/B测试便是基于数据来进行优选的常用方法，<span style="font-size: 14px;text-align: justify">在记录多次上班路线长度后，我们便会从数据中发现到一些模式（例如路线A比路线B花的时间更少），然后最终一致选择某条路线。</span></span></p>
<p style="text-align: justify;line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px;text-align: justify">当A/B测试遇到非简单情况时（如分组不够随机时，或用户量不够大到可以忽略组间差异，或不希望大规模A/B测试长期影响一部分用户的收益），该怎样通过掌握理论知识来更好的指导实践呢？本文尝试通过由浅入深的介绍，希望能够帮助大家对A/B测试有更加深入的理解。</span></p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">1</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">为什么需要A/B测试</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em;text-align: justify"><span style="color: #0070c0"><strong><span style="font-size: 14px">任何问题，只要它的每个选项能够被多次进行测试，并且每个选项在被测试时都能返回固定的结果，那么它就能使用A/B测试技术来进行优化。</span></strong></span><span style="font-size: 14px">在上述例子中，每天的上下班路线是确定的，所以我们能够在账本中记下往返路线的长度。</span></p>
<p style="text-align: justify;line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">那么什么样的路线对于用户来说才是一个好的方案呢？是考虑路线A还是B？什么时候用户才有充分的数据去确定哪条线路是最好的？测试线路好与不好的最优策略又是什么？图1用形式化概括定义了问题。</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_012.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="1121" height="659"></p>
<p style="text-align: center"><span style="color: #a5a5a5;font-size: 14px">图1 形式化定义的问题</span></p>
<section style="width: 100%;vertical-align: top;border-width: 4px 1px 1px 4px;border-style: solid;border-color: #0070c0">
<section style="margin: 15px"><strong style="font-size: 14px">在这个场景中，参与的用户正面临一个选择，根据他的决策会生成一个结果，而这个结果会对应一份给参与者的反馈。假设用户持续地暴露于这个决策，他应该怎么制定获得最大收益（或等效地说，最小成本）的策略？</strong></section>
</section>
</section>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">图
1中假定了用户多次处于需要进行选择的场景中，每一次进行决策都会达成一项结果，而这个结果会关联相应的反馈。在上下班这个例子中，假定他每天都需要上下
班，而且他每次上下班都必须进行线路的选择，产出的结果是这次上下班中所有因素的结合体，反馈就是从这些因素中构建出来的（陈运文 达观数据）。</span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">这是个浅显的例子，在互联网产品研发时，有大量类似的场景需要做出各种正确的选择，例如：</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>1</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><span style="font-size: 14px;color: #262626"><strong>着陆页优化（Landing-page optimization）</strong></span></p>
</section>
</section>
<section style="border: 0px none initial">
<p style="text-align: center;margin-left: 10px;margin-right: 10px">
</p></section>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">在
用户点击去往的页面（着陆页），如何获得最大的转化率（常用计算方法为有购买行为或深度网页交互行为的用户数占网站访问总用户数的比率）。决策要考虑到着
陆页的形式和内容（要从可能已有的3或4个备选方案中做出选择），希望能够从候选集合中选出最好的着陆页，以能够吸引来访的用户，并让深度交互或者购买行
为的概率最大化。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>2</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><span style="font-size: 14px;color: #262626"><strong>广告创意优化（Ad creative optimization）</strong></span></p>
</section>
</section>
<p style="line-height: 1.75em;text-align: justify;margin-left: 10px;margin-right: 10px"><span style="color: #0070c0"><strong><span style="font-size: 14px">在线广告提出了许多适合机器学习技术应用的挑战，其中之一就是如何选择广告的形式和内容。</span></strong></span><span style="font-size: 14px">当我们决定将要进行广告展示，以及确定了广告的价格后，在这个广告位上选择放置什么广告呢？我们需要对大量的决策进行测试，选出正确的广告创意组合。</span></p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">2</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">什么是A/B测试</span></section>
</section>
</section>
<p><span style="font-size: 14px">经常遇到的问题是，我们应该怎么评估各不相同的决策，以及应该采用哪些策略来测试我
们的产出？ A/B测试（A/B 
testing）就是其中之一的方法。A/B测试近年来很受欢迎，但大部分产品经理也许会简单地认为它只不过是一种包含两个组的实验，其实背后有更为复杂
的数学统计理论知识。</span></p>
<section style="border: 0px none initial">
<section style="border: 0px none initial">
<section style="padding: 10px">
<section style="padding: 8px;border: 1px solid #0070c0">
<section style="padding: 10px;border: 1px solid #0070c0">
<section style="width: auto;background-color: #0070c0;color: #ffffff;line-height: 30px;padding-right: 20px;padding-left: 20px;max-width: 60%">具体细节</section>
</section>
<section style="padding: 10px;border: 1px solid #0070c0"><span style="font-size: 14px;text-align: justify">当进行A/B测试时，通常会采用两个（或多个）组：A组和B组。第一个组是对照组，第二个组会改变其中一些因素。就以着陆页优化为例，A组会展示现有的着陆页，B组会展示一个内容或者内容作了某些修改的新着陆页。</span><strong style="font-size: 14px;text-align: justify">A/B测试的目的就是尝试了解新的布局是否在统计上显著地改变了转化率。</strong></section>
</section>
</section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px"><strong>特别值得注意的是</strong>，将用户分配到对应的组需要经过深思熟虑。对于A/B测试，我们可以高效地进行随机分组。当用户数量较大时，各组间用户行为可以假设是相同的（即组间没有偏差）。但是，这里有三个非常重要的关键点，是大家有必要进一步理解其数学理论原理的原因：</span></p>
<section style="border: 0px none initial">
<section style="margin-right: 0%;margin-left: 0%">
<section style="width: 100%;vertical-align: top">
<section>
<section style="padding-right: 0.2em;padding-top: 0.5em">
<section style="vertical-align: top;width: 3em;height: 3em;padding: 3px;border: 1px solid #0070c0">
<section style="width: 100%;height: 100%;background-color: #0070c0;text-align: center;color: #ffffff;font-size: 20.16px;line-height: 1.4">
<section title="">1</section>
</section>
</section>
<section style="vertical-align: top;width: 6px;height: 6px;margin-left: -0.5em;background-color: #0070c0;color: #ffffff"></section>
<section style="vertical-align: top;width: 3px;height: 3px;margin-top: -0.22em;margin-left: 0.1em;background-color: #0070c0;color: #ffffff"></section>
</section>
</section>
<section style="margin-right: 0%;margin-left: 0%">
<section style="width: 100%;vertical-align: top;padding-left: 16px;border-width: 0px">
<section style="width: 100%;vertical-align: top;border-left: 2px dashed #a0a0a0;padding-left: 10px;border-bottom-color: #0070c0;border-top-color: #0070c0;border-right-color: #0070c0">
<section style="margin: -30px 0% 10px">
<section style="width: 100%;vertical-align: top;padding-left: 15px">
<section>
<section style="text-align: justify;font-weight: bold;font-size: 18px">问题1</section>
</section>
</section>
</section>
<section style="margin-right: 0%;margin-bottom: 5px;margin-left: 0%;font-size: 14px"><strong>怎样验证两个组的用户的行为是无偏差、完全相同的</strong></section>
</section>
</section>
</section>
</section>
</section>
</section>
<section style="border: 0px none initial">
<section style="margin-right: 0%;margin-left: 0%">
<section style="width: 100%;vertical-align: top">
<section>
<section style="padding-right: 0.2em;padding-top: 0.5em">
<section style="vertical-align: top;width: 3em;height: 3em;padding: 3px;border: 1px solid #0070c0">
<section style="width: 100%;height: 100%;background-color: #0070c0;text-align: center;color: #ffffff;font-size: 20.16px;line-height: 1.4">
<section title="">2</section>
</section>
</section>
<section style="vertical-align: top;width: 6px;height: 6px;margin-left: -0.5em;background-color: #0070c0;color: #ffffff"></section>
<section style="vertical-align: top;width: 3px;height: 3px;margin-top: -0.22em;margin-left: 0.1em;background-color: #0070c0;color: #ffffff"></section>
</section>
</section>
<section style="margin-right: 0%;margin-left: 0%">
<section style="width: 100%;vertical-align: top;padding-left: 16px;border-width: 0px">
<section style="width: 100%;vertical-align: top;border-left: 2px dashed #a0a0a0;padding-left: 10px;border-bottom-color: #0070c0;border-top-color: #0070c0;border-right-color: #0070c0">
<section style="margin: -30px 0% 10px">
<section style="width: 100%;vertical-align: top;padding-left: 15px">
<section>
<section style="text-align: justify;font-weight: bold;font-size: 18px">问题2</section>
</section>
</section>
</section>
<section style="margin-right: 0%;margin-bottom: 5px;margin-left: 0%;font-size: 14px">
<p style="line-height: 1.75em"><strong>当两个组的用户行为不完全相同时（例如分组不够随机或者组内用户数量较小时），该如何设计AB测试以实现期望的验证结果</strong></p>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
<section style="border: 0px none initial">
<section style="margin-right: 0%;margin-left: 0%">
<section style="width: 100%;vertical-align: top">
<section>
<section style="padding-right: 0.2em;padding-top: 0.5em">
<section style="vertical-align: top;width: 3em;height: 3em;padding: 3px;border: 1px solid #0070c0">
<section style="width: 100%;height: 100%;background-color: #0070c0;text-align: center;color: #ffffff;font-size: 20.16px;line-height: 1.4">
<section title="">3</section>
</section>
</section>
<section style="vertical-align: top;width: 6px;height: 6px;margin-left: -0.5em;background-color: #0070c0;color: #ffffff"></section>
<section style="vertical-align: top;width: 3px;height: 3px;margin-top: -0.22em;margin-left: 0.1em;background-color: #0070c0;color: #ffffff"></section>
</section>
</section>
<section style="margin-right: 0%;margin-left: 0%">
<section style="width: 100%;vertical-align: top;padding-left: 16px;border-width: 0px">
<section style="width: 100%;vertical-align: top;border-left: 2px dashed #a0a0a0;padding-left: 10px;border-bottom-color: #0070c0;border-top-color: #0070c0;border-right-color: #0070c0">
<section style="margin: -30px 0% 10px">
<section style="width: 100%;vertical-align: top;padding-left: 15px">
<section>
<section style="text-align: justify;font-weight: bold;font-size: 18px">问题3</section>
</section>
</section>
</section>
<section style="margin-right: 0%;margin-bottom: 5px;margin-left: 0%;font-size: 14px">
<p style="line-height: 1.75em"><strong>当用户基础行为受其他因素影响发生整体变化了呢？例如季节、时间波动、热度等因素影响下，怎样更好的剔除干扰来评估结果</strong></p>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">3</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe"><strong style="font-size: 14px">AB测试的统计理论</strong></span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">假
设我们已经构建了两组数目较大的用户组，这些用户组的区别仅在于他们到达的着陆页。我们现在希望能测试两组间的转化率在统计上是否存在明显差异。由于样本
量大，我们可以采用双样本单尾z-检验（two-sample, one-tailed 
z-test）。另外，对于较小的样本集合，我们可以依赖于t-检验。</span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px"><strong>z检验（z-test）</strong>是在数据是正态分布和随机抽样的假设下运行的，目的是验证测试集（B组）是否与该对照集（A组）有显著不同，但是如何执行这个测试呢？</span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">假
设有来自A组和B组中的每一组的5,000个样本。我们需要一个数学公式来说明我们的零假设（null 
hypothesis）——两组群体的转化率没有显著的正差异，和备择假设（或称对立假设，alternative 
hypothesis）——不同人群间的转化率确实存在着正差异。</span></p>
<p style="margin-left: 10px;margin-right: 10px;text-align: justify;line-height: 1.75em"><span style="font-size: 14px">我
们可将采样转化率视为一个正态分布的随机变量，也就是说，采样的转化率是在正态分布下对转化率的一个观测。要了解这一点，请考虑从同一组中提取多个样本进
行实验将导致略有不同的转化率。每当对某组进行抽样时，可获得群体转化率的估计，对于A组和B组都是如此。为此我们提出一个新的正态随机变量，它是A和B
组的随机变量的组合，是差值的分布。让我们用X来表示这个新的随机变量，定义为:</span></p>
<p><img style="width: 118px;height: 30px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814_007.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></p>
<p style="margin-left: 8px;margin-right: 8px"><span style="font-size: 14px">其中，Xe表示实验组的转化率的随机变量，Xn表示对照组的转化率的随机变量。现在我们可以写出零假设和备择假设。零假设可以表示为：</span></p>
<p><img style="width: 100px;height: 25px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814_002.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></p>
<p style="margin-left: 8px;margin-right: 8px"><span style="font-size: 14px">这表示实验组和对照组是相同的。两个随机变量Xe和Xn分布在相同的群体平均值周围，所以我们的新随机变量X应该分布在0左右。我们的备择假设可以表示如下:</span></p>
<p><img style="width: 88px;height: 24px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814_003.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">实验组的随机变量的期望值大于对照组的期望值；该群体的平均值较高。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">我们可以在零假设的前提下，对X的分布执行单尾z检验，以确定是否有证据支持备择假设。为了达到这个目的，我们对X进行采样，计算标准分，并测试已知的显著性水平。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">X的采样等效于运行两个实验，确定它们各自的转化率，并将对照组和实验组的转化率相减。按照标准分的定义，可以写作：</span></p>
<p><img style="width: 257px;height: 34px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814_006.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">其中，P_experiment是实验组的转化率，P_control&nbsp;是对照组的转化率，SE是转化率差值的标准差。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">为确定标准误差，注意到转化过程是符合二项分布的，因此访问该网站可以被看作单次伯努利试验（single Bernoulli trial），而积极结果（完成转化）的可能性是未知的。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">假设样本数量足够大，我们可以使用广泛采用的Wald方法（</span><span style="font-size: 14px;color: #888888">参
考Lawrence D. Brown, T. Tony Cai, and Anirban DasGupta, “Confidence 
Intervals for a Binomial Proportion and Asymptotic Expansions,” The 
Annals of Statistics 30, no. 1 (2002): 160–201.</span><span style="font-size: 14px">）将该分布近似为正态分布。为了捕获特定转化率的不确定性，我们可以将标准误差（SE）写入实验组和对照组，其中p是转化的可能性，n是样本数量，具体如下：</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px"><img style="height: 36px;font-size: 16px;width: 115px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814-151.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">从二项分布（np（1-p））的方差得到分子，而分母表示当采用更多的样本时，转化率的误差会随之下降。请注意正面结果的概率等同于转化率，并且因为两个变量的标准误差可以通过相加来合并，得到如下结果：</span></p>
<p><img style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814-15.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="341"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">通过替换，可获得如下的z检验公式，这是一个符合二项分布的Wald（或正态）区间的公式：</span></p>
<p><img style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_008.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="426"></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">z的值越大，反对零假设的证据就越多。为了获得单尾测试的90％置信区间，我们的z值将需要大于1.28。这实际上这是指在零假设（A组和B组的人口平均值是相同的）的条件下，等于或大于这个转化率差值的偶然发生的概率小于10％。 </span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="color: #0c0c0c"><strong><span style="font-size: 14px">换句话说，在对照组和实验组的转化率来自具有相同平均值的分布的假设前提下，如果运行相同的实验100次，只会有10次具有这样的极端值。我们可以通过95％的置信区间，更严格的边界和更多的证据来反对零假设，这时需要将z值增加到1.65。</span></strong></span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">研
究影响z大小的因素会带来很多有用的帮助。很显然，如果在一个给定的时间点从一个实验集和一个对照集中提取两个转化率，转化率的差值越大将导致z分数越
大。因此就有了更多的证据表明两个集合分别来自不同的人群，而且这些人群带有不同的均值。然而样品的数量也很重要，如你所见，大量样本将导致总体较小的标
准误差。这表明运行实验的时间越长，转化率的估算越准确。</span></p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">4</span></section>
</section>
</section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">评估效果的代码实现</span></section>
</section>
</section>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">设想你在负责大型零售网站，设计团队刚刚修改了着陆页。每周有约20,000用户，并可以量化用户的转化率：即购买产品的百分比。设计团队向你保证新网站将带来更多的客户。但你不太确定，希望运行A / B测试来看看效果是否真的会提高。</span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="font-size: 14px">用
户在第一次访问网站时被随机分配到A组或B组，并在实验期间始终保留在该组中，实验结束时评估两组用户的平均转化率。统计结果是，新着陆页的平均转化率是
0.002，而原先的着陆页的平均转化率是0.001。在着陆页永久更改为新设计之前，你需要知道这一增长是否足够明确。下面这段代码帮你回答这个问题。</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_004.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="959" height="420"></p>
<p style="line-height: 1.75em;text-align: justify"><span style="font-size: 14px">这
段代码获取实验中z的值，在上述参数条件下z值为1.827，超过了92％置信区间，但不在95％的区间内。可以说，从控制分布中抽取数据的概率小于
0.08。因此在该区间内数据提升是显著的。我们应该否定零假设，接受备择假设，即组之间有差异，第二组具有较高的转化率。如果我们控制了用户组的所有其
他方面，就意味着网站的新设计产生了积极的效果。</span></p>
<p style="line-height: 1.75em;text-align: justify"><span style="font-size: 14px">你
应该能够从代码中看到转化率分布的标准误差对返回的z值有直接影响。 
对给定的常数值p_experiment和p_control，两个组的SE越高，z的数值越小，结果就越不显著。还注意到由于SE的定义，z的数值与样
本的数量具有直接关系，对于给定的转换概率也同样如此。图2展示了这种关系。</span></p>
<figure><img class="aligncenter" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_002.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="971" height="778"></figure>
<p style="text-align: center"><span style="font-size: 14px;color: #a5a5a5">图2</span></p>
<p style="line-height: 1.75em;text-align: justify;margin-left: 8px;margin-right: 8px"><span style="font-size: 14px"><strong>图2</strong> <strong>展示了A / B组的固定转化率，以及A / B组中的用户数量和z值之间的关系。</strong> <strong>假设转化率不会随着我们收集更多数据而改变，我们需要每个组中大约3,000个用户达到70％的置信区间。</strong> <strong>要达到80%的置信区间时需要每组约5000个用户，达到90%时需要 7500个用户，达到95%时需要12000个用户。</strong></span></p>
<p style="margin-left: 8px;margin-right: 8px"><span style="font-size: 14px">图2中可见对于两个组的给定转化率，测试组中的用户越多，备择假设的证据就越充分。直观上来看这很容易理解：当收集的数据越多，我们对结果越自信！我们也可以绘制一张类似的图，保持用户数量不变，改变组之间的差异。<span style="color: #0070c0"><strong>但必须注意，对正在关注的应用，不应该期望效果的大幅度变化。</strong></span></span></p>
<p>&nbsp;</p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">5</span></section>
</section>
<section style="margin-left: 0.5em;text-align: left;color: inherit;border-left: 1px solid #0070c0">
<section style="color: inherit;text-align: inherit;padding-left: 6px"></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">A/B测试方法的副作用和处理办法</span></section>
</section>
</section>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="font-size: 14px">对于非常小的效果变化，往往都需要创建相当大的对照组和测试组来实现AB测试，这个的代价往往是很大的。设想下在零售商场中，每天观察到的用户数量，往往需要很久的时间才能得出明显的结论。在实际业务应用中，会遇到的问题是：</span><strong style="font-size: 14px">当你运行测试时整体运行的效果是受到很大影响的，因为必须有一半的用户处于效果不佳的实验组，或者有一半的用户处于效果不佳的对照组，而且你必须等待测试完成才能停止这种局面。</strong></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="color: #0070c0"><strong><span style="font-size: 14px">这是被称为探索利用难题（explore-exploit conundrum）的一个经典问题。我们需要运行次优方法，以探索空间，并找到效果更好的解决方案，而一旦找到了更好的解决方案，我们还需要尽快利用它们来实现效果提升。</span></strong></span><span style="font-size: 14px">能否可以更快地利用新的解决方案，而不必等待测试完全完成呢？答案是肯定的。下面简单介绍下多臂赌博机（multi-armed bandit，MAB）的概念。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>1</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><span style="font-size: 14px;color: #262626"><strong>多臂赌博机的定义</strong></span></p>
</section>
</section>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="font-size: 14px">多
臂赌博机（multi-armed bandit，MAB）的名字来源于著名的赌博游戏角子赌博机（one-armed 
bandit）。对那些从没去过赌场的人，我们来做下解释：角子机（又称老虎机）是一个需要你拉杠杆（或摇臂）的赌博机器，根据机器展示的数值，你可能会
得到一笔奖励，也可能（更大几率）得不到任何东西。和你想的一样，这些机器的设置都对庄家有利，所以能获的奖励的几率是非常非常小的。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em;text-align: justify"><span style="font-size: 14px">多
臂赌博机（理论上的）扩展了这种形式，想象你面对的是一堆角子赌博机，每个赌博机都被分配按照一个独立的概率进行奖励。作为一个玩家，你不知道在这些机器
后的获奖概率，你唯一可以找到获奖概率的方法是进行游戏。你的任务是通过玩这些机器，最大限度地提高所获的奖励。那么你应该使用什么策略呢？</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>2</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<p><span style="font-size: 14px;color: #262626"><strong>多臂赌博机策略</strong></span></p>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">为了更严格地定义问题，我们通过数学形式化来表达，假设现在有k个赌博机，可观察到的每台的获奖概率等于</span><span style="font-size: 14px">p_k</span><span style="font-size: 14px">。假设一次只能拉动一个摇臂，并且赌博机只会按照它关联的概率机型奖励。这是一个设置了限定局数的有限次的游戏。在游戏期间任意时间点时，水平线H被定义为允许的剩余游戏的数量。</span></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">对所有机器用户会尝试最大化的获奖回报。在游戏中的任一时间点，我们都可以通过使用称为遗憾值（regret）来度量用户的表现。遗憾值的意思是，假设用户能在每一步选择最优的赌博机，得到的奖励和目前获得的实际奖励的差值。遗憾值的数学定义为:</span></p>
<p><img style="width: 149px;height: 60px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814_004.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></p>
<p style="text-align: justify;margin-left: 8px;margin-right: 8px;line-height: 1.75em"><span style="font-size: 14px">其
中T表示我们到目前为止进行过的步数，r_t表示在第t步获得的奖励，u_opt表示每一局从最优赌博机返回来的期望奖励。遗憾值的数值越低，策略越优。
但因为这个度量值会受到偶然性的影响（奖励可能会被从最优赌博机选择中获得的期望奖励更高），我们可以选择使用遗憾值的期望值代替,定义为:</span></p>
<p><img style="width: 113px;height: 62px" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-2814_005.png" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起"></p>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">其中μ_t是在第t步从赌博机中获得的平均奖励（不可观测的）。因为第二项是来自所选策略的期望奖励，所以它将小于或等于来自最优策略（每一步都选择最优的赌博机）的期望奖励。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>3</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<h2 style="font-weight: bold;line-height: 1.6em"><span style="font-size: 14px"><strong>Epsilon优先方法</strong></span></h2>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">Epsilon优先（Epsilon first）是MAB策略中最简单的一种方式，它被认为和事先执行A/B测试方法具有同等意义。给定ε，执行探索空间操作的次数为(1 – ε) × N，其中N是游戏中总共的局数，剩余的次数都是执行后续探索的局数。</span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">update_best_bandit算法会持续统计记录每一个赌博机的奖励收入和游戏局数。变best_bandit会在每一局结束进行更新，记录当前具有最高获奖概率的赌博机的编号，流程如下：</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_005.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="1001" height="478"></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>4</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<h2 style="font-weight: bold;line-height: 1.6em"><span style="font-size: 14px"><strong>Epsilon贪婪</strong></span></h2>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">Epsilon贪婪（epsilon-greedy）策略中，ε表示我们进行探索空间的概率，和进行利用已知最优摇臂的事件互斥</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_010.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="1200" height="427"></p>
<p><strong><span style="font-size: 14px">该方法的特点：</span></strong><span style="font-size: 14px">不需要等到探索阶段完成，才能开始利用有关赌博机的奖励表现的知识。但要小心，该算法不会考虑效果数据的统计意义。因此可能发生这样的情况：个别赌博机的奖励峰值导致后续的所有局游戏都错误地选择了这个赌博机（陈运文 达观数据）。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>5</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<h2 style="font-weight: bold;line-height: 1.6em"><span style="font-size: 14px"><strong>Epsilon递减</strong></span></h2>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">Epsilon递减（epsilon-decreasing）策略在实验开始阶段，会有一个很高的ε值，所以探索空间的可能性很高。ε值会随着水平线H上升而不断递减，致使利用似然知识的可能性更高。</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_007.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="1134" height="497"></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">需要注意这里有几种方法去来选择一个最优的速率来更新ε值，具体取决于赌博机的数量，以及他们各自进行奖励的权重。</span></p>
<section style="border: 0px none initial">
<section style="text-align: center">
<section style="background-color: #fefefe;padding-right: 10px;padding-left: 10px"><span style="font-size: 28px"><strong>6</strong></span></section>
<section style="margin-top: -1.2em;margin-bottom: 0.65em">
<section style="border-top: 1px solid #0070c0;border-right-color: #0070c0;border-bottom-color: #0070c0;border-left-color: #0070c0;width: 20%;margin-right: auto;margin-left: auto"></section>
</section>
<h2 style="font-weight: bold;line-height: 1.6em"><span style="font-size: 14px"><strong>贝叶斯赌博机</strong></span></h2>
</section>
</section>
<p style="text-align: justify;line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">与A / B测试类似，贝叶斯赌博机（Bayesian bandits）假设每个赌博机的获奖概率被建模为获奖概率的分布。当我们开始实验时，每个赌博机都有一个通用的先验概率（任意赌博机的奖励比率初始都是同等的）。</span></p>
<p style="text-align: justify;line-height: 1.75em;margin-left: 10px;margin-right: 10px"><span style="font-size: 14px">在某一个赌博机上进行的局数越多，我们对它的奖励信息就了解越多，所以基于可能的奖励概率更新其获奖概率分布。当需要选择玩哪一个赌博机的时候，从获奖概率分布中采样，并选择对应样本中具有最高奖励比率的赌博机。图3提供了在给定时间内对三个赌博机所含信息的图形化表示。</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_009.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="750" height="622"></p>
<p style="text-align: center"><span style="color: #a5a5a5"><strong style="font-size: 14px">图3</strong></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em;text-align: justify"><span style="font-size: 14px"><strong>使用贝叶斯赌博机策略对三个赌博机的获奖概率信息进行建模。第1、2和3个赌博机的平均获奖率分别为0.1、0.3和0.4。</strong> <strong>第1个赌博机具有较低的平均值而且方差也比较大，第2个赌博机具有较高的平均值和较小的方差，第3个赌博机具有更高的平均值和更小的方差。</strong></span></p>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em;text-align: justify"><span style="font-size: 14px">可
以看到关于赌博机的获奖概率分布的信息被编码为三个分布。每个分布具有递增的平均值和递减的方差。因此，我们不太确定奖励期望值为0.1的真实奖励率，最
可靠的是奖励期望值为0.4的赌博机。因为赌博机的选择是通过对分布进行抽样来进行的，所以分布期望值是0.1的赌博机的摇臂也可能被拉动。这个事件会发
生在第2个赌博机和第3个赌博机的采样样本奖励值异常小，而且第1个赌博机的采样样本异常大时，相应代码如下（陈运文 达观数据）：</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_003.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="760" height="324"></p>
<section style="border: 0px none initial">
<section style="margin-top: 1em;margin-bottom: 1em;color: #333333;border-bottom: 2px solid #0070c0;clear: both;text-align: right">
<section style="float: left">
<section style="color: inherit;font-size: 30px;line-height: 0em">
<section style="color: inherit;border-color: #0070c0">NO.<span title="">6</span></section>
</section>
</section>
<section style="border-top: 30px solid transparent;border-right: 15px solid #0070c0;border-bottom: 0px solid transparent;border-left-color: #0070c0;vertical-align: top"></section>
<section style="padding: 3px 10px;height: 30px;color: #ffffff;background-color: #0070c0"><span style="color: #fefefe">总结</span></section>
</section>
</section>
<p style="margin-left: 10px;margin-right: 10px;line-height: 1.75em"><span style="font-size: 14px">A/B测试和贝叶斯赌博机的各自的优点和局限是：两者有各自适用的场景，也验证的变量数量也各不相同，具体如下表。</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_006.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="918" height="140"></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="font-size: 14px">此
外，两个方法的收敛速度也很不一样。在A/B测试中是指获得统计意义，在贝叶斯赌博机中是指累积遗憾值不再增加。以本章最开始的网站优化为例，首先请注
意，任何行为的改变可能是微小的（&lt;0.01），而我们已经知道贝叶斯赌博机相比大的改变提升，需要更多的收敛时间。如果加了多种选择，在同一个实
验中测试多种登陆页面，将更加会影响收敛速度。假如用户变化导致的底层分布变的比模型收敛更快呢？比如，季节趋势，销售或者其他因素可能会影响。</span></p>
<p><img class="aligncenter" style="cursor: pointer;text-align: center" title="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" src="52nlp.cn_files/beepress-beepress-weixin-zhihu-jianshu-plugin-2-4-2-281_011.jpeg" alt="技术干货 | 如何选择上班路线最省时间？从A/B测试数学原理说起" width="892" height="119"></p>
<p style="line-height: 1.75em;margin-left: 10px;margin-right: 10px;text-align: justify"><span style="font-size: 14px">显然，收集的数据越多，对效果的潜在变化的把握度就越高。当2个组划分本身就存在统计差异时，通过多臂赌博机而不是A/B测试的方法可以从概率上修正我们选择的分布。本文还重点介绍了z检验（z-test）的数学知识，因为其构成了A/B测试的统计理论基础。</span></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/abceshi" title="11:19" rel="bookmark"><time class="entry-date" datetime="2018-03-02T11:19:37+00:00">2018年03月2号</time></a>。属于<a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98" rel="category tag">数据挖掘</a>、<a href="http://www.52nlp.cn/category/%e7%bb%9f%e8%ae%a1%e5%ad%a6" rel="category tag">统计学</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/a-b%e6%b5%8b%e8%af%95" rel="tag">A/B测试</a>、<a href="http://www.52nlp.cn/tag/%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86" rel="tag">数学原理</a>、<a href="http://www.52nlp.cn/tag/%e7%bb%9f%e8%ae%a1%e5%ad%a6" rel="tag">统计学</a>、<a href="http://www.52nlp.cn/tag/%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0" rel="tag">统计学习</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/chenyunwen" title="查看所有由recommender发布的文章" rel="author">recommender</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
							
	<article id="post-10117" class="post-10117 post type-post status-publish format-standard hentry category-1153 category-nlp tag-ai tag-andrew-ng tag-andrew-ng- tag-cnn tag-coursera tag-deep-learning tag-deep-learning- tag-deeplearning-ai tag-gru tag-lstm tag-machine-learning tag-mooc tag-tensorflow tag-343 tag-1367 tag-480 tag-1393 tag-522 tag-1361 tag-1362 tag-1363 tag-1239 tag-581 tag-1018 tag-1350 tag-1380 tag-1379 tag-1343 tag-1357 tag-1354 tag-1358 tag-1359 tag-1356 tag-1355 tag-1381 tag-1351">
				<header class="entry-header">
			
						<h1 class="entry-title">
				<a href="http://www.52nlp.cn/andrew-ng-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%ac%e5%bc%80%e8%af%be%e7%b3%bb%e5%88%97%e7%ac%ac%e4%ba%94%e9%97%a8%e8%af%be%e7%a8%8b%e5%ba%8f%e5%88%97%e6%a8%a1%e5%9e%8b%e5%bc%80%e8%af%be" rel="bookmark">Andrew Ng 深度学习公开课系列第五门课程序列模型开课</a>
			</h1>
										<div class="comments-link">
					<a href="http://www.52nlp.cn/andrew-ng-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%ac%e5%bc%80%e8%af%be%e7%b3%bb%e5%88%97%e7%ac%ac%e4%ba%94%e9%97%a8%e8%af%be%e7%a8%8b%e5%ba%8f%e5%88%97%e6%a8%a1%e5%9e%8b%e5%bc%80%e8%af%be#respond"><span class="leave-reply">发表回复</span></a>				</div><!-- .comments-link -->
					</header><!-- .entry-header -->

				<div class="entry-content">
			<p>Andrew Ng <a href="http://www.52nlp.cn/andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%B0%8F%E8%AE%B0" rel="noopener" target="_blank">深度学习课程</a>系列第五门课程<a href="https://click.linksynergy.com/deeplink?id=9IqCvd3EEQc&amp;mid=40328&amp;murl=http%3A%2F%2Fwww.coursera.org%2Flearn%2Fnlp-sequence-models" rel="noopener" target="_blank">序列模型（Sequence Models）</a>在1月的尾巴终于开课 ，在跳票了几次之后，这门和NLP比较相关的深度学习课程终于开课了。这门课程属于Coursera上的<a href="http://www.52nlp.cn/deeplearning.php" rel="noopener" target="_blank">深度学习专项系列 </a>，这个系列有5门课，目前终于完备，感兴趣的同学可以关注：<a href="http://www.52nlp.cn/deeplearning.php" rel="noopener" target="_blank">Deep Learning Specialization</a></p>
<blockquote><p>This course will teach you how to build models for 
natural language, audio, and other sequence data. Thanks to deep 
learning, sequence algorithms are working far better than just two years
 ago, and this is enabling numerous exciting applications in speech 
recognition, music synthesis, chatbots, machine translation, natural 
language understanding, and many others. You will: - Understand how to 
build and train Recurrent Neural Networks (RNNs), and commonly-used 
variants such as GRUs and LSTMs. - Be able to apply sequence models to 
natural language problems, including text synthesis. - Be able to apply 
sequence models to audio applications, including speech recognition and 
music synthesis. This is the fifth and final course of the Deep Learning
 Specialization.</p></blockquote>
<p>这门课程主要面向自然语言，语音和其他序列数据进行深度学习建模，将会学习递归神经网络，GRU，LSTM等内容，以及如何将其应用到语音识别，机器翻译，自然语言理解等任务中去。个人认为这是目前互联网上最适合入门<a href="http://www.52nlp.cn/tag/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" rel="noopener" target="_blank">深度学习</a>的系列系列课程了，Andrew Ng 老师善于讲课，另外用Python代码抽丝剥茧扣作业，课程学起来非常舒服，希望最后这门RNN课程也不负众望。参考我之前写得两篇小结：</p>
<p><a href="http://www.52nlp.cn/andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%B0%8F%E8%AE%B0" rel="noopener" target="_blank">Andrew Ng 深度学习课程小记</a></p>
<p><a href="http://www.52nlp.cn/andrew-ng-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%B0%8F%E7%BB%93" rel="noopener" target="_blank">Andrew Ng (吴恩达) 深度学习课程小结</a></p>
<p>额外推荐: <a href="http://blog.coursegraph.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86" rel="noopener" target="_blank">深度学习课程资源整理</a></p>
					</div><!-- .entry-content -->
		
		<footer class="entry-meta">
			本条目发布于<a href="http://www.52nlp.cn/andrew-ng-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%ac%e5%bc%80%e8%af%be%e7%b3%bb%e5%88%97%e7%ac%ac%e4%ba%94%e9%97%a8%e8%af%be%e7%a8%8b%e5%ba%8f%e5%88%97%e6%a8%a1%e5%9e%8b%e5%bc%80%e8%af%be" title="12:28" rel="bookmark"><time class="entry-date" datetime="2018-02-01T12:28:48+00:00">2018年02月1号</time></a>。属于<a href="http://www.52nlp.cn/category/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="category tag">深度学习</a>、<a href="http://www.52nlp.cn/category/nlp" rel="category tag">自然语言处理</a>分类，被贴了 <a href="http://www.52nlp.cn/tag/ai" rel="tag">AI</a>、<a href="http://www.52nlp.cn/tag/ai%e8%af%be%e7%a8%8b" rel="tag">AI课程</a>、<a href="http://www.52nlp.cn/tag/andrew-ng" rel="tag">Andrew Ng</a>、<a href="http://www.52nlp.cn/tag/andrew-ng-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">Andrew Ng 机器学习</a>、<a href="http://www.52nlp.cn/tag/andrew-ng-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">Andrew Ng 深度学习</a>、<a href="http://www.52nlp.cn/tag/andrew-ng-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b" rel="tag">Andrew Ng 深度学习课程</a>、<a href="http://www.52nlp.cn/tag/cnn" rel="tag">CNN</a>、<a href="http://www.52nlp.cn/tag/coursera" rel="tag">Coursera</a>、<a href="http://www.52nlp.cn/tag/coursera%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">Coursera深度学习</a>、<a href="http://www.52nlp.cn/tag/coursera%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b" rel="tag">Coursera深度学习课程</a>、<a href="http://www.52nlp.cn/tag/deep-learning" rel="tag">Deep Learning</a>、<a href="http://www.52nlp.cn/tag/deep-learning-%e4%b8%93%e9%a2%98" rel="tag">Deep Learning 专题</a>、<a href="http://www.52nlp.cn/tag/deep-learning-%e8%af%be%e7%a8%8b" rel="tag">Deep Learning 课程</a>、<a href="http://www.52nlp.cn/tag/deeplearning-ai" rel="tag">deeplearning.ai</a>、<a href="http://www.52nlp.cn/tag/gru" rel="tag">GRU</a>、<a href="http://www.52nlp.cn/tag/lstm" rel="tag">LSTM</a>、<a href="http://www.52nlp.cn/tag/machine-learning" rel="tag">Machine Learning</a>、<a href="http://www.52nlp.cn/tag/mooc" rel="tag">MOOC</a>、<a href="http://www.52nlp.cn/tag/tensorflow" rel="tag">TensorFlow</a>、<a href="http://www.52nlp.cn/tag/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd" rel="tag">人工智能</a>、<a href="http://www.52nlp.cn/tag/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e8%af%be%e7%a8%8b" rel="tag">人工智能课程</a>、<a href="http://www.52nlp.cn/tag/%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">公开课</a>、<a href="http://www.52nlp.cn/tag/%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" rel="tag">卷积神经网络</a>、<a href="http://www.52nlp.cn/tag/%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%ae%97%e6%b3%95" rel="tag">反向传播算法</a>、<a href="http://www.52nlp.cn/tag/%e5%90%b4%e6%81%a9%e8%be%be" rel="tag">吴恩达</a>、<a href="http://www.52nlp.cn/tag/%e5%90%b4%e6%81%a9%e8%be%be%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">吴恩达深度学习</a>、<a href="http://www.52nlp.cn/tag/%e5%90%b4%e6%81%a9%e8%be%be%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b" rel="tag">吴恩达深度学习课程</a>、<a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="tag">机器学习</a>、<a href="http://www.52nlp.cn/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">机器学习公开课</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" rel="tag">深度学习</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%ac%e5%bc%80%e8%af%be" rel="tag">深度学习公开课</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%b7%a5%e5%85%b7" rel="tag">深度学习工具</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a1%86%e6%9e%b6" rel="tag">深度学习框架</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b" rel="tag">深度学习课程</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b%e4%bd%93%e9%aa%8c" rel="tag">深度学习课程体验</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b%e5%80%bc%e4%b8%8d%e5%80%bc" rel="tag">深度学习课程值不值</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b%e5%b0%8f%e7%bb%93" rel="tag">深度学习课程小结</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b%e6%8e%a8%e8%8d%90" rel="tag">深度学习课程推荐</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b%e6%b5%8b%e8%af%84" rel="tag">深度学习课程测评</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b%e8%af%84%e4%bb%b7" rel="tag">深度学习课程评价</a>、<a href="http://www.52nlp.cn/tag/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%b5%84%e6%96%99" rel="tag">深度学习资料</a>、<a href="http://www.52nlp.cn/tag/%e8%af%be%e7%a8%8b" rel="tag">课程</a> 标签。<span class="by-author">作者是<span class="author vcard"><a class="url fn n" href="http://www.52nlp.cn/author/admin" title="查看所有由52nlp发布的文章" rel="author">52nlp</a></span>。</span>								</footer><!-- .entry-meta -->
	</article><!-- #post -->
			
						<nav id="nav-below" class="navigation" role="navigation">
				<h3 class="assistive-text">文章导航</h3>
				<div class="nav-previous"><a href="http://www.52nlp.cn/page/2"><span class="meta-nav">←</span> 早期文章</a></div>
				<div class="nav-next"></div>
			</nav><!-- .navigation -->
		
		
		</div><!-- #content -->
	</div><!-- #primary -->


			<div id="secondary" class="widget-area" role="complementary">
			<aside id="text-5" class="widget widget_text"><h3 class="widget-title">关注我们的微信公众号</h3>			<div class="textwidget"><a href="http://blog.coursegraph.com/wp-content/uploads/2013/03/qrcode_for_gh_5034e4edc067_430-300x300.jpg"><img class="size-medium wp-image-85" alt="NLPJob" src="52nlp.cn_files/qrcode_for_gh_5034e4edc067_430-300x300.jpg" width="300" height="300"></a> </div>
		</aside><aside id="search-3" class="widget widget_search"><h3 class="widget-title">站内搜索</h3><form role="search" method="get" id="searchform" class="searchform" action="http://www.52nlp.cn/">
				<div>
					<label class="screen-reader-text" for="s">搜索：</label>
					<input name="s" id="s" type="text">
					<input id="searchsubmit" value="搜索" type="submit">
				</div>
			</form></aside><aside id="categories-309398092" class="widget widget_categories"><h3 class="widget-title">分类目录</h3>		<ul>
	<li class="cat-item cat-item-72"><a href="http://www.52nlp.cn/category/mit-nlp" title="麻省理工学院开放式课程&quot;自然语言处理“的相关翻译文章">MIT自然语言处理</a>
</li>
	<li class="cat-item cat-item-976"><a href="http://www.52nlp.cn/category/pattern-recognition-and-machine-learning-2">PRML</a>
</li>
	<li class="cat-item cat-item-469"><a href="http://www.52nlp.cn/category/topic-model">Topic Model</a>
</li>
	<li class="cat-item cat-item-87"><a href="http://www.52nlp.cn/category/wordpress">wordpress</a>
</li>
	<li class="cat-item cat-item-317"><a href="http://www.52nlp.cn/category/%e4%b8%93%e9%a2%98">专题</a>
</li>
	<li class="cat-item cat-item-263"><a href="http://www.52nlp.cn/category/chinese-information-processing">中文信息处理</a>
</li>
	<li class="cat-item cat-item-62"><a href="http://www.52nlp.cn/category/word-segmentation">中文分词</a>
</li>
	<li class="cat-item cat-item-1490"><a href="http://www.52nlp.cn/category/%e5%85%ac%e5%bc%80%e8%af%be" title="公开课 &amp; 课程">公开课</a>
</li>
	<li class="cat-item cat-item-420"><a href="http://www.52nlp.cn/category/%e5%b9%b6%e8%a1%8c%e7%ae%97%e6%b3%95">并行算法</a>
</li>
	<li class="cat-item cat-item-268"><a href="http://www.52nlp.cn/category/%e6%8b%9b%e8%81%98">招聘</a>
</li>
	<li class="cat-item cat-item-560"><a href="http://www.52nlp.cn/category/%e6%8e%a8%e8%8d%90%e7%b3%bb%e7%bb%9f">推荐系统</a>
</li>
	<li class="cat-item cat-item-354"><a href="http://www.52nlp.cn/category/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98">数据挖掘</a>
</li>
	<li class="cat-item cat-item-241"><a href="http://www.52nlp.cn/category/text-classification">文本分类</a>
</li>
	<li class="cat-item cat-item-1149"><a href="http://www.52nlp.cn/category/wen-ben-chu-li-yan-shi-xi-tong">文本处理演示系统</a>
</li>
	<li class="cat-item cat-item-193"><a href="http://www.52nlp.cn/category/maximum-entropy-model">最大熵模型</a>
</li>
	<li class="cat-item cat-item-344"><a href="http://www.52nlp.cn/category/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">机器学习</a>
</li>
	<li class="cat-item cat-item-1"><a href="http://www.52nlp.cn/category/machine-translation">机器翻译</a>
</li>
	<li class="cat-item cat-item-195"><a href="http://www.52nlp.cn/category/%e6%9d%a1%e4%bb%b6%e9%9a%8f%e6%9c%ba%e5%9c%ba">条件随机场</a>
</li>
	<li class="cat-item cat-item-153"><a href="http://www.52nlp.cn/category/tagging">标注</a>
</li>
	<li class="cat-item cat-item-1153"><a href="http://www.52nlp.cn/category/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0">深度学习</a>
</li>
	<li class="cat-item cat-item-885"><a href="http://www.52nlp.cn/category/%e7%a7%91%e5%ad%a6%e8%ae%a1%e7%ae%97">科学计算</a>
</li>
	<li class="cat-item cat-item-538"><a href="http://www.52nlp.cn/category/%e7%bb%9f%e8%ae%a1%e5%ad%a6">统计学</a>
</li>
	<li class="cat-item cat-item-126"><a href="http://www.52nlp.cn/category/translation-model">翻译模型</a>
</li>
	<li class="cat-item cat-item-51"><a href="http://www.52nlp.cn/category/nlp">自然语言处理</a>
</li>
	<li class="cat-item cat-item-106"><a href="http://www.52nlp.cn/category/computational-linguistics">计算语言学</a>
</li>
	<li class="cat-item cat-item-22"><a href="http://www.52nlp.cn/category/dictionary">词典</a>
</li>
	<li class="cat-item cat-item-221"><a href="http://www.52nlp.cn/category/semantics">语义学</a>
</li>
	<li class="cat-item cat-item-1121"><a href="http://www.52nlp.cn/category/%e8%af%ad%e4%b9%89%e7%9b%b8%e4%bc%bc%e5%ba%a6">语义相似度</a>
</li>
	<li class="cat-item cat-item-161"><a href="http://www.52nlp.cn/category/semantic-web">语义网</a>
</li>
	<li class="cat-item cat-item-37"><a href="http://www.52nlp.cn/category/corpus">语料库</a>
</li>
	<li class="cat-item cat-item-86"><a href="http://www.52nlp.cn/category/language-model">语言模型</a>
</li>
	<li class="cat-item cat-item-156"><a href="http://www.52nlp.cn/category/speech-recognition">语音识别</a>
</li>
	<li class="cat-item cat-item-314"><a href="http://www.52nlp.cn/category/%e8%b4%9d%e5%8f%b6%e6%96%af%e6%a8%a1%e5%9e%8b">贝叶斯模型</a>
</li>
	<li class="cat-item cat-item-110"><a href="http://www.52nlp.cn/category/reprint">转载</a>
</li>
	<li class="cat-item cat-item-451"><a href="http://www.52nlp.cn/category/%e9%97%ae%e7%ad%94%e7%b3%bb%e7%bb%9f">问答系统</a>
</li>
	<li class="cat-item cat-item-3"><a href="http://www.52nlp.cn/category/informal-essay">随笔</a>
</li>
	<li class="cat-item cat-item-60"><a href="http://www.52nlp.cn/category/hidden-markov-model">隐马尔科夫模型</a>
</li>
		</ul>
</aside>		<aside id="recent-posts-2" class="widget widget_recent_entries">		<h3 class="widget-title">最新文章</h3>		<ul>
											<li>
					<a href="http://www.52nlp.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8-1080ti-ubuntu16-04-cuda9-2-cudnn7-1-tensorflow-keras">从零开始搭建深度学习服务器: 1080TI四卡并行（Ubuntu16.04+CUDA9.2+cuDNN7.1+TensorFlow+Keras）</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/ai-challenger-2017-%e5%a5%87%e9%81%87%e8%ae%b0">AI Challenger 2017 奇遇记</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e5%ad%a6%e8%af%be%e7%a8%8b-%e6%95%b0%e5%ad%a6%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90">Coursera上数学类相关课程（公开课）汇总推荐</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/%e7%94%b5%e4%bf%a1%e8%bf%90%e8%90%a5%e5%95%86%e5%8a%ab%e6%8c%81%e4%bd%95%e6%97%b6%e4%bc%91">电信运营商劫持何时休</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e5%8d%9a%e5%bc%88%e8%ae%ba%e8%af%be%e7%a8%8b-%e5%8d%9a%e5%bc%88%e8%ae%ba%e5%85%ac%e5%bc%80%e8%af%be-game-theory-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90">Coursera上博弈论相关课程（公开课）汇总推荐</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/%e6%8e%a8%e8%8d%90nlpir%e5%a4%a7%e6%95%b0%e6%8d%ae%e8%af%ad%e4%b9%89%e6%99%ba%e8%83%bd%e5%88%86%e6%9e%90%e5%b9%b3%e5%8f%b0">推荐NLPIR大数据语义智能分析平台</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/juzhenfenjiedatagrand">推荐系统中的矩阵分解技术（达观数据  周颢钰）</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/coursera%e4%b8%8a%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84-%e7%ae%97%e6%b3%95%e8%af%be%e7%a8%8b-%e7%ae%97%e6%b3%95%e5%85%ac%e5%bc%80%e8%af%be-%e6%b1%87%e6%80%bb%e6%8e%a8%e8%8d%90">Coursera上数据结构 &amp; 算法课程（公开课）汇总推荐</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/abceshi">A/B测试的数学原理与深入理解（达观数据 陈运文）</a>
									</li>
											<li>
					<a href="http://www.52nlp.cn/andrew-ng-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e5%85%ac%e5%bc%80%e8%af%be%e7%b3%bb%e5%88%97%e7%ac%ac%e4%ba%94%e9%97%a8%e8%af%be%e7%a8%8b%e5%ba%8f%e5%88%97%e6%a8%a1%e5%9e%8b%e5%bc%80%e8%af%be">Andrew Ng 深度学习公开课系列第五门课程序列模型开课</a>
									</li>
					</ul>
		</aside><aside id="recentcomments" class="widget widget_recentcomments"><h3 class="widget-title">最近评论</h3><ul style="cursor: auto;"><li id="rc-comment-278476" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/darts-double-array-trie-system-%e7%bf%bb%e8%af%91%e6%96%87%e6%a1%a3#comment-278476" title="Darts: Double-ARray Trie System  翻译文档">天志鹏飞</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">首先说下下面的那个，那个应<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278350" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/hmm-learn-best-practices-four-hidden-markov-models#comment-278350" title="HMM学习最佳范例四：隐马尔科夫模型">askr</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">之前看HMM和MM的解释一<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278306" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/%e4%b8%ad%e8%8b%b1%e6%96%87%e7%bb%b4%e5%9f%ba%e7%99%be%e7%a7%91%e8%af%ad%e6%96%99%e4%b8%8a%e7%9a%84word2vec%e5%ae%9e%e9%aa%8c#comment-278306" title="中英文维基百科语料上的Word2Vec实验">wwt</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">求一份已下载好的完整语料，<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278305" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/%e4%b8%ad%e8%8b%b1%e6%96%87%e7%bb%b4%e5%9f%ba%e7%99%be%e7%a7%91%e8%af%ad%e6%96%99%e4%b8%8a%e7%9a%84word2vec%e5%ae%9e%e9%aa%8c#comment-278305" title="中英文维基百科语料上的Word2Vec实验">wwt</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">我也遇到这个问题了，已经下<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278159" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae-ubuntu17-04-nvidia-gtx-1080-cuda-9-0-cudnn-7-0-tensorflow-1-3#comment-278159" title="深度学习服务器环境配置: Ubuntu17.04+Nvidia GTX 1080+CUDA 9.0+cuDNN 7.0+TensorFlow 1.3">52nlp</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">随便找个空目录git cl<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278157" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%9c%8d%e5%8a%a1%e5%99%a8%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae-ubuntu17-04-nvidia-gtx-1080-cuda-9-0-cudnn-7-0-tensorflow-1-3#comment-278157" title="深度学习服务器环境配置: Ubuntu17.04+Nvidia GTX 1080+CUDA 9.0+cuDNN 7.0+TensorFlow 1.3">smallfource</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">cfn@cfn-Super<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278143" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-3#comment-278143" title="HMM学习最佳范例五：前向算法3">张峰</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">t=2时刻的隐藏状态的概率<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278033" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-2#comment-278033" title="HMM学习最佳范例五：前向算法2">52nlp</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">9年前翻译的文章，看起来原<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-278030" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/hmm-learn-best-practices-five-forward-algorithm-2#comment-278030" title="HMM学习最佳范例五：前向算法2">大中华的吕先生</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">我按照你给出的原文地址没有<span class="rc-ellipsis">...</span></div></li><li id="rc-comment-277982" class="rc-item rc-comment rc-clearfix"><div class="rc-info"><a class="rc-reviewer" rel="nofollow" href="http://www.52nlp.cn/%e4%b8%ad%e8%8b%b1%e6%96%87%e7%bb%b4%e5%9f%ba%e7%99%be%e7%a7%91%e8%af%ad%e6%96%99%e4%b8%8a%e7%9a%84word2vec%e5%ae%9e%e9%aa%8c#comment-277982" title="中英文维基百科语料上的Word2Vec实验">hustxu</a></div><div class="rc-timestamp"></div><div class="rc-excerpt">#要修改连两个位置的   <span class="rc-ellipsis">...</span></div></li></ul></aside><aside id="archives-3" class="widget widget_archive"><h3 class="widget-title">文章归档</h3>		<ul>
			<li><a href="http://www.52nlp.cn/2018/06">2018年六月</a></li>
	<li><a href="http://www.52nlp.cn/2018/05">2018年五月</a></li>
	<li><a href="http://www.52nlp.cn/2018/04">2018年四月</a></li>
	<li><a href="http://www.52nlp.cn/2018/03">2018年三月</a></li>
	<li><a href="http://www.52nlp.cn/2018/02">2018年二月</a></li>
	<li><a href="http://www.52nlp.cn/2018/01">2018年一月</a></li>
	<li><a href="http://www.52nlp.cn/2017/12">2017年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2017/11">2017年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2017/10">2017年十月</a></li>
	<li><a href="http://www.52nlp.cn/2017/09">2017年九月</a></li>
	<li><a href="http://www.52nlp.cn/2017/08">2017年八月</a></li>
	<li><a href="http://www.52nlp.cn/2017/07">2017年七月</a></li>
	<li><a href="http://www.52nlp.cn/2017/04">2017年四月</a></li>
	<li><a href="http://www.52nlp.cn/2016/11">2016年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2016/09">2016年九月</a></li>
	<li><a href="http://www.52nlp.cn/2016/07">2016年七月</a></li>
	<li><a href="http://www.52nlp.cn/2016/06">2016年六月</a></li>
	<li><a href="http://www.52nlp.cn/2016/04">2016年四月</a></li>
	<li><a href="http://www.52nlp.cn/2015/12">2015年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2015/11">2015年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2015/09">2015年九月</a></li>
	<li><a href="http://www.52nlp.cn/2015/08">2015年八月</a></li>
	<li><a href="http://www.52nlp.cn/2015/07">2015年七月</a></li>
	<li><a href="http://www.52nlp.cn/2015/06">2015年六月</a></li>
	<li><a href="http://www.52nlp.cn/2015/05">2015年五月</a></li>
	<li><a href="http://www.52nlp.cn/2015/04">2015年四月</a></li>
	<li><a href="http://www.52nlp.cn/2015/03">2015年三月</a></li>
	<li><a href="http://www.52nlp.cn/2015/01">2015年一月</a></li>
	<li><a href="http://www.52nlp.cn/2014/12">2014年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2014/11">2014年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2014/09">2014年九月</a></li>
	<li><a href="http://www.52nlp.cn/2014/07">2014年七月</a></li>
	<li><a href="http://www.52nlp.cn/2014/06">2014年六月</a></li>
	<li><a href="http://www.52nlp.cn/2014/05">2014年五月</a></li>
	<li><a href="http://www.52nlp.cn/2014/04">2014年四月</a></li>
	<li><a href="http://www.52nlp.cn/2014/01">2014年一月</a></li>
	<li><a href="http://www.52nlp.cn/2013/12">2013年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2013/06">2013年六月</a></li>
	<li><a href="http://www.52nlp.cn/2013/05">2013年五月</a></li>
	<li><a href="http://www.52nlp.cn/2013/04">2013年四月</a></li>
	<li><a href="http://www.52nlp.cn/2013/03">2013年三月</a></li>
	<li><a href="http://www.52nlp.cn/2013/02">2013年二月</a></li>
	<li><a href="http://www.52nlp.cn/2013/01">2013年一月</a></li>
	<li><a href="http://www.52nlp.cn/2012/12">2012年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2012/11">2012年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2012/10">2012年十月</a></li>
	<li><a href="http://www.52nlp.cn/2012/09">2012年九月</a></li>
	<li><a href="http://www.52nlp.cn/2012/08">2012年八月</a></li>
	<li><a href="http://www.52nlp.cn/2012/07">2012年七月</a></li>
	<li><a href="http://www.52nlp.cn/2012/06">2012年六月</a></li>
	<li><a href="http://www.52nlp.cn/2012/05">2012年五月</a></li>
	<li><a href="http://www.52nlp.cn/2012/04">2012年四月</a></li>
	<li><a href="http://www.52nlp.cn/2012/03">2012年三月</a></li>
	<li><a href="http://www.52nlp.cn/2012/01">2012年一月</a></li>
	<li><a href="http://www.52nlp.cn/2011/12">2011年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2011/11">2011年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2011/10">2011年十月</a></li>
	<li><a href="http://www.52nlp.cn/2011/09">2011年九月</a></li>
	<li><a href="http://www.52nlp.cn/2011/08">2011年八月</a></li>
	<li><a href="http://www.52nlp.cn/2011/07">2011年七月</a></li>
	<li><a href="http://www.52nlp.cn/2011/06">2011年六月</a></li>
	<li><a href="http://www.52nlp.cn/2011/05">2011年五月</a></li>
	<li><a href="http://www.52nlp.cn/2011/04">2011年四月</a></li>
	<li><a href="http://www.52nlp.cn/2011/03">2011年三月</a></li>
	<li><a href="http://www.52nlp.cn/2011/02">2011年二月</a></li>
	<li><a href="http://www.52nlp.cn/2011/01">2011年一月</a></li>
	<li><a href="http://www.52nlp.cn/2010/12">2010年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2010/11">2010年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2010/10">2010年十月</a></li>
	<li><a href="http://www.52nlp.cn/2010/09">2010年九月</a></li>
	<li><a href="http://www.52nlp.cn/2010/08">2010年八月</a></li>
	<li><a href="http://www.52nlp.cn/2010/07">2010年七月</a></li>
	<li><a href="http://www.52nlp.cn/2010/06">2010年六月</a></li>
	<li><a href="http://www.52nlp.cn/2010/05">2010年五月</a></li>
	<li><a href="http://www.52nlp.cn/2010/04">2010年四月</a></li>
	<li><a href="http://www.52nlp.cn/2010/03">2010年三月</a></li>
	<li><a href="http://www.52nlp.cn/2010/02">2010年二月</a></li>
	<li><a href="http://www.52nlp.cn/2010/01">2010年一月</a></li>
	<li><a href="http://www.52nlp.cn/2009/12">2009年十二月</a></li>
	<li><a href="http://www.52nlp.cn/2009/11">2009年十一月</a></li>
	<li><a href="http://www.52nlp.cn/2009/10">2009年十月</a></li>
	<li><a href="http://www.52nlp.cn/2009/09">2009年九月</a></li>
	<li><a href="http://www.52nlp.cn/2009/08">2009年八月</a></li>
	<li><a href="http://www.52nlp.cn/2009/07">2009年七月</a></li>
	<li><a href="http://www.52nlp.cn/2009/06">2009年六月</a></li>
	<li><a href="http://www.52nlp.cn/2009/05">2009年五月</a></li>
	<li><a href="http://www.52nlp.cn/2009/04">2009年四月</a></li>
	<li><a href="http://www.52nlp.cn/2009/03">2009年三月</a></li>
	<li><a href="http://www.52nlp.cn/2009/02">2009年二月</a></li>
	<li><a href="http://www.52nlp.cn/2009/01">2009年一月</a></li>
	<li><a href="http://www.52nlp.cn/2008/12">2008年十二月</a></li>
		</ul>
		</aside><aside id="buffercode_fl_widget_info-2" class="widget buffercode_fl_widget_info"><h3 class="widget-title"></h3>				<li style="list-style-type:none;">
					<a href="http://textminingonline.com/" target="_blank">Text Mining Online					</a>
				</li>
							<li style="list-style-type:none;">
					<a href="http://textanalysisonline.com/" target="_blank">Text Analysis Online					</a>
				</li>
							<li style="list-style-type:none;">
					<a href="http://textprocessing.org/" target="_blank">Text Processing					</a>
				</li>
							<li style="list-style-type:none;">
					<a href="http://keywordextraction.net/" target="_blank">Keyword Extraction					</a>
				</li>
							<li style="list-style-type:none;">
					<a href="http://textsummarization.net/" target="_blank">Text Summarization					</a>
				</li>
			</aside><aside id="buffercode_fl_widget_info-3" class="widget buffercode_fl_widget_info"><h3 class="widget-title"></h3>				<li style="list-style-type:none;">
					<a href="http://documentsimilarity.com/" target="_blank">Document Similarity					</a>
				</li>
							<li style="list-style-type:none;">
					<a href="http://wordsimilarity.com/" target="_blank">Word Similarity					</a>
				</li>
			</aside>		</div><!-- #secondary -->
		</div><!-- #main .wrapper -->
	<footer id="colophon" role="contentinfo">
		<div class="site-info">
									<a href="https://cn.wordpress.org/" class="imprint" title="优雅的个人发布平台">
				自豪地采用WordPress			</a>
		</div><!-- .site-info -->
	</footer><!-- #colophon -->
</div><!-- #page -->

<!-- Matomo -->
<script type="text/javascript">
  var _paq = _paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="//piwik.52nlp.com/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code --><script>
/* <![CDATA[ */
var rcGlobal = {
	serverUrl		:'http://www.52nlp.cn',
	infoTemp		:'%REVIEWER% 在 %POST%',
	loadingText		:'正在加载',
	noCommentsText	:'没有任何评论',
	newestText		:'&laquo; 最新的',
	newerText		:'&laquo; 上一页',
	olderText		:'下一页 &raquo;',
	showContent		:'',
	external		:'',
	avatarSize		:'0',
	avatarPosition	:'left',
	anonymous		:'匿名'
};
/* ]]> */
</script>
<script type="text/javascript" src="52nlp.cn_files/wp-recentcomments.js"></script>
<script type="text/javascript" src="52nlp.cn_files/wp-syntax.js"></script>
<script type="text/javascript" src="52nlp.cn_files/navigation.js"></script>
<script type="text/javascript" src="52nlp.cn_files/new-tab.js"></script>
<script type="text/javascript" src="52nlp.cn_files/wp-embed.js"></script>


</body></html>